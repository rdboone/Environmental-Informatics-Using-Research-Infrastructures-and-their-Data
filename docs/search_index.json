[["index.html", "Environmental Informatics Using Research Infrastructures and their Data Preface Acknowledgements", " Environmental Informatics Using Research Infrastructures and their Data Dr. Katharyn Duffy, Dr. Ben Ruddell Preface This textbook provides an introduction to environmental and ecological informatics in the context of big science- that is, in the context of research infrastructures and observatories that collect and publish reams of observations and derived data products about the earth and its environment. The reader will learn how to make use of environmental infrastructures data products. This textbook introduces a framework of learning outcomes that covers the broad context of these data products that is necessary to make proper use of the information: The infrastructures organizational structure and scientific scope; Instrumentation, quality control, metadata, data catalogs, APIs, and data products; Key references, key tutorials, and informatics best practices; Professional career tracks in informatics; This textbook is intended primarily for graduate students enrolled in computer science and informatics programs but engaged in studies and research on environmental and geoscience topics- with a special emphasis on ecological topics. Advanced undergraduates and other graduate students enrolled in STEM programs may also be well-served if they have a strong background in programming and computing. Additionally, professional scientists may find this textbook useful as a reference and as a training manual when they encounter the need to make use of the research infrastructures and data that are directly covered by the books content- or find the need to train a junior scientist on the use of these infrastructures data. The scope of the infrastructures and data products is mostly U.S. focused in this edition, but some of these infrastructures have a global reach, and the material is almost as useful for students in other countries as for U.S. students. The textbook can be tackled one unit at a time as a lab manual within a university course, or- in its intended application- a standard semester-long three-credit-hour graduate course should be offered to cover the entire textbook from start to finish. Digital supplements are provided with examples of successful projects. Efforts have been made to select activities using data products, software, and tools that are relatively mature and stable. Even so, because this textbook covers a rapidly moving field, portions will, unfortunately, become dated quickly. It is the authors intent to release frequent editions that update and expand the material to keep pace with the rapid development of our field. Informatics is arguably the key scientific discipline of the 21st century, and research infrastructures are the source of the raw natural resource fueling the informatics revolution: observational data. Most 21st century scientists and scientific staff will spend their careers immersed in the data revolution. We sincerely hope that this textbook provides the launchpad you need for your career or for your next project in environmental science. Acknowledgements This first version of the textbook was developed to offer INF550, a graduate course in the School of Informatics Computing and Cyber Systems at Northern Arizona University in Fall 2020, with funding and leadership from the National Science Foundation funded National Research Traineeship T3 option in Ecological and Environmental Informatics within a PhD program in Informatics and Computing (NRT-HDR #1829075, PIs Ogle, Barber, Richardson, Ruddell, and Sankey). Our research infrastructure partners were critical to the creation of the material. Our partners at NEON - Battelle deserve special gratitude for anchoring the project. The opinions expressed are those of the researchers, and not necessarily the funding agencies. Special thanks to Megan Jones and Donal OLeary at NEON-Battelle for their support in pulling NEON materials. Key Contributors Alphabetized by organization, then last name: Alycia Crall, NEON - Battelle Chris Florian, NEON - Battelle Megan Jones, NEON - Battelle Hank Loescher, NEON - Battelle Paula Mabee, NEON - Battelle Donal OLeary, NEON - Battelle Kate Thibault, NEON - Battelle Andrew Richardson, PhenoCam - NAU Bijan Seyednasrollah, PhenoCam - NAU Theresa Crimmons, USA-NPN Kathy Gerst, USA-NPN Lee Marsh, USA-NPN This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["pre-course-setup-ecoinformatics-tools.html", "Pre-Course Setup: EcoInformatics Tools 0.1 Pre-Course Skills &amp; Setup 0.2 Linux R/RStudio Setup 0.3 Installing and Setting up Git &amp; Github on Your Machine 0.4 Installing Atom 0.5 Linking RStudio to Git 0.6 How we will be Conducting this Course 0.7 Exercises:", " Pre-Course Setup: EcoInformatics Tools The purpose of this course is to train you in key ecoinformatics practices. Therefore, as an Ecoinformatician you need to be able to: Pull data from Application Programming Interfaces (APIs) More on this in Chapter 2 Organize and document your code and data Version control your code to avoid disaster and make it reproducible For you, your collaborators, and/or the wider community Push your code up to public-facing repositories Pull others code from public repositories. More thoughts on the benefits and power of reproducibility can be found here To be successful, both in this course and in your careers you will need these skills. This is why they are a requirement for this course. If you are already using these skills on a daily basis, fantastic! If you dont feel that you have mastery in the workflows listed above we have placed lesson links throughout this chapter so that you can build these skills and be successful in this course. 0.1 Pre-Course Skills &amp; Setup For the purpose of this course we will largely be using the following tools to access, pull, and explore data: R &amp; Rstudio Git, GitHub, &amp; Atom.io Markdown &amp; Rmarkdown As such we will need to install and/or update these tools on your personal computer before our first day of class. While we chose R for this course, nearly all of the packages and data are fully available and transferable to Python or other languages. If youd like to brush up on your R skills I highly recommend Data Carpentry Boot camps free R for Reproducible Scientific Analysis course. 0.1.1 Installing or Updating R Please check your version of R. You will need R 3.6.0+ How to check your version in R or RStudio if you already have it: &gt; version _ platform x86_64-apple-darwin15.6.0 arch x86_64 os darwin15.6.0 system x86_64, darwin15.6.0 status major 3 minor 5.1 year 2018 month 07 day 02 svn rev 74947 language R version.string R version 3.5.1 (2018-07-02) nickname Feather Spray If you dont already have R or need to update it do so here. 0.1.2 Windows R/RStudio Setup After you have downloaded R, run the .exe file that was just downloaded Go to the RStudio Download page Under Installers select RStudio X.XX.XXX - e.g. Windows Vista/7/8/10 Double click the file to install it Once R and RStudio are installed, click to open RStudio. If you dont get any error messages you are set. If there is an error message, you will need to re-install the program. 0.1.3 Mac R/RStudio Setup After you have downloaded R, double click on the file that was downloaded and R will install Go to the RStudio Download page Under Installers select RStudio 1.2.1135 - Mac OS X XX.X (64-bit) to download it. Once its downloaded, double click the file to install it. Once R and RStudio are installed, click to open RStudio. If you dont get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2 Linux R/RStudio Setup R is available through most Linux package managers. You can download the binary files for your distribution from CRAN. Or you can use your package manager. e.g. for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo yum install R To install RStudio, go to the RStudio Download page Under Installers select the version for your distribution. Once its downloaded, double click the file to install it Once R and RStudio are installed, click to open RStudio. If you dont get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2.1 Install basic packages for this course You can run the following script to make sure all the required packages are properly installed on your computer. # list of required packages list.of.packages &lt;- c( &#39;data.table&#39;, &#39;tidyverse&#39;, &#39;jsonlite&#39;, &#39;jpeg&#39;, &#39;png&#39;, &#39;raster&#39;, &#39;rgdal&#39;, &#39;rmarkdown&#39;, &#39;knitr&#39; ) # identify new (not installed) packages new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # install new (not installed) packages if(length(new.packages)) install.packages(new.packages, repos=&#39;http://cran.rstudio.com/&#39;) # load all of the required libraries sapply(list.of.packages, library, character.only = T) Note: On some operating systems, you may need to install the Geospatial Data Abstraction Library (GDAL). More information about GDAL can be found from here. 0.3 Installing and Setting up Git &amp; Github on Your Machine For this course you will need: 1. Git installed on your local machine 2. Very basic bash scripting 3. A linked GitHub account 4. To link RStudio to git via RStudio or Atom.io As we will be using these skills constantly, they are a pre-requisite for this course. If you dont yet have these skills its okay! You can learn everything that you need to know via the following freely available resources: The Unix Shell Version Control with Git Happy Git with R If you are learning these skills from scratch I estimate that you will need to devote ~4-6 hours to get set up and comfortable with the various workflows. Also remember that I have code office hours every week and that Stack Exchange is your friend. 0.4 Installing Atom Atom.io is a powerful and useful text editor for the following reasons: It is language agnostic It fully integrates with git and github + You can use it to push/pull/resolve conflicts and write code all in one space. 0.5 Linking RStudio to Git Happy Git with R has a fantastic tutorial to help you link Rstudio-Git-Github on your local machine and push/pull from or to public repositories. 0.6 How we will be Conducting this Course If you find a broken link or error in this course text submit an issue on the course github repository. At the end of each chapter you will find a set of Exercises. At the end of the assigned chapter you will be expected to submit two files to the course webdrive: An RMarkdown file with the naming convention: LASTNAME_COURSECODE_Section#.Rmd, and A knitted .PDF with the same naming convention: LASTNAME_COURSECODE_Section#.pdf To generate these files you have two options: Click on the pencil and pad logo in the top of this text, copy the exercise section code, and drop it into your own .Rmd. Git clone our course Github Repository, navigate to the _Exercises folder, and use that .Rmd as a template. Note: Exercises submitted in any other format, or those missing questions will not be graded To generate your .PDF to upload, in your RMarkdown file simply push the Knit button at the top of your document. 0.7 Exercises: 0.7.1 Exercise 0.1: A git introduction Navigate to our course github git fork our repo onto your own personal github account. git clone the repo onto your own personal machine in a place that is functional and not temporary (e.g. not your downloads folder). #hints cd `Your/Path/Here&#39; git clone &#39;repo HTTPS&#39; Add 2-3 sentences introducing yourself in the _Course-participants folder. For example: *** Hi, I&#39;m Dr. Katharyn Duffy. I have a Ph.D in Earth Science from Northern Arizona University. Over the past two years I&#39;ve worked as an open-source software engineer in the PhenoCam lab, and now I&#39;m the coding and lab support for your course. I really look forward to working with all of you! *** Submit a pull request to add your introduction to our course participants folder. #hints git add ... git commit ... git status.... git push --set-upstream git remote -v git remote add upstream... Note: You may complete these either on the command line or via a program like Atom.io. If you havent yet made commits to a remote repository or submitted pull requests please reference the resources listed above. "],["why-ecoinformatics.html", "Chapter 1 Why EcoInformatics? 1.1 The Framework of this Course 1.2 Final Course Project: Proposed Derived Data Product", " Chapter 1 Why EcoInformatics? Portions of the following introduction were adapted from Michener &amp; Jones 2012, Trends in Ecology &amp; Evolution Ecoinformatics: supporting ecology as a data-intensive science Ecology is increasingly becoming a data-intensive science, relying on massive amounts of data collected by both remote-sensing platforms and sensor networks embedded in the environment. New observatory networks, such as the US National Ecological Observatory Network (NEON), provide research platforms that enable scientists to examine phenomena across diverse ecosystem types through access to thousands of sensors collecting diverse environmental observations. These networks spatially and temporaly overlap with a number of other networks and infrastructures ranging from remote sensing, to citizen science, and so on. It has been argued that data-intensive science represents the fourth scientific paradigm following the empirical (i.e. description of natural phenomena), theoretical (e.g. modeling and generalization) and computational (e.g. simulation) scientific approaches, and comprises an approach for unifying theory, experimentation and simulation. Ecologists increasingly address questions at broader scales that have both scientific and societal relevance. For example, the 40 top priorities for science that can inform conservation and management policy in the USA rely principally on a sound foundation of ecological research, and the ability to scale knowledge and inter-connect data. Continental-scale patterns and dynamics result from climate and people as broad-scale drivers interacting with finer-scale vectors that redistribute materials within and among linked terrestrial and aquatic systems. Climate and land-use change interact with patterns and processes at multiple, finer scales (blue arrows). (a) These drivers can influence broad-scale patterns directly, and these constraints may act to overwhelm heterogeneity and processes at (b) mesoscales and at (c) the finer scale of local sites. Broad-scale drivers can also exert an indirect impact on broad-scale patterns through their interactions with disturbances, including (d) the spread of invasive species, (e) patternprocess relationships at meso-scales, or (f) at finer scales within a site. Connectivity imparted by the transfer of materials occurs both at (g) the meso-scale and at (h) finer scales within sites where terrestrial and aquatic systems are connected. These dynamics at fine scales can propagate to influence larger spatial extents (red arrows). Feedbacks occur throughout the system. The term drivers refers to both forcing functions that are part of the system and to external drivers. Peters et al., 2008 Ecology is also affected by changes that are occurring throughout science as a whole. In particular, scientists, professional societies and research sponsors are recognizing the value of data as a product of the scientific enterprise and placing increased emphasis on data stewardship, data sharing, openness and supporting study repeatability. Data on ecological and environmental systems are (A) acquired, checked for quality, documented using an acquisition workflow, and then both the raw and derived data products are versioned and deposited in the DataONE federated data archive (red dashed arrows). Researchers discover and access data from the federation and then (B) integrate and process the data in an analysis workflow, resulting in derived data products, visualizations, and scholarly papers that are in turn archived in the data federation (red dashed arrows). Other researchers directly cite any of the versioned data, workflows, and visualizations that are archived in the DataONE federation. Richman et al., 2011 The changes that are occurring in ecology create challenges with respect to acquiring, managing and analyzing the large volumes of data that are collected by scientists worldwide. One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological processes occurring at the scale of the continent and biosphere. The diversity in scales studied and the ways in which studies are carried out results in large numbers of small, idiosyncratic data sets that accumulate from the thousands of scientists that collect relevant biological, ecological and environmental data. A proposed high-level architecture for ecological and environmental data management is shown consisting of three primary levels. Data stored within distributed data repositories (a) is mediated by standard metadata and ontologies (b) to power software tools used by scientists and data managers (c). Software applications use community-endorsed ontologies and metadata standards from the middle level to provide tools that are more effective for publishing, querying, integrating and analyzing data. Ontologies are separated into framework ontologies and domain-specific extensions, enabling contributions from multiple research groups, disciplines and individuals. Cross-disciplinary data are maintained in local repositories, but made accessible to the broader research community through distributed systems based on shared, open protocols (such as Metacat). Example repositories include the LTER network, National Ecological Observatory Network, United States Geographical Survey and SEEKs EarthGrid. Madin et al. 2008, Ecoinformatics is a framework that enables scientists to generate new knowledge through innovative tools and approaches for: discovering, managing, integrating, analyzing, visualizing, and preserving relevant biological, environmental, and socioeconomic data and information. Many ecoinformatics solutions have been developed over the past decade, increasing scientists efficiency and supporting faster and easier data discovery, integration and analysis; however, many challenges remain, especially in relation to installing ecoinformatics practices into mainstream research and education. And that, course participants, is why we are here. 1.1 The Framework of this Course Over the duration of this course we will survey a wide array of observation platforms and networks and build hands-on experience with the framework of Ecoinformatics. For coherance we will cover the following overarching themes: Each networks mission and design Each networks spatial design e.g. opprtunistic vs. planned, citizen science vs. orbital sensors The types of data that stream from each network e.g. sensors, derived products, metadata How to access that data e.g. APIs, landing pages, r packages etc. Opportunities to interact with or contribute to each network e.g. RFPs coming down the pipeline, internships, and post-doctoral scholar programs. At the conclusion of each networks section you will be asked to write a 1-page summary reviewing the above framework for each network, and highlight how it potentially aligns with your own research. These series of 1-page summaries will then culminate into a final presentation where you propose to derive your own data product for your own research touching upon multiple networks and accounting for differences in spatial footprints, frequency of observations, and important data cross-walks. 1.2 Final Course Project: Proposed Derived Data Product For your final project, you will present a 4-6 minute IGNITE-style derived data product pitch, followed by 2-3 minutes of questions from your audience (which will include members from the infrastrures weve covered). Think of this project as your sales pitch to the research infrastructure whose data you are using, and/or the scientific community as a whole. In the IGNITE theme of Enlighten us but make it quick, you will construct a series of slides that auto-advance every 30 seconds. Specific instructions for the content of each slide are below. Ideally, this final presentation will feed upon a number of the culmination write-ups you have conducted over the course of the semester. Ideally, this derived data product will utilize data from a number of sources, either covered within this course or external to it. Ideally, it will also convince your audience that your idea is novel, useful, and possible. In order to complete this presentation, you will need to have worked with the various data products you propose, have an in-depth understanding of them, and their challenges, along with original, clean, high-level summary graphics. Further, giving an IGNITE-style presentation takes practice. IGNITE-style presentations are powerful, as they keep you moving forward, and give your audience a high-level understanding of your topic. We fully recommend rehearsing your presentation many times before giving it live and recording yourself to learn how you can improve. Heres an example (of an even faster) ignite talk from one of your books authors: In your derived data product pitch you will cover these themes: The need for the derived data that you are proposing to produce. What data you will use to derive this product, including the justification for this exact data. The processing pipeline for this product, along with estimates for a timeline. Potential hurdles you will have to overcome. How this product will serve the infrastructure and/or the scientific community. Specific slide criteria are as follows: Slide 1: Title, authors (including contacts at infrastructures covered if applicable) Slide 2: Justification for the derived data product; the gap or need that it fills Slides 3-x: 1 slide per data product used including: The exact data product (e.g. NEON data product id and full title) A 1 sentence summary of the data product and its justification for this purpose An original, clean, polished high-level plot, gif or .mp4 of the data Slide x + 1: A high-level workflow diagram of the processing pipeline E.g.: Original data and how you pull it in (API, r-package etc) Filtering process using QA/QC or metadata Orthorectification in time or space Example generated using draw.io: Slide x + 2: A clean plot of all of the data you mentioned together, and/or the derived data product itself with a 1 sentence summary Example: Slide x+3: Summary: Circle back on how this derived data product serves your research, the infrastructure, and the wider science community (no more than 10 words, suggestion: graphics or bullet points) Slide x + 4: Data citations for all data used in proposed derived data product An example slide deck with specific ideas can be found here The rubric for your final presentation grade is as follows: Presentation meets all requirements and criteria: 60% Aesthetics and craft of presentation: 10% Live presentation of materials: 30% "],["introduction-to-neon-its-data.html", "Chapter 2 Introduction to NEON &amp; its Data 2.1 Learning Objectives 2.2 The NEON Project Mission &amp; Design 2.3 The Science and Design of NEON 2.4 NEONs Spatial Design 2.5 How NEON Collects Data 2.6 Accessing NEON Data 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.8 Additional Resources 2.9 Hands on: NEON TOS Data 2.10 Intro to NEON Exercises Part 1 2.11 Hands on: Pulling NEON Data via the API 2.12 What is an API? 2.13 Stacking NEON data 2.14 Intro to NEON Exercises Part 2", " Chapter 2 Introduction to NEON &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will work on the over this semester. At the end of this section you will document an initial research question, or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 2.1 Learning Objectives At the end of this activity, you will be able to: Explain the mission of the National Ecological Observatory Network (NEON). Explain the how sites are located within the NEON project design. Determine how the different types of data that are collected and provided by NEON, and how they align with your own research. Pull NEON data from the API and neonUtilities package [@R-neonUtilites] 2.2 The NEON Project Mission &amp; Design To capture ecological heterogeneity across the United States, NEONs design divides the continent into 20 statistically different eco-climatic domains. Each NEON field site is located within an eco-climatic domain. 2.3 The Science and Design of NEON To gain a better understanding of the broad scope of NEON watch this 4:08 minute long video. 2.4 NEONs Spatial Design Watch this 4:22 minute video exploring the spatial design of NEON field sites. Please read the following page about NEONs Spatial Design: Read this primer on NEONs Sampling Design Read about the different types of field sites - core and relocatable 2.4.1 NEON Samples All 20 Eco-Regions Explore the NEON Field Site map taking note of the locations of: Aquatic &amp; terrestrial field sites. Core &amp; relocatable field sites. Click here to view the NEON Field Site Map Explore the NEON field site map. Do the following: Zoom in on a study area of interest to see if there are any NEON field sites that are nearby. Click the More button in the upper right hand corner of the map to filter sites by name, site host, domain or state. Select one field site of interest. Click on the marker in the map. Then click on the name of the field site to jump to the field site landing page. Data Tip: You can download maps, kmz, or shapefiles of the field sites here. 2.5 How NEON Collects Data Watch this 3:06 minute video exploring the data that NEON collects. Read the Data Collection Methods page to learn more about the different types of data that NEON collects and provides. Then, follow the links below to learn more about each collection method: Aquatic Observation System (AOS) Aquatic Instrument System (AIS) Terrestrial Instrument System (TIS)  Flux Tower Terrestrial Instrument System (TIS)  Soil Sensors and Measurements Terrestrial Organismal System (TOS) Airborne Observation Platform (AOP) All data collection protocols and processing documents are publicly available. Read more about the standardized protocols and how to access these documents. 2.5.1 Specimens &amp; Samples NEON also collects samples and specimens from which the other data products are based. These samples are also available for research and education purposes. Learn more: NEON Biorepository. 2.5.2 Airborne Remote Sensing Watch this 4:02 minute video to better understand the NEON Airborne Observation Platform (AOP). Data Tip: NEON also provides support to your own research including proposals to fly the AOP over other study sites, a mobile tower/instrumentation setup and others. Learn more here the Assignable Assets programs . 2.6 Accessing NEON Data NEON data are processed and go through quality assurance quality control checks at NEON headquarters in Boulder, CO. NEON carefully documents every aspect of sampling design, data collection, processing and delivery. This documentation is freely available through the NEON data portal. Visit the NEON Data Portal - data.neonscience.org Read more about the quality assurance and quality control processes for NEON data and how the data are processed from raw data to higher level data products. Explore NEON Data Products. On the page for each data product in the catalog you can find the basic information about the product, find the data collection and processing protocols, and link directly to downloading the data. Additionally, some types of NEON data are also available through the data portals of other organizations. For example, NEON Terrestrial Insect DNA Barcoding Data is available through the Barcode of Life Datasystem (BOLD). Or NEON phenocam images are available from the Phenocam network site. More details on where else the data are available from can be found in the Availability and Download section on the Product Details page for each data product (visit Explore Data Products to access individual Product Details pages). 2.6.1 Pathways to access NEON Data There are several ways to access data from NEON: Via the NEON data portal. Explore and download data. Note that much of the tabular data is available in zipped .csv files for each month and site of interest. To combine these files, use the neonUtilities package (R tutorial, Python tutorial). Use R or Python to programmatically access the data. NEON and community members have created code packages to directly access the data through an API. Learn more about the available resources by reading the Code Resources page or visiting the NEONScience GitHub repo. Using the NEON API. Access NEON data directly using a custom API call. Access NEON data through partners portals. Where NEON data directly overlap with other community resources, NEON data can be accessed through the portals. Examples include Phenocam, BOLD, Ameriflux, and others. You can learn more in the documentation for individual data products. 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.7.1 Via the NEON API, with your User Token NEON data can be downloaded from either the NEON Data Portal or the NEON API. When downloading from the Data Portal, you can create a user account. Read about the benefits of an account on the User Account page. You can also use your account to create a token for using the API. Your token is unique to your account, so dont share it. While using a token is optional in general, it is required for this course. Using a token when downloading data via the API, including when using the neonUtilities package, links your downloads to your user account, as well as enabling faster download speeds. For more information about token usage and benefits, see the NEON API documentation page. For now, in addition to faster downloads, using a token helps NEON to track data downloads. Using anonymized user information, they can then calculate data access statistics, such as which data products are downloaded most frequently, which data products are downloaded in groups by the same users, and how many users in total are downloading data. This information helps NEON to evaluate the growth and reach of the observatory, and to advocate for training activities, workshops, and software development. Tokens can (and should) be used whenever you use the NEON API. In this tutorial, well focus on using tokens with the neonUtilities R package. 2.7.2 Objectives After completing this section, you will be able to: Create a NEON API token Use your token when downloading data with neonUtilities 2.7.3 Things Youll Need To Complete This Tutorial You will need a version of R (3.4.1 or higher) and RStudio loaded on your computer. 2.7.4 Install R Packages neonUtilities: install.packages(\"neonUtilities\") 2.8 Additional Resources NEON Data Portal NEONScience GitHub Organization neonUtilities tutorial If youve never downloaded NEON data using the neonUtilities package before, we recommend starting with the Download and Explore tutorial before proceeding with this tutorial. In the next sections, well get an API token from the NEON Data Portal, and then use it in neonUtilities when downloading data. 2.8.1 Get a NEON API Token The first step is create a NEON user account, if you dont have one. Follow the instructions on the Data Portal User Accounts page. If you do already have an account, go to the NEON Data Portal, sign in, and go to your My Account profile page. Once you have an account, you can create an API token for yourself. At the bottom of the My Account page, you should see this bar: Click the GET API TOKEN button. After a moment, you should see this: Click on the Copy button to copy your API token to the clipboard. 2.8.2 Use the API token in neonUtilities In the next section, well walk through saving your token somewhere secure but accessible to your code. But first lets try out using the token the easy way. First, we need to load the neonUtilities package and set the working directory: # install neonUtilities - can skip if already installed, but # API tokens are only enabled in neonUtilities v1.3.4 and higher # if your version number is lower, re-install install.packages(&quot;neonUtilities&quot;) # load neonUtilities library(neonUtilities) # set working directory wd &lt;- &quot;~/data&quot; # this will depend on your local machine setwd(wd) NEON API tokens are very long, so it would be annoying to keep pasting the entire text string into functions. Assign your token an object name: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Now well use the loadByProduct() function to download data. Your API token is entered as the optional token input parameter. For this example, well download Plant foliar traits (DP1.10026.001). foliar &lt;- loadByProduct(dpID=&quot;DP1.10026.001&quot;, site=&quot;all&quot;, package=&quot;expanded&quot;, check.size=F, token=NEON_TOKEN) You should now have data saved in the foliar object; the API silently used your token. If youve downloaded data without a token before, you may notice this is faster! This format applies to all neonUtilities functions that involve downloading data or otherwise accessing the API; you can use the token input with all of them. For example, when downloading remote sensing data: chm &lt;- byTileAOP(dpID=&quot;DP3.30015.001&quot;, site=&quot;WREF&quot;, year=2017, check.size=F, easting=c(571000,578000), northing=c(5079000,5080000), savepath=wd, token=NEON_TOKEN) 2.8.3 Token management for open code Your API token is unique to your account, so dont share it! If youre writing code that will be shared with colleagues or available publicly, such as in a GitHub repository or supplemental materials of a published paper, you cant include the line of code above where we assigned your token to NEON_TOKEN, since your token is fully visible in the code there. Instead, youll need to save your token locally on your computer, and pull it into your code without displaying it. There are a few ways to do this, well show two options here. Option 1: Save the token in a local file, and source() that file at the start of every script. This is fairly simple but requires a line of code in every script. Option 2: Add the token to a .Renviron file to create an environment variable that gets loaded when you open R. This is a little harder to set up initially, but once its done, its done globally, and it will work in every script you run. 2.8.3.1 Option 1: Save token in a local file Open a new, empty R script (.R). Put a single line of code in the script: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Save this file within your current R project and call the file neon_token_source.R. So that you dont accidently push your token up to GitHub, move over to the command line or Atom.io and add it to your .gitignore file: knitr::include_graphics(&#39;./docs/images/git_ignore.png&#39;) Now, whenever you want to pull NEON data via the API, at the start of any analysis you would place this line of code: source(&#39;neon_token_source.R&#39;) Then youll be able to use token=NEON_TOKEN when you run neonUtilities functions, and you can share your code without accidentally sharing your token. 2.8.3.2 Option 2: Save your toekn to your R environment Instructions for finding and editing your .Renviron can be found in this tutorial in NEONs Data Tutorials section. 2.9 Hands on: NEON TOS Data 2.9.1 Pull in Tree Data from NEONs TOS and investigate relationships Adapted from Claire Lunchs Compare tree height measured from the ground to a Lidar-based Canopy Height Model tutorial Later in this course we will be working with NEONs LiDAR-based Canopy Height Model (CHM) data from their extensive Airborne Observation Platform (AOP). In this section we will pull in DP1.10098.001, Woody plant vegetation structure from NEONs Terrestrial Observation Sampling (TOS) data and explore the data, from requesting it to plotting it. Generalized TOS sampling schematic, showing the placement of Distributed, Tower, and Gradient Plots from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 The vegetation structure data are collected by by field staff on the ground. This data product contains the quality-controlled, native sampling resolution data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. The exact measurements collected per individual depend on growth form, and these measurements are focused on enabling biomass and productivity estimation, estimation of shrub volume and biomass, and calibration / validation of multiple NEON airborne remote-sensing data products. In general, comparatively large individuals that are visible to remote-sensing instruments are mapped, tagged and measured, and other smaller individuals are tagged and measured but not mapped. Smaller individuals may be subsampled according to a nested subplot approach in order to standardize the per plot sampling effort. Structure and mapping data are reported per individual per plot; sampling metadata, such as per growth form sampling area, are reported per plot. Illustration of a 20 m x 20 m Distributed/Gradient/Tower base plot (left), a 40 m x 40 m Tower base plot (right), and associated nested subplots used for measuring woody stem vegetation. Locations of subplots are denoted with plain text numbers, and locations of nested subplots are denoted with italic numbers from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 For the purpose of this hands-on activity we will be using data from the Wind River Experimental Forest NEON field site located in Washington state. The predominant vegetation at that site is tall evergreen conifers. Note: this is also a core site for many other networks such as AmeriFlux and FLUXNET, which we will cover later. Image of the Wind River Crane Flux Tower from Ameriflux Lets begin by: Installing the geoNEON package Making sure that the packages that we need are loaded, and Supressing strings as factors in R, as factors make all sorts of functions in R cranky. options(stringsAsFactors=F) #install.packages(&quot;devtools&quot;) #uncomment if you don&#39;t yet have devtools #devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;) library(neonUtilities) library(geoNEON) library(sp) Now lets begin by pulling in the vegetation structure data using the loadByProduct() function in the neonUtilities package. Inputs needed to the function are: dpID: data product ID; (woody vegetation structure = DP1.10098.001 site: 4-letter site code; Wind River = WREF package: basic or expanded; well begin with a basic here veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |===== | 7% | |========= | 13% | |============== | 20% | |=================== | 27% | |======================= | 33% | |============================ | 40% | |================================= | 47% | |===================================== | 53% | |========================================== | 60% | |=============================================== | 67% | |=================================================== | 73% | |======================================================== | 80% | |============================================================= | 87% | |================================================================= | 93% | |======================================================================| 100% ## ## Downloading files totaling approximately 18.79386 MB ## Downloading 15 files ## | | | 0% | |===== | 7% | |========== | 14% | |=============== | 21% | |==================== | 29% | |========================= | 36% | |============================== | 43% | |=================================== | 50% | |======================================== | 57% | |============================================= | 64% | |================================================== | 71% | |======================================================= | 79% | |============================================================ | 86% | |================================================================= | 93% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Stacking table vst_non-woody ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 4 data tables and 4 metadata tables! ## Stacking took 0.7870159 secs Now, use the getLocTOS() function in the geoNEON package to get precise locations for the tagged plants. You can refer to the package documentation for more details. vegmap &lt;- getLocTOS(veglist$vst_mappingandtagging, &quot;vst_mappingandtagging&quot;) ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |==== | 5% | |==== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======= | 11% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========== | 14% | |========== | 15% | |=========== | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================= | 24% | |================== | 25% | |================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |===================== | 31% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |======================== | 35% | |========================= | 36% | |========================== | 37% | |========================== | 38% | |=========================== | 38% | |=========================== | 39% | |============================ | 39% | |============================ | 40% | |============================ | 41% | |============================= | 41% | |============================= | 42% | |============================== | 42% | |============================== | 43% | |=============================== | 44% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |====================================== | 54% | |====================================== | 55% | |======================================= | 56% | |======================================== | 57% | |======================================== | 58% | |========================================= | 58% | |========================================= | 59% | |========================================== | 59% | |========================================== | 60% | |========================================== | 61% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 62% | |============================================ | 63% | |============================================= | 64% | |============================================== | 65% | |============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |======================================================== | 81% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================= | 88% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 89% | |=============================================================== | 90% | |=============================================================== | 91% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 92% | |================================================================= | 93% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% Now we need to merge the mapped locations of individuals (the vst_mappingandtagging table) with the annual measurements of height, diameter, etc (the vst_apparentindividual table). The two tables join based on individualID, the identifier for each tagged plant, but well include namedLocation, domainID, siteID, and plotID in the list of variables to merge on, to avoid ending up with duplicates of each of those columns. Refer to the variables table and to the Data Product User Guide for Woody plant vegetation structure for more information about the contents of each data table. veg &lt;- merge(veglist$vst_apparentindividual, vegmap, by=c(&quot;individualID&quot;,&quot;namedLocation&quot;, &quot;domainID&quot;,&quot;siteID&quot;,&quot;plotID&quot;)) What did you just pull in? Are you sure you know what youre working with? A best practice is to always do a quick visualization to make sure that you have the right data and that you understand its spread: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) A key component of any measurement, and therefore a reoccuring theme in this course, is an estimate of uncertainty. Lets overlay estimates of uncertainty for the location of each stem in blue: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$adjCoordinateUncertainty[which(veg$plotID==&quot;WREF_075&quot;)], inches=F, add=T, fg=&quot;lightblue&quot;) 2.10 Intro to NEON Exercises Part 1 2.10.1 Exercise 2.1: NEON Coding Lab - TOS Vegetation Structure 2.10.1.1 Part 1: Sign up for and Use an NEON API Token: Submit via .Rmd and .pdf a simple script that uses a HIDDEN token to access NEON data. Example: source(&#39;neon_token_source.R&#39;) veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |===== | 7% | |========= | 13% | |============== | 20% | |=================== | 27% | |======================= | 33% | |============================ | 40% | |================================= | 47% | |===================================== | 53% | |========================================== | 60% | |=============================================== | 67% | |=================================================== | 73% | |======================================================== | 80% | |============================================================= | 87% | |================================================================= | 93% | |======================================================================| 100% ## ## Downloading files totaling approximately 18.79386 MB ## Downloading 15 files ## | | | 0% | |===== | 7% | |========== | 14% | |=============== | 21% | |==================== | 29% | |========================= | 36% | |============================== | 43% | |=================================== | 50% | |======================================== | 57% | |============================================= | 64% | |================================================== | 71% | |======================================================= | 79% | |============================================================ | 86% | |================================================================= | 93% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Stacking table vst_non-woody ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 4 data tables and 4 metadata tables! ## Stacking took 0.510011 secs summary(veglist) ## Length Class Mode ## categoricalCodes_10098 5 data.table list ## issueLog_10098 9 data.table list ## readme_10098 1 data.frame list ## validation_10098 8 data.table list ## variables_10098 9 data.table list ## vst_apparentindividual 41 data.frame list ## vst_mappingandtagging 30 data.frame list ## vst_non-woody 47 data.frame list ## vst_perplotperyear 43 data.frame list 2.10.1.2 Part 2: Further Investigation of NEON TOS Vegetation Structure Data Suggested Timing: Complete this exercise before our next class session In the following section all demonstration code uses the iris dataset for R as examples. In this exercise the iris data is merely used for example code to get your started, you will complete all plots and models using the NEON TOS vegetation structure data Convert the above diameter plot into a ggplot: If you need some refreshers on ggplot Derek Sondereggers Introductory Data Science using R: Graphing Part II is a wonderful resource. Ive pulled some of his plotting examples here. library(ggplot2) print (&#39;your code here&#39;) ## [1] &quot;your code here&quot; Set the color your circles to be a function of each species: #hints: data(&quot;iris&quot;) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() Generate a histogram of tree heights for each plot. Color your stacked bar as a function of each species: #hints for faceting: ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + facet_grid( . ~ Species ) Use dplyr to remove dead trees: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union #hints: #veg=veg%&gt;% #filter(..... !=....) Create a simple linear model that uses Diameter at Breast Height (DBH) and height to predict allometries. Print the summary information of your model: #hints mdl=lm(Some_diameter + Some_height, data=something) #Question: looking at the metadata which &#39;height&#39; and &#39;diameter&#39; variables should you use? print(mdl) Plot your linear model: # hints: mdl &lt;- lm( Petal.Length ~ Sepal.Length * Species, data = iris ) iris &lt;- iris %&gt;% select( -matches(&#39;fit&#39;), -matches(&#39;lwr&#39;), -matches(&#39;upr&#39;) ) %&gt;% cbind( predict(mdl, newdata=., interval=&#39;confidence&#39;) ) head(iris, n=3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species fit lwr ## 1 5.1 3.5 1.4 0.2 setosa 1.474373 1.398783 ## 2 4.9 3.0 1.4 0.2 setosa 1.448047 1.371765 ## 3 4.7 3.2 1.3 0.2 setosa 1.421721 1.324643 ## upr ## 1 1.549964 ## 2 1.524329 ## 3 1.518798 ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() + geom_line( aes(y=fit) ) + geom_ribbon( aes( ymin=lwr, ymax=upr, fill=Species), alpha=.3 ) # alpha is the ribbon transparency Answer the following questions: What do you think about your simile linear model? What are its limitations? How many unique species are present at WREF? What are the top_5 trees based on height? Diameter? What proportion of sampled trees are dead? 2.11 Hands on: Pulling NEON Data via the API This section covers pulling data from the NEON API or Application Programming Interface using R and the R package httr, but the core information about the API is applicable to other languages and approaches. As a reminder, there are 3 basic categories of NEON data: Observational - Data collected by a human in the field, or in an analytical laboratory, e.g. beetle identification, foliar isotopes Instrumentation - Data collected by an automated, streaming sensor, e.g. net radiation, soil carbon dioxide Remote sensing - Data collected by the airborne observation platform, e.g. LIDAR, surface reflectance This lab covers all three types of data, it is required to complete these sections in order and not skip ahead, since the query principles are explained in the first section, on observational data. 2.11.1 Objectives After completing this activity, you will be able to: Pull observational, instrumentation, and geolocation data from the NEON API. Transform API-accessed data from JSON to tabular format for analyses. 2.11.2 Things Youll Need To Complete This Section To complete this tutorial you will need the most current version of R and, preferably, RStudio loaded on your computer. 2.11.2.1 Install R Packages httr: install.packages(\"httr\") jsonlite: install.packages(\"jsonlite\") dplyr: install.packages(\"dplyr\") devtools: install.packages(\"devtools\") downloader: install.packages(\"downloader\") geoNEON: devtools::install_github(\"NEONScience/NEON-geolocation/geoNEON\") neonUtilities: devtools::install_github(\"NEONScience/NEON-utilities/neonUtilities\") Note, you must have devtools installed &amp; loaded, prior to loading geoNEON or neonUtilities. 2.11.2.2 Additional Resources Webpage for the NEON API GitHub repository for the NEON API ROpenSci wrapper for the NEON API (not covered in this tutorial) 2.12 What is an API? The following material was adapted from: Using the NEON API in R description: Tutorial for getting data from the NEON API, using R and the R package httr dateCreated: 2017-07-07 authors: [Claire K. Lunch] contributors: [Christine Laney, Megan A. Jones] If you are unfamiliar with the concept of an API, think of an API as a middle person that provides a communication path for a software application to obtain information from a digital data source. APIs are becoming a very common means of sharing digital information. Many of the apps that you use on your computer or mobile device to produce maps, charts, reports, and other useful forms of information pull data from multiple sources using APIs. In the ecological and environmental sciences, many researchers use APIs to programmatically pull data into their analyses. (Quoted from the NEON Observatory Blog story: API and data availability viewer now live on the NEON data portal.) There are actually many types or constructions of APIs. If youre interested you can read a little more about them here 2.12.1 Anatomy of an API call An example API call: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 This includes the base URL, endpoint, and target. 2.12.1.1 Base URL: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 Specifics are appended to this in order to get the data or metadata youre looking for, but all calls to this API will include the base URL. For the NEON API, this is http://data.neonscience.org/api/v0  not clickable, because the base URL by itself will take you nowhere! 2.12.1.2 Endpoints: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 What type of data or metadata are you looking for? ~/products Information about one or all of NEONs data products ~/sites Information about data availability at the site specified in the call ~/locations Spatial data for the NEON locations specified in the call ~/data Data! By product, site, and date (in monthly chunks). 2.12.2 Targets: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 The specific data product, site, or location you want to get data for. 2.12.3 Observational data (OS) Which product do you want to get data for? Consult the Explore Data Products page. Well pick Breeding landbird point counts, DP1.10003.001 First query the products endpoint of the API to find out which sites and dates have data available. In the products endpoint, the target is the numbered identifier for the data product: # Load the necessary libraries library(httr) library(jsonlite) library(dplyr, quietly=T) library(downloader) # Request data using the GET function &amp; the API call req &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.10003.001&quot;) req ## Response [https://data.neonscience.org/api/v0/products/DP1.10003.001] ## Date: 2022-09-29 17:30 ## Status: 200 ## Content-Type: application/json;charset=UTF-8 ## Size: 47.6 kB The object returned from GET() has many layers of information. Entering the name of the object gives you some basic information about what you downloaded. The content() function returns the contents in the form of a highly nested list. This is typical of JSON-formatted data returned by APIs. We can use the names() function to view the different types of information within this list. # View requested data req.content &lt;- content(req, as=&quot;parsed&quot;) names(req.content$data) ## [1] &quot;productCodeLong&quot; &quot;productCode&quot; ## [3] &quot;productCodePresentation&quot; &quot;productName&quot; ## [5] &quot;productDescription&quot; &quot;productStatus&quot; ## [7] &quot;productCategory&quot; &quot;productHasExpanded&quot; ## [9] &quot;productScienceTeamAbbr&quot; &quot;productScienceTeam&quot; ## [11] &quot;productPublicationFormatType&quot; &quot;productAbstract&quot; ## [13] &quot;productDesignDescription&quot; &quot;productStudyDescription&quot; ## [15] &quot;productBasicDescription&quot; &quot;productExpandedDescription&quot; ## [17] &quot;productSensor&quot; &quot;productRemarks&quot; ## [19] &quot;themes&quot; &quot;changeLogs&quot; ## [21] &quot;specs&quot; &quot;keywords&quot; ## [23] &quot;releases&quot; &quot;siteCodes&quot; You can see all of the information by running the line print(req.content), but this will result in a very long printout in your console. Instead, you can view list items individually. Here, we highlight a couple of interesting examples: # View Abstract req.content$data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see protocol [NEON.DOC.014041](http://data.neonscience.org/api/v0/documents/NEON.DOC.014041vF): TOS Protocol and Procedure: Breeding Landbird Abundance and Diversity and science design [NEON.DOC.000916](http://data.neonscience.org/api/v0/documents/NEON.DOC.000916vB): TOS Science Design for Breeding Landbird Abundance and Diversity.\\n\\nLatency: The expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; # View Available months and associated URLs for Onaqui, Utah - ONAQ req.content$data$siteCodes[[27]] ## $siteCode ## [1] &quot;ONAQ&quot; ## ## $availableMonths ## $availableMonths[[1]] ## [1] &quot;2017-05&quot; ## ## $availableMonths[[2]] ## [1] &quot;2018-05&quot; ## ## $availableMonths[[3]] ## [1] &quot;2018-06&quot; ## ## $availableMonths[[4]] ## [1] &quot;2019-05&quot; ## ## $availableMonths[[5]] ## [1] &quot;2020-05&quot; ## ## $availableMonths[[6]] ## [1] &quot;2021-06&quot; ## ## ## $availableDataUrls ## $availableDataUrls[[1]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05&quot; ## ## $availableDataUrls[[2]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05&quot; ## ## $availableDataUrls[[3]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06&quot; ## ## $availableDataUrls[[4]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05&quot; ## ## $availableDataUrls[[5]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2020-05&quot; ## ## $availableDataUrls[[6]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2021-06&quot; ## ## ## $availableReleases ## $availableReleases[[1]] ## $availableReleases[[1]]$release ## [1] &quot;PROVISIONAL&quot; ## ## $availableReleases[[1]]$availableMonths ## $availableReleases[[1]]$availableMonths[[1]] ## [1] &quot;2021-06&quot; ## ## ## ## $availableReleases[[2]] ## $availableReleases[[2]]$release ## [1] &quot;RELEASE-2022&quot; ## ## $availableReleases[[2]]$availableMonths ## $availableReleases[[2]]$availableMonths[[1]] ## [1] &quot;2017-05&quot; ## ## $availableReleases[[2]]$availableMonths[[2]] ## [1] &quot;2018-05&quot; ## ## $availableReleases[[2]]$availableMonths[[3]] ## [1] &quot;2018-06&quot; ## ## $availableReleases[[2]]$availableMonths[[4]] ## [1] &quot;2019-05&quot; ## ## $availableReleases[[2]]$availableMonths[[5]] ## [1] &quot;2020-05&quot; To get a more accessible view of which sites have data for which months, youll need to extract data from the nested list. There are a variety of ways to do this, in this tutorial well explore a couple of them. Here well use fromJSON(), in the jsonlite package, which doesnt fully flatten the nested list, but gets us the part we need. To use it, we need a text version of the content. The text version is not as human readable but is readable by the fromJSON() function. # make this JSON readable -&gt; &quot;text&quot; req.text &lt;- content(req, as=&quot;text&quot;) # Flatten data frame to see available data. avail &lt;- jsonlite::fromJSON(req.text, simplifyDataFrame=T, flatten=T) avail ## $data ## $data$productCodeLong ## [1] &quot;NEON.DOM.SITE.DP1.10003.001&quot; ## ## $data$productCode ## [1] &quot;DP1.10003.001&quot; ## ## $data$productCodePresentation ## [1] &quot;NEON.DP1.10003&quot; ## ## $data$productName ## [1] &quot;Breeding landbird point counts&quot; ## ## $data$productDescription ## [1] &quot;Count, distance from observer, and taxonomic identification of breeding landbirds observed during point counts&quot; ## ## $data$productStatus ## [1] &quot;ACTIVE&quot; ## ## $data$productCategory ## [1] &quot;Level 1 Data Product&quot; ## ## $data$productHasExpanded ## [1] TRUE ## ## $data$productScienceTeamAbbr ## [1] &quot;TOS&quot; ## ## $data$productScienceTeam ## [1] &quot;Terrestrial Observation System (TOS)&quot; ## ## $data$productPublicationFormatType ## [1] &quot;TOS Data Product Type&quot; ## ## $data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see protocol [NEON.DOC.014041](http://data.neonscience.org/api/v0/documents/NEON.DOC.014041vF): TOS Protocol and Procedure: Breeding Landbird Abundance and Diversity and science design [NEON.DOC.000916](http://data.neonscience.org/api/v0/documents/NEON.DOC.000916vB): TOS Science Design for Breeding Landbird Abundance and Diversity.\\n\\nLatency: The expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; ## ## $data$productDesignDescription ## [1] &quot;Depending on the size of the site, sampling for this product occurs at either randomly distributed individual points or grids of nine points each. At larger sites, point count sampling occurs at five to ten 9-point grids, with grid centers collocated with distributed base plot centers (where plant, beetle, and/or soil sampling may also occur), if possible. At smaller sites (i.e., sites that cannot accommodate a minimum of 5 grids) point counts occur at the southwest corner (point 21) of 5-25 distributed base plots. Point counts are conducted once per breeding season at large sites and twice per breeding season at smaller sites. Point counts are six minutes long, with each minute tracked by the observer, following a two-minute settling-in period. All birds are recorded to species and sex, whenever possible, and the distance to each individual or flock is measured with a laser rangefinder, except in the case of flyovers.&quot; ## ## $data$productStudyDescription ## [1] &quot;This sampling occurs at all NEON terrestrial sites.&quot; ## ## $data$productBasicDescription ## [1] &quot;The basic package contains the per point metadata table that includes data pertaining to the observer and the weather conditions and the count data table that includes all of the observational data.&quot; ## ## $data$productExpandedDescription ## [1] &quot;The expanded package includes two additional tables and two additional fields within the count data table. The personnel table provides institutional information about each observer, as well as their performance on identification quizzes, where available. The references tables provides the list of resources used by an observer to identify birds. The additional fields in the countdata table are family and nativeStatusCode, which are derived from the NEON master list of birds.&quot; ## ## $data$productSensor ## [1] &quot;&quot; ## ## $data$productRemarks ## [1] &quot;Queries for this data product will return data collected during the date range specified for `brd_perpoint` and `brd_countdata`, but will return data from all dates for `brd_personnel` (quiz scores may occur over time periods which are distinct from when sampling occurs) and `brd_references` (which apply to a broad range of sampling dates). A record from `brd_perPoint` should have 6+ child records in `brd_countdata`, at least one per pointCountMinute. Duplicates or missing data may exist where protocol and/or data entry aberrations have occurred; users should check data carefully for anomalies before joining tables. Taxonomic IDs of species of concern have been &#39;fuzzed&#39;; see data package readme files for more information.&quot; ## ## $data$themes ## [1] &quot;Organisms, Populations, and Communities&quot; ## ## $data$changeLogs ## id parentIssueID issueDate resolvedDate ## 1 16607 NA 2020-10-28T00:00:00Z 2020-01-01T00:00:00Z ## 2 17938 NA 2021-01-06T00:00:00Z 2021-12-31T00:00:00Z ## 3 25404 NA 2021-06-24T00:00:00Z 2018-03-28T00:00:00Z ## 4 37004 NA 2021-12-03T00:00:00Z 2021-12-31T00:00:00Z ## 5 38204 NA 2021-12-09T00:00:00Z 2021-12-31T00:00:00Z ## 6 67504 NA 2022-09-13T00:00:00Z &lt;NA&gt; ## 7 67505 NA 2022-09-13T00:00:00Z &lt;NA&gt; ## dateRangeStart dateRangeEnd locationAffected ## 1 2013-01-01T00:00:00Z 2020-01-01T00:00:00Z All ## 2 2020-03-23T00:00:00Z 2021-12-31T00:00:00Z All ## 3 2012-01-01T00:00:00Z 2018-03-28T00:00:00Z All ## 4 2015-01-01T00:00:00Z 2021-12-31T00:00:00Z All ## 5 2012-01-01T00:00:00Z 2021-12-31T00:00:00Z All ## 6 2020-03-23T00:00:00Z 2022-12-31T00:00:00Z TOOL ## 7 2022-06-12T00:00:00Z 2022-12-31T00:00:00Z YELL ## issue ## 1 There was not a way to indicate that a scheduled sampling event did not occur. ## 2 Safety measures to protect personnel during the COVID-19 pandemic resulted in reduced or canceled sampling activities for extended periods at NEON sites. Data availability may be reduced during this time. ## 3 It was too expensive to have a maximum of 15 grids per site. ## 4 EventIDs in `brd_perpoint` and `brd_countdata` records didn&#39;t always match. ## 5 Prior to the 2022 data release, publication of species identifications were obfuscated to a higher taxonomic rank when the taxon was found to be listed as threatened, endangered, or sensitive at the state level where the observation was recorded. Obfuscating state-listed taxa has created challenges for data users studying biodiversity. ## 6 Toolik Field Station required a quarantine period prior to starting work in the 2020, 2021, and 2022 field seasons to protect all personnel during the COVID-19 pandemic. This complicated NEON field scheduling logistics, which typically involves repeated travel across the state on short time frames. Consequently, NEON reduced staffing traveling to Toolik and was thus unable to complete all planned sampling efforts. Missed data collection events are indicated in data records via the samplingImpractical field. ## 7 Severe flooding destroyed several roads into Yellowstone National Park in June 2022, making the YELL and BLDE sites inaccessible to NEON staff. Observational data collection was halted during this time. Canceled data collection events are indicated in data records via the samplingImpractical field. ## resolution ## 1 The fields samplingImpracticalRemarks and samplingImpractical were added prior to the 2020 field season. The contractor supplies the samplingImpracticalRemarks field, and this field autopopulates the samplingImpractical field. The samplingImpractical field has a value other than OK if something prevented sampling from occurring. ## 2 The primary impact of the pandemic on observational data was reduced data collection. Training procedures and data quality reviews were maintained throughout the pandemic, although some previously in-person training was conducted virtually. Scheduled measurements and sampling that were not carried out due to COVID-19 or any other causes are indicated in data records via the samplingImpractical data field. ## 3 In version J of the bird protocol NEON reduced the maximum number grids per site from 15 to 10. ## 4 The inconsistent time zone issue within the date component of the eventID was resolved by stripping time of day from the eventID. ## 5 The state-level obfuscation routine was removed from the data publication process at all locations excluding sites located in D01 and D20. Data have been reprocessed to remove the obfuscation of state-listed taxa. Federally listed threatened and endangered or sensitive species remain obfuscated at all sites and sensitive species remain redacted at National Park sites. ## 6 ## 7 ## ## $data$specs ## specId specNumber specType specSize ## 1 5844 NEON.DOC.014041vK application/pdf 3778463 ## 2 7294 NEON.DOC.000916vD application/pdf 1654075 ## 3 7352 NEON_bird_userGuide_vC application/pdf 297054 ## 4 QSG:165 NEON.QSG.DP1.10003.001v1 application/pdf 241528 ## specDescription ## 1 TOS Protocol and Procedure: BRD  Breeding Landbird Abundance and Diversity ## 2 TOS Science Design for Breeding Landbird Abundance and Diversity ## 3 NEON User Guide to Breeding Landbird Point Counts (DP1.10003.001) ## 4 Quick Start Guide for Breeding landbird point counts (DP1.10003.001) ## specUrl ## 1 https://data.neonscience.org/api/v0/documents/NEON.DOC.014041vK ## 2 https://data.neonscience.org/api/v0/documents/NEON.DOC.000916vD ## 3 https://data.neonscience.org/api/v0/documents/NEON_bird_userGuide_vC ## 4 https://data.neonscience.org/api/v0/documents/quick-start-guides/NEON.QSG.DP1.10003.001v1 ## ## $data$keywords ## [1] &quot;vertebrates&quot; &quot;diversity&quot; &quot;species composition&quot; ## [4] &quot;point counts&quot; &quot;landbirds&quot; &quot;invasive&quot; ## [7] &quot;distance sampling&quot; &quot;Aves&quot; &quot;animals&quot; ## [10] &quot;birds&quot; &quot;avian&quot; &quot;Chordata&quot; ## [13] &quot;Animalia&quot; &quot;taxonomy&quot; &quot;population&quot; ## [16] &quot;introduced&quot; &quot;community composition&quot; &quot;native&quot; ## ## $data$releases ## release generationDate ## 1 RELEASE-2021 2021-01-23T02:30:02Z ## 2 RELEASE-2022 2022-01-20T17:39:46Z ## url ## 1 https://data.neonscience.org/api/v0/releases/RELEASE-2021 ## 2 https://data.neonscience.org/api/v0/releases/RELEASE-2022 ## productDoi.generationDate productDoi.url ## 1 2021-01-25T18:14:30Z https://doi.org/10.48443/s730-dy13 ## 2 2022-01-21T02:53:07Z https://doi.org/10.48443/88sy-ah40 ## ## $data$siteCodes ## siteCode ## 1 ABBY ## 2 BARR ## 3 BART ## 4 BLAN ## 5 BONA ## 6 CLBJ ## 7 CPER ## 8 DCFS ## 9 DEJU ## 10 DELA ## 11 DSNY ## 12 GRSM ## 13 GUAN ## 14 HARV ## 15 HEAL ## 16 JERC ## 17 JORN ## 18 KONA ## 19 KONZ ## 20 LAJA ## 21 LENO ## 22 MLBS ## 23 MOAB ## 24 NIWO ## 25 NOGP ## 26 OAES ## 27 ONAQ ## 28 ORNL ## 29 OSBS ## 30 PUUM ## 31 RMNP ## 32 SCBI ## 33 SERC ## 34 SJER ## 35 SOAP ## 36 SRER ## 37 STEI ## 38 STER ## 39 TALL ## 40 TEAK ## 41 TOOL ## 42 TREE ## 43 UKFS ## 44 UNDE ## 45 WOOD ## 46 WREF ## 47 YELL ## availableMonths ## 1 2017-05, 2017-06, 2018-06, 2018-07, 2019-05, 2020-06, 2021-05 ## 2 2017-07, 2018-07, 2019-06, 2020-07, 2021-06 ## 3 2015-06, 2016-06, 2017-06, 2018-06, 2019-06, 2020-06, 2020-07, 2021-06 ## 4 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-06, 2021-05 ## 5 2017-06, 2018-06, 2018-07, 2019-06, 2020-06, 2020-07, 2021-06 ## 6 2017-05, 2018-04, 2019-04, 2019-05, 2020-04, 2020-05, 2021-04, 2021-05 ## 7 2013-06, 2015-05, 2016-05, 2017-05, 2017-06, 2018-05, 2019-06, 2020-05, 2021-05, 2021-06 ## 8 2017-06, 2017-07, 2018-07, 2019-06, 2019-07, 2020-07, 2021-07 ## 9 2017-06, 2018-06, 2019-06, 2020-06, 2021-06 ## 10 2015-06, 2017-06, 2018-05, 2019-06, 2020-05, 2021-05 ## 11 2015-06, 2016-05, 2017-05, 2018-05, 2019-05, 2020-05, 2021-05 ## 12 2016-06, 2017-05, 2017-06, 2018-05, 2019-05, 2020-06, 2021-06 ## 13 2015-05, 2017-05, 2018-05, 2019-05, 2019-06, 2020-07, 2021-06 ## 14 2015-05, 2015-06, 2016-06, 2017-06, 2018-06, 2019-06, 2020-06, 2021-06 ## 15 2017-06, 2018-06, 2018-07, 2019-06, 2019-07, 2020-06, 2021-06 ## 16 2016-06, 2017-05, 2018-06, 2019-06, 2020-05, 2021-05 ## 17 2017-04, 2017-05, 2018-04, 2018-05, 2019-04, 2020-05, 2021-04, 2021-05 ## 18 2018-05, 2018-06, 2019-06, 2020-05, 2020-06, 2021-06 ## 19 2017-06, 2018-05, 2018-06, 2019-06, 2020-05, 2021-06 ## 20 2017-05, 2018-05, 2019-05, 2019-06, 2020-07, 2021-06 ## 21 2017-06, 2018-05, 2019-06, 2020-05, 2021-05 ## 22 2018-06, 2019-05, 2020-05, 2021-05, 2021-06 ## 23 2015-06, 2017-05, 2018-05, 2019-05, 2020-05, 2020-06, 2021-06 ## 24 2015-07, 2017-07, 2018-07, 2019-07, 2020-07, 2021-07 ## 25 2017-07, 2018-07, 2019-07, 2020-07, 2021-07 ## 26 2017-05, 2017-06, 2018-04, 2018-05, 2019-05, 2020-05, 2021-05 ## 27 2017-05, 2018-05, 2018-06, 2019-05, 2020-05, 2021-06 ## 28 2016-05, 2016-06, 2017-05, 2018-06, 2019-05, 2020-05, 2021-05, 2021-06 ## 29 2016-05, 2017-05, 2018-05, 2019-05, 2020-06, 2021-05 ## 30 2018-04, 2021-03 ## 31 2017-06, 2017-07, 2018-06, 2018-07, 2019-06, 2019-07, 2020-06, 2020-07, 2021-06, 2021-07 ## 32 2015-06, 2016-05, 2016-06, 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-05, 2020-06, 2021-05 ## 33 2017-05, 2017-06, 2018-05, 2019-05, 2020-05, 2020-06, 2021-06 ## 34 2017-04, 2018-04, 2019-04, 2020-04, 2021-04 ## 35 2017-05, 2018-05, 2019-05, 2020-05, 2021-05 ## 36 2017-05, 2018-04, 2018-05, 2019-04, 2020-04, 2021-04 ## 37 2016-05, 2016-06, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-06, 2021-05, 2021-06 ## 38 2013-06, 2015-05, 2016-05, 2017-05, 2018-05, 2019-05, 2019-06, 2020-06, 2021-05 ## 39 2015-06, 2016-07, 2017-06, 2018-06, 2019-05, 2020-05, 2020-06, 2021-05 ## 40 2017-06, 2018-06, 2019-06, 2019-07, 2020-06, 2021-06 ## 41 2017-06, 2018-07, 2019-06, 2020-06, 2021-06 ## 42 2016-06, 2017-06, 2018-06, 2019-06, 2020-06, 2021-06 ## 43 2017-06, 2018-06, 2019-06, 2020-05, 2020-06, 2021-06 ## 44 2016-06, 2016-07, 2017-06, 2018-06, 2019-06, 2020-06, 2021-06 ## 45 2015-07, 2017-07, 2018-07, 2019-06, 2019-07, 2020-07, 2021-07 ## 46 2018-06, 2019-05, 2019-06, 2020-06, 2021-05 ## 47 2018-06, 2019-06, 2020-06, 2021-07 ## availableDataUrls ## 1 https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2021-05 ## 2 https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2021-06 ## 3 https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2021-06 ## 4 https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2021-05 ## 5 https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2021-06 ## 6 https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2020-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2021-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2021-05 ## 7 https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2021-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2021-06 ## 8 https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2021-07 ## 9 https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2021-06 ## 10 https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2021-05 ## 11 https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2021-05 ## 12 https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2021-06 ## 13 https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2021-06 ## 14 https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2021-06 ## 15 https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2021-06 ## 16 https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2021-05 ## 17 https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2021-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2021-05 ## 18 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2021-06 ## 19 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2021-06 ## 20 https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2021-06 ## 21 https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2021-05 ## 22 https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2021-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2021-06 ## 23 https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2021-06 ## 24 https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2021-07 ## 25 https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2021-07 ## 26 https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2021-05 ## 27 https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2021-06 ## 28 https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2021-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2021-06 ## 29 https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2021-05 ## 30 https://data.neonscience.org/api/v0/data/DP1.10003.001/PUUM/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/PUUM/2021-03 ## 31 https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2021-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2021-07 ## 32 https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2021-05 ## 33 https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2021-06 ## 34 https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2020-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2021-04 ## 35 https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2021-05 ## 36 https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2020-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2021-04 ## 37 https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2021-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2021-06 ## 38 https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2021-05 ## 39 https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2021-05 ## 40 https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2021-06 ## 41 https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2021-06 ## 42 https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2021-06 ## 43 https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2020-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2021-06 ## 44 https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2021-06 ## 45 https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2020-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2021-07 ## 46 https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2021-05 ## 47 https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2020-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2021-07 ## availableReleases ## 1 PROVISIONAL, RELEASE-2022, 2020-06, 2021-05, 2017-05, 2017-06, 2018-06, 2018-07, 2019-05 ## 2 PROVISIONAL, RELEASE-2022, 2020-07, 2021-06, 2017-07, 2018-07, 2019-06 ## 3 PROVISIONAL, RELEASE-2022, 2021-06, 2015-06, 2016-06, 2017-06, 2018-06, 2019-06, 2020-06, 2020-07 ## 4 PROVISIONAL, RELEASE-2022, 2021-05, 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-06 ## 5 PROVISIONAL, RELEASE-2022, 2021-06, 2017-06, 2018-06, 2018-07, 2019-06, 2020-06, 2020-07 ## 6 PROVISIONAL, RELEASE-2022, 2021-04, 2021-05, 2017-05, 2018-04, 2019-04, 2019-05, 2020-04, 2020-05 ## 7 PROVISIONAL, RELEASE-2022, 2021-05, 2021-06, 2013-06, 2015-05, 2016-05, 2017-05, 2017-06, 2018-05, 2019-06, 2020-05 ## 8 PROVISIONAL, RELEASE-2022, 2021-07, 2017-06, 2017-07, 2018-07, 2019-06, 2019-07, 2020-07 ## 9 PROVISIONAL, RELEASE-2022, 2021-06, 2017-06, 2018-06, 2019-06, 2020-06 ## 10 PROVISIONAL, RELEASE-2022, 2021-05, 2015-06, 2017-06, 2018-05, 2019-06, 2020-05 ## 11 PROVISIONAL, RELEASE-2022, 2020-05, 2021-05, 2015-06, 2016-05, 2017-05, 2018-05, 2019-05 ## 12 PROVISIONAL, RELEASE-2022, 2021-06, 2016-06, 2017-05, 2017-06, 2018-05, 2019-05, 2020-06 ## 13 PROVISIONAL, RELEASE-2022, 2021-06, 2015-05, 2017-05, 2018-05, 2019-05, 2019-06, 2020-07 ## 14 PROVISIONAL, RELEASE-2022, 2021-06, 2015-05, 2015-06, 2016-06, 2017-06, 2018-06, 2019-06, 2020-06 ## 15 PROVISIONAL, RELEASE-2022, 2021-06, 2017-06, 2018-06, 2018-07, 2019-06, 2019-07, 2020-06 ## 16 PROVISIONAL, RELEASE-2022, 2021-05, 2016-06, 2017-05, 2018-06, 2019-06, 2020-05 ## 17 PROVISIONAL, RELEASE-2022, 2021-04, 2021-05, 2017-04, 2017-05, 2018-04, 2018-05, 2019-04, 2020-05 ## 18 PROVISIONAL, RELEASE-2022, 2021-06, 2018-05, 2018-06, 2019-06, 2020-05, 2020-06 ## 19 PROVISIONAL, RELEASE-2022, 2021-06, 2017-06, 2018-05, 2018-06, 2019-06, 2020-05 ## 20 PROVISIONAL, RELEASE-2022, 2021-06, 2017-05, 2018-05, 2019-05, 2019-06, 2020-07 ## 21 PROVISIONAL, RELEASE-2022, 2021-05, 2017-06, 2018-05, 2019-06, 2020-05 ## 22 PROVISIONAL, RELEASE-2022, 2021-05, 2021-06, 2018-06, 2019-05, 2020-05 ## 23 PROVISIONAL, RELEASE-2022, 2021-06, 2015-06, 2017-05, 2018-05, 2019-05, 2020-05, 2020-06 ## 24 PROVISIONAL, RELEASE-2022, 2021-07, 2015-07, 2017-07, 2018-07, 2019-07, 2020-07 ## 25 PROVISIONAL, RELEASE-2022, 2021-07, 2017-07, 2018-07, 2019-07, 2020-07 ## 26 PROVISIONAL, RELEASE-2022, 2021-05, 2017-05, 2017-06, 2018-04, 2018-05, 2019-05, 2020-05 ## 27 PROVISIONAL, RELEASE-2022, 2021-06, 2017-05, 2018-05, 2018-06, 2019-05, 2020-05 ## 28 PROVISIONAL, RELEASE-2022, 2021-05, 2021-06, 2016-05, 2016-06, 2017-05, 2018-06, 2019-05, 2020-05 ## 29 PROVISIONAL, RELEASE-2022, 2021-05, 2016-05, 2017-05, 2018-05, 2019-05, 2020-06 ## 30 PROVISIONAL, RELEASE-2022, 2021-03, 2018-04 ## 31 PROVISIONAL, RELEASE-2022, 2021-06, 2021-07, 2017-06, 2017-07, 2018-06, 2018-07, 2019-06, 2019-07, 2020-06, 2020-07 ## 32 PROVISIONAL, RELEASE-2022, 2021-05, 2015-06, 2016-05, 2016-06, 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-05, 2020-06 ## 33 PROVISIONAL, RELEASE-2022, 2021-06, 2017-05, 2017-06, 2018-05, 2019-05, 2020-05, 2020-06 ## 34 PROVISIONAL, RELEASE-2022, 2020-04, 2021-04, 2017-04, 2018-04, 2019-04 ## 35 PROVISIONAL, RELEASE-2022, 2020-05, 2021-05, 2017-05, 2018-05, 2019-05 ## 36 PROVISIONAL, RELEASE-2022, 2021-04, 2017-05, 2018-04, 2018-05, 2019-04, 2020-04 ## 37 PROVISIONAL, RELEASE-2022, 2021-05, 2021-06, 2016-05, 2016-06, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06, 2020-06 ## 38 PROVISIONAL, RELEASE-2022, 2021-05, 2013-06, 2015-05, 2016-05, 2017-05, 2018-05, 2019-05, 2019-06, 2020-06 ## 39 PROVISIONAL, RELEASE-2022, 2021-05, 2015-06, 2016-07, 2017-06, 2018-06, 2019-05, 2020-05, 2020-06 ## 40 PROVISIONAL, RELEASE-2022, 2020-06, 2021-06, 2017-06, 2018-06, 2019-06, 2019-07 ## 41 PROVISIONAL, RELEASE-2022, 2020-06, 2021-06, 2017-06, 2018-07, 2019-06 ## 42 PROVISIONAL, RELEASE-2022, 2021-06, 2016-06, 2017-06, 2018-06, 2019-06, 2020-06 ## 43 PROVISIONAL, RELEASE-2022, 2021-06, 2017-06, 2018-06, 2019-06, 2020-05, 2020-06 ## 44 PROVISIONAL, RELEASE-2022, 2021-06, 2016-06, 2016-07, 2017-06, 2018-06, 2019-06, 2020-06 ## 45 PROVISIONAL, RELEASE-2022, 2021-07, 2015-07, 2017-07, 2018-07, 2019-06, 2019-07, 2020-07 ## 46 PROVISIONAL, RELEASE-2022, 2020-06, 2021-05, 2018-06, 2019-05, 2019-06 ## 47 PROVISIONAL, RELEASE-2022, 2021-07, 2018-06, 2019-06, 2020-06 The object contains a lot of information about the data product, including: keywords under $data$keywords, references for documentation under $data$specs, data availability by site and month under $data$siteCodes, and specific URLs for the API calls for each site and month under $data$siteCodes$availableDataUrls. We need $data$siteCodes to tell us what we can download. $data$siteCodes$availableDataUrls allows us to avoid writing the API calls ourselves in the next steps. # get data availability list for the product bird.urls &lt;- unlist(avail$data$siteCodes$availableDataUrls) length(bird.urls) #total number of URLs ## [1] 314 bird.urls[1:10] #show first 10 URLs available ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05&quot; ## [2] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06&quot; ## [3] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06&quot; ## [4] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07&quot; ## [5] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05&quot; ## [6] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2020-06&quot; ## [7] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2021-05&quot; ## [8] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07&quot; ## [9] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07&quot; ## [10] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06&quot; These are the URLs showing us what files are available for each month where there are data. Lets look at the bird data from Woodworth (WOOD) site from July 2015. We can do this by using the above code but now specifying which site/date we want using the grep() function. Note that if there were only one month of data from a site, you could leave off the date in the function. If you want data from more than one site/month you need to iterate this code, GET fails if you give it more than one URL. # get data availability for WOOD July 2015 brd &lt;- GET(bird.urls[grep(&quot;WOOD/2015-07&quot;, bird.urls)]) brd.files &lt;- jsonlite::fromJSON(content(brd, as=&quot;text&quot;)) # view just the available data files brd.files$data$files ## name ## 1 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20211222T043153Z.csv ## 2 NEON.D09.WOOD.DP1.10003.001.variables.20211222T043153Z.csv ## 3 NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20211222T043153Z.csv ## 4 NEON.D09.WOOD.DP0.10003.001.categoricalCodes.20211222T043153Z.csv ## 5 NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.20211222T043153Z.csv ## 6 NEON.D09.WOOD.DP0.10003.001.validation.20211222T043153Z.csv ## 7 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20211222T043153Z.csv ## 8 NEON.D09.WOOD.DP1.10003.001.readme.20220120T173946Z.txt ## 9 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20220120T173946Z.xml ## 10 NEON.D09.WOOD.DP1.10003.001.readme.20220120T173946Z.txt ## 11 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20220120T173946Z.xml ## 12 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20211222T043153Z.csv ## 13 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20211222T043153Z.csv ## 14 NEON.D09.WOOD.DP0.10003.001.validation.20211222T043153Z.csv ## 15 NEON.D09.WOOD.DP0.10003.001.categoricalCodes.20211222T043153Z.csv ## 16 NEON.D09.WOOD.DP1.10003.001.variables.20211222T043153Z.csv ## size md5 crc32 crc32c ## 1 343356 5a199b5f04b6d8cf11c22840d82de24a NA NA ## 2 8670 437dd6434e1cd8538dc9676dcf6ca7f9 NA NA ## 3 1410 9835f082c527eb1b4b136b853ba9b074 NA NA ## 4 10522 1ccac9924bee55f42ca37e2d7d07746f NA NA ## 5 83076 31e36079cd2f74d0899cd01d835350f5 NA NA ## 6 11700 d1296d18260b4f83e8605ad078fbf967 NA NA ## 7 22623 3850ca4f9abcfc0544bc5722f1e07a1a NA NA ## 8 11152 2b6beae1df3c37a100bfae9f82bc93de NA NA ## 9 187787 345ae36c459945f243900995cd0d7f2f NA NA ## 10 10858 8c23069bf1ac232e89eaf4e8da8bf848 NA NA ## 11 170547 2237727df2560081f4f5e3f33e3c2ba5 NA NA ## 12 322242 fed00f0da15c2370158464c6c179a1c0 NA NA ## 13 22623 3850ca4f9abcfc0544bc5722f1e07a1a NA NA ## 14 11700 d1296d18260b4f83e8605ad078fbf967 NA NA ## 15 10522 1ccac9924bee55f42ca37e2d7d07746f NA NA ## 16 8670 437dd6434e1cd8538dc9676dcf6ca7f9 NA NA ## url ## 1 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20211222T043153Z.csv ## 2 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.variables.20211222T043153Z.csv ## 3 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20211222T043153Z.csv ## 4 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP0.10003.001.categoricalCodes.20211222T043153Z.csv ## 5 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.20211222T043153Z.csv ## 6 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP0.10003.001.validation.20211222T043153Z.csv ## 7 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20211222T043153Z.csv ## 8 https://storage.googleapis.com/neon-publication/release/tag/RELEASE-2022/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.readme.20220120T173946Z.txt ## 9 https://storage.googleapis.com/neon-publication/release/tag/RELEASE-2022/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20220120T173946Z.xml ## 10 https://storage.googleapis.com/neon-publication/release/tag/RELEASE-2022/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.readme.20220120T173946Z.txt ## 11 https://storage.googleapis.com/neon-publication/release/tag/RELEASE-2022/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20220120T173946Z.xml ## 12 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20211222T043153Z.csv ## 13 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20211222T043153Z.csv ## 14 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP0.10003.001.validation.20211222T043153Z.csv ## 15 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP0.10003.001.categoricalCodes.20211222T043153Z.csv ## 16 https://storage.googleapis.com/neon-publication/NEON.DOM.SITE.DP1.10003.001/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.variables.20211222T043153Z.csv In this output, name and url are key fields. It provides us with the names of the files available for this site and month, and URLs where we can get the files. Well use the file names to pick which ones we want. The available files include both data and metadata, and both the basic and expanded data packages. Typically the expanded package includes additional quality or uncertainty data, either in additional files or additional fields than in the basic files. Basic and expanded data packages are available for most NEON data products (some only have basic). Metadata are described by file name below. The format for most of the file names is: NEON.[domain number].[site code].[data product ID].[file-specific name]. [date of file creation] Some files omit the domain and site, since theyre not specific to a location, like the data product readme. The date of file creation uses the ISO6801 format, in this case 20170720T182547Z, and can be used to determine whether data have been updated since the last time you downloaded. Available files in our query for July 2015 at Woodworth are all of the following (leaving off the initial NEON.D09.WOOD.10003.001): ~.2015-07.expanded.20170720T182547Z.zip: zip of all files in the expanded package ~.brd_countdata.2015-07.expanded.20170720T182547Z.csv: count data table, expanded package version: counts of birds at each point ~.brd_perpoint.2015-07.expanded.20170720T182547Z.csv: point data table, expanded package version: metadata at each observation point NEON.Bird Conservancy of the Rockies.brd_personnel.csv: personnel data table, accuracy scores for bird observers ~.2015-07.basic.20170720T182547Z.zip: zip of all files in the basic package ~.brd_countdata.2015-07.basic.20170720T182547Z.csv: count data table, basic package version: counts of birds at each point ~.brd_perpoint.2015-07.basic.20170720T182547Z.csv: point data table, basic package version: metadata at each observation point NEON.DP1.10003.001_readme.txt: readme for the data product (not specific to dates or location). Appears twice in the list, since its in both the basic and expanded package ~.20150101-20160613.xml: Ecological Metadata Language (EML) file. Appears twice in the list, since its in both the basic and expanded package ~.validation.20170720T182547Z.csv: validation file for the data product, lists input data and data entry rules. Appears twice in the list, since its in both the basic and expanded package ~.variables.20170720T182547Z.csv: variables file for the data product, lists data fields in downloaded tables. Appears twice in the list, since its in both the basic and expanded package Well get the data tables for the point data and count data in the basic package. The list of files doesnt return in the same order every time, so we wont use position in the list to select. Plus, we want code we can re-use when getting data from other sites and other months. So we select files based on the data table name and the package name. # Get both files brd.count &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;countdata&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) brd.point &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;perpoint&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can access it in R. Just to show that the files we pulled have actual data in them, lets make a quick graphic: # Cluster by species clusterBySp &lt;- brd.count %&gt;% dplyr::group_by(scientificName) %&gt;% dplyr::summarise(total=sum(clusterSize, na.rm=T)) # Reorder so list is ordered most to least abundance clusterBySp &lt;- clusterBySp[order(clusterBySp$total, decreasing=T),] # Plot barplot(clusterBySp$total, names.arg=clusterBySp$scientificName, ylab=&quot;Total&quot;, cex.names=0.5, las=2) Wow! There are lots of Agelaius phoeniceus (Red-winged Blackbirds) at WOOD in July. 2.12.4 Instrumentation data (IS) The process is essentially the same for sensor data. Well do the same series of queries for Soil Temperature, DP1.00041.001. Lets use data from Moab in June 2017 this time. # Request soil temperature data availability info req.soil &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.00041.001&quot;) # make this JSON readable # Note how we&#39;ve change this from two commands into one here avail.soil &lt;- jsonlite::fromJSON(content(req.soil, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product temp.urls &lt;- unlist(avail.soil$data$siteCodes$availableDataUrls) # get data availability from location/date of interest tmp &lt;- GET(temp.urls[grep(&quot;MOAB/2017-06&quot;, temp.urls)]) tmp.files &lt;- jsonlite::fromJSON(content(tmp, as=&quot;text&quot;)) length(tmp.files$data$files$name) # There are a lot of available files ## [1] 188 tmp.files$data$files$name[1:10] # Let&#39;s print the first 10 ## [1] &quot;NEON.D13.MOAB.DP1.00041.001.variables.20211210T224045Z.csv&quot; ## [2] &quot;NEON.D13.MOAB.DP1.00041.001.003.505.030.ST_30_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [3] &quot;NEON.D13.MOAB.DP1.00041.001.001.505.030.ST_30_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [4] &quot;NEON.D13.MOAB.DP1.00041.001.004.509.001.ST_1_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [5] &quot;NEON.D13.MOAB.DP1.00041.001.003.508.001.ST_1_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [6] &quot;NEON.D13.MOAB.DP1.00041.001.004.503.001.ST_1_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [7] &quot;NEON.D13.MOAB.DP1.00041.001.003.505.001.ST_1_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [8] &quot;NEON.D13.MOAB.DP1.00041.001.002.508.030.ST_30_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [9] &quot;NEON.D13.MOAB.DP1.00041.001.005.504.030.ST_30_minute.2017-06.basic.20211210T224045Z.csv&quot; ## [10] &quot;NEON.D13.MOAB.DP1.00041.001.002.502.001.ST_1_minute.2017-06.basic.20211210T224045Z.csv&quot; These file names start and end the same way as the observational files, but the middle is a little more cryptic. The structure from beginning to end is: NEON.[domain number].[site code].[data product ID].00000. [soil plot number].[depth].[averaging interval].[data table name]. [year]-[month].[data package].[date of file creation] So NEON.D13.MOAB.DP1.00041.001.003.507.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv is the: NEON (NEON.) Domain 13 (.D13.) Moab field site (.MOAB.) soil temperature data (.DP1.00041.001.) collected in Soil Plot 2, (.002.) at the 7th depth below the surface (.507.) and reported as a 30-minute mean of (.030. and .ST_30_minute.) only for the period of June 2017 (.2017-06.) and provided in a expanded data package (.basic.) published on June 20th, 2020 (.0200620T070859Z.). More information about interpreting file names can be found in the readme that accompanies each download. Lets get data (and the URL) for only the 2nd depth described above by selecting 002.502.030 and the word basic in the file name. Go get it: soil.temp &lt;- read.delim(tmp.files$data$files$url [intersect(grep(&quot;002.502.030&quot;, tmp.files$data$files$name), grep(&quot;basic&quot;, tmp.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can use it to conduct our analyses. To take a quick look at it, lets plot the mean soil temperature by date. # plot temp ~ date plot(soil.temp$soilTempMean~as.POSIXct(soil.temp$startDateTime, format=&quot;%Y-%m-%d T %H:%M:%S Z&quot;), pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;T&quot;) As wed expect we see daily fluctuation in soil temperature. 2.12.5 Remote sensing data (AOP) Again, the process of determining which sites and time periods have data, and finding the URLs for those data, is the same as for the other data types. Well go looking for High resolution orthorectified camera imagery, DP1.30010.001, and well look at the flight over San Joaquin Experimental Range (SJER) in March 2017. # Request camera data availability info req.aop &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.30010.001&quot;) # make this JSON readable # Note how we&#39;ve changed this from two commands into one here avail.aop &lt;- jsonlite::fromJSON(content(req.aop, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product cam.urls &lt;- unlist(avail.aop$data$siteCodes$availableDataUrls) # get data availability from location/date of interest cam &lt;- GET(cam.urls[intersect(grep(&quot;SJER&quot;, cam.urls), grep(&quot;2017&quot;, cam.urls))]) cam.files &lt;- jsonlite::fromJSON(content(cam, as=&quot;text&quot;)) # this list of files is very long, so we&#39;ll just look at the first ten head(cam.files$data$files$name, 10) ## [1] &quot;17032816_EH021656(20170328193532)-0922_ort.tif&quot; ## [2] &quot;17032816_EH021656(20170328193032)-0877_ort.tif&quot; ## [3] &quot;17032816_EH021656(20170328185340)-0573_ort.tif&quot; ## [4] &quot;17032816_EH021656(20170328174604)-0014_ort.tif&quot; ## [5] &quot;17032816_EH021656(20170328195216)-1070_ort.tif&quot; ## [6] &quot;17032816_EH021656(20170328192011)-0803_ort.tif&quot; ## [7] &quot;17032816_EH021656(20170328182204)-0324_ort.tif&quot; ## [8] &quot;17032816_EH021656(20170328181633)-0279_ort.tif&quot; ## [9] &quot;17032816_EH021656(20170328183509)-0428_ort.tif&quot; ## [10] &quot;17032816_EH021656(20170328182306)-0337_ort.tif&quot; File names for AOP data are more variable than for IS or OS data; different AOP data products use different naming conventions. File formats differ by product as well. This particular product, camera imagery, is stored in TIFF files. Instead of reading a TIFF into R, well download it to the working directory. This is one option for getting AOP files from the API. To download the TIFF file, we use the downloader package, and well select a file based on the time stamp in the file name: 20170328192931 download(cam.files$data$files$url[grep(&quot;20170328192931&quot;, cam.files$data$files$name)], paste(getwd(), &quot;/SJER_image.tif&quot;, sep=&quot;&quot;), mode=&quot;wb&quot;) The image, below, of the San Joaquin Experimental Range should now be in your working directory. An example of camera data (DP1.30010.001) from the San Joaquin Experimental Range. Source: National Ecological Observatory Network (NEON) 2.12.6 Geolocation data You may have noticed some of the spatial data referenced above are a bit vague, e.g. soil plot 2, 4th depth below the surface. This section describes how to get spatial data and what to do with it depends on which type of data youre working with. 2.12.6.1 Instrumentation data (both aquatic and terrestrial) Downloads of instrument system (IS) data include a file called sensor_positions.csv. The sensor positions file contains information about the coordinates of each sensor, relative to a reference location. While the specifics vary, techniques are generalizable for working with sensor data and the sensor_positions.csv file. Lets look at the sensor locations for photosynthetically active radiation (PAR; DP1.00024.001) at the NEON Treehaven site (TREE) in July 2018. To reduce our file size, well use the 30 minute averaging interval. Our final product from this section is to create a spatially explicit picture of light attenuation through the canopy. # load PAR data of interest par &lt;- loadByProduct(dpID=&quot;DP1.00024.001&quot;, site=&quot;TREE&quot;, startdate=&quot;2018-07&quot;, enddate=&quot;2018-07&quot;, avg=30, check.size=F, token=NEON_TOKEN) ## Input parameter avg is deprecated; use timeIndex to download by time interval. ## Finding available files ## | | | 0% | |======================================================================| 100% ## ## Downloading files totaling approximately 0.963266 MB ## Downloading 9 files ## | | | 0% | |========= | 12% | |================== | 25% | |========================== | 38% | |=================================== | 50% | |============================================ | 62% | |==================================================== | 75% | |============================================================= | 88% | |======================================================================| 100% ## ## Stacking operation across a single core. ## Stacking table PARPAR_30min ## Merged the most recent publication of sensor position files for each site and saved to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 1 data tables and 3 metadata tables! ## Stacking took 0.370913 secs Now we can specifically look at the sensor positions file: # create object for sens. pos. file pos &lt;- par$sensor_positions_00024 # view names names(pos) ## [1] &quot;siteID&quot; &quot;HOR.VER&quot; &quot;name&quot; ## [4] &quot;description&quot; &quot;start&quot; &quot;end&quot; ## [7] &quot;referenceName&quot; &quot;referenceDescription&quot; &quot;referenceStart&quot; ## [10] &quot;referenceEnd&quot; &quot;xOffset&quot; &quot;yOffset&quot; ## [13] &quot;zOffset&quot; &quot;pitch&quot; &quot;roll&quot; ## [16] &quot;azimuth&quot; &quot;referenceLatitude&quot; &quot;referenceLongitude&quot; ## [19] &quot;referenceElevation&quot; &quot;eastOffset&quot; &quot;northOffset&quot; ## [22] &quot;xAzimuth&quot; &quot;yAzimuth&quot; &quot;publicationDate&quot; The sensor locations are indexed by the HOR.VER variable - see the file naming conventions page for more details. Using unique() we can view all the locations indexes in this file. # view names unique(pos$HOR.VER) ## [1] &quot;000.010&quot; &quot;000.020&quot; &quot;000.030&quot; &quot;000.040&quot; &quot;000.050&quot; &quot;000.060&quot; PAR data are collected at multiple levels of the NEON tower but along a single vertical plane. We see this reflected in the data where HOR=000 (all data collected) at the tower location. The VER index varies (VER = 010 to 060) showing that the vertical position is changing and that PAR is measured at six different levels. The x, y, and z offsets in the sensor positions file are the relative distance, in meters, to the reference latitude, longitude, and elevation in the file. The HOR and VER indices in the sensor positions file correspond to the verticalPosition and horizontalPosition fields in par$PARPAR_30min. Say we wanted to plot a profile of the PAR through the canopy, we would need to start by using the aggregate() function to calculate mean PAR at each vertical position on the tower over the month: # calc mean PAR at each level parMean &lt;- aggregate(par$PARPAR_30min$PARMean, by=list(par$PARPAR_30min$verticalPosition), FUN=mean, na.rm=T) Now we can plot mean PAR relative to height on the tower (or the zOffset): # plot PAR plot(parMean$x, parMean$Group.1, type=&quot;b&quot;, pch=20, xlab=&quot;Photosynthetically active radiation&quot;, ylab=&quot;Height above tower base (m)&quot;) 2.12.6.2 Observational data - Terrestrial Latitude, longitude, elevation, and associated uncertainties are included in data downloads (Remember NEON COding Lab part 1?). These are the coordinates and uncertainty of the sampling plot; for many protocols it is possible to calculate a more precise location. Instructions for doing this are in the respective data product user guides, and code is in the geoNEON package on GitHub. 2.12.7 Querying a single named location Lets look at the named locations in the bird data we downloaded above. To do this, look for the field called namedLocation, which is present in all observational data products, both aquatic and terrestrial. # view named location head(brd.point$namedLocation) ## [1] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; ## [4] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; Here we see the first six entries in the namedLocation column which tells us the names of the Terrestrial Observation plots where the bird surveys were conducted. We can query the locations endpoint of the API for the first named location, WOOD_013.birdGrid.brd. # location data req.loc &lt;- GET(&quot;http://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd&quot;) # make this JSON readable brd.WOOD_013 &lt;- jsonlite::fromJSON(content(req.loc, as=&quot;text&quot;)) brd.WOOD_013 ## $data ## $data$locationName ## [1] &quot;WOOD_013.birdGrid.brd&quot; ## ## $data$locationDescription ## [1] &quot;Plot \\&quot;WOOD_013\\&quot; at site \\&quot;WOOD\\&quot;&quot; ## ## $data$locationType ## [1] &quot;OS Plot - brd&quot; ## ## $data$domainCode ## [1] &quot;D09&quot; ## ## $data$siteCode ## [1] &quot;WOOD&quot; ## ## $data$locationDecimalLatitude ## [1] 47.13912 ## ## $data$locationDecimalLongitude ## [1] -99.23243 ## ## $data$locationElevation ## [1] 579.31 ## ## $data$locationUtmEasting ## [1] 482375.7 ## ## $data$locationUtmNorthing ## [1] 5220650 ## ## $data$locationUtmHemisphere ## [1] &quot;N&quot; ## ## $data$locationUtmZone ## [1] 14 ## ## $data$alphaOrientation ## [1] 0 ## ## $data$betaOrientation ## [1] 0 ## ## $data$gammaOrientation ## [1] 0 ## ## $data$xOffset ## [1] 0 ## ## $data$yOffset ## [1] 0 ## ## $data$zOffset ## [1] 0 ## ## $data$offsetLocation ## NULL ## ## $data$locationProperties ## locationPropertyName locationPropertyValue ## 1 Value for Coordinate source GeoXH 6000 ## 2 Value for Coordinate uncertainty 0.28 ## 3 Value for Country unitedStates ## 4 Value for County Stutsman ## 5 Value for Elevation uncertainty 0.48 ## 6 Value for Filtered positions 121 ## 7 Value for Geodetic datum WGS84 ## 8 Value for Horizontal dilution of precision 1 ## 9 Value for Maximum elevation 579.31 ## 10 Value for Minimum elevation 569.79 ## 11 Value for National Land Cover Database (2001) grasslandHerbaceous ## 12 Value for Plot dimensions 500m x 500m ## 13 Value for Plot ID WOOD_013 ## 14 Value for Plot size 250000 ## 15 Value for Plot subtype birdGrid ## 16 Value for Plot type distributed ## 17 Value for Positional dilution of precision 2.4 ## 18 Value for Reference Point Position B2 ## 19 Value for Slope aspect 238.91 ## 20 Value for Slope gradient 2.83 ## 21 Value for Soil type order Mollisols ## 22 Value for State province ND ## 23 Value for Subtype Specification ninePoints ## 24 Value for UTM Zone 14N ## ## $data$locationParent ## [1] &quot;WOOD&quot; ## ## $data$locationParentUrl ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD&quot; ## ## $data$locationChildren ## [1] &quot;WOOD_013.birdGrid.brd.C3&quot; &quot;WOOD_013.birdGrid.brd.B2&quot; ## [3] &quot;WOOD_013.birdGrid.brd.A3&quot; &quot;WOOD_013.birdGrid.brd.A2&quot; ## [5] &quot;WOOD_013.birdGrid.brd.A1&quot; &quot;WOOD_013.birdGrid.brd.C2&quot; ## [7] &quot;WOOD_013.birdGrid.brd.B1&quot; &quot;WOOD_013.birdGrid.brd.B3&quot; ## [9] &quot;WOOD_013.birdGrid.brd.C1&quot; ## ## $data$locationChildrenUrls ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C3&quot; ## [2] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B2&quot; ## [3] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A3&quot; ## [4] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A2&quot; ## [5] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A1&quot; ## [6] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C2&quot; ## [7] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B1&quot; ## [8] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B3&quot; ## [9] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C1&quot; Note spatial information under $data$[nameOfCoordinate] and under $data$locationProperties. Also note $data$locationChildren: these are the finer scale locations that can be used to calculate precise spatial data for bird observations. For convenience, well use the geoNEON package to make the calculations. First well use getLocByName() to get the additional spatial information available through the API, and look at the spatial resolution available in the initial download: # load the geoNEON package library(geoNEON) # extract the spatial data brd.point.loc &lt;- getLocByName(brd.point) ## | | | 0% | |========== | 14% | |==================== | 29% | |============================== | 43% | |======================================== | 57% | |================================================== | 71% | |============================================================ | 86% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.loc$easting, brd.point.loc$northing, circles=brd.point.loc$coordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) And use getLocTOS() to calculate the point locations of observations. brd.point.pt &lt;- getLocTOS(brd.point, &quot;brd_perpoint&quot;) ## | | | 0% | |= | 2% | |== | 3% | |=== | 5% | |==== | 6% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 16% | |============ | 17% | |============= | 19% | |============== | 21% | |================ | 22% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 29% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 46% | |================================= | 48% | |================================== | 49% | |==================================== | 51% | |===================================== | 52% | |====================================== | 54% | |======================================= | 56% | |======================================== | 57% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 62% | |============================================ | 63% | |============================================== | 65% | |=============================================== | 67% | |================================================ | 68% | |================================================= | 70% | |================================================== | 71% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 78% | |======================================================== | 79% | |========================================================= | 81% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================== | 94% | |=================================================================== | 95% | |==================================================================== | 97% | |===================================================================== | 98% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.pt$adjEasting, brd.point.pt$adjNorthing, circles=brd.point.pt$adjCoordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) Now you can see the individual points where the respective point counts were located. 2.12.8 Taxonomy NEON maintains accepted taxonomies for many of the taxonomic identification data we collect. NEON taxonomies are available for query via the API; they are also provided via an interactive user interface, the Taxon Viewer. NEON taxonomy data provides the reference information for how NEON validates taxa; an identification must appear in the taxonomy lists in order to be accepted into the NEON database. Additions to the lists are reviewed regularly. The taxonomy lists also provide the author of the scientific name, and the reference text used. The taxonomy endpoint of the API works a little bit differently from the other endpoints. In the Anatomy of an API Call section above, each endpoint has a single type of target - a data product number, a named location name, etc. For taxonomic data, there are multiple query options, and some of them can be used in combination. For example, a query for taxa in the Pinaceae family: http://data.neonscience.org/api/v0/taxonomy/?family=Pinaceae The available types of queries are listed in the taxonomy section of the API web page. Briefly, they are: taxonTypeCode: Which of the taxonomies maintained by NEON are you looking for? BIRD, FISH, PLANT, etc. Cannot be used in combination with the taxonomic rank queries. each of the major taxonomic ranks from genus through kingdom scientificname: Genus + specific epithet (+ authority). Search is by exact match only, see final example below. verbose: Do you want the short (false) or long (true) response offset: Skip this number of items in the list. Defaults to 50. limit: Result set will be truncated at this length. Defaults to Staff on the NEON project have plans to modify the settings for offset and limit, such that offset will default to 0 and limit will default to , but in the meantime users will want to set these manually. They are set to non-default values in the examples below. For the first example, lets query for the loon family, Gaviidae, in the bird taxonomy. Note that query parameters are case-sensitive. loon.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?family=Gaviidae&amp;offset=0&amp;limit=500&quot;) Parse the results into a list using fromJSON(): loon.list &lt;- jsonlite::fromJSON(content(loon.req, as=&quot;text&quot;)) And look at the $data element of the results, which contains: The full taxonomy of each taxon The short taxon code used by NEON (taxonID/acceptedTaxonID) The author of the scientific name (scientificNameAuthorship) The vernacular name, if applicable The reference text used (nameAccordingToID) The terms used for each field are matched to Darwin Core (dwc) and the Global Biodiversity Information Facility (gbif) terms, where possible, and the matches are indicated in the column headers. loon.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 BIRD ARLO ARLO Gavia arctica ## 2 BIRD COLO COLO Gavia immer ## 3 BIRD PALO PALO Gavia pacifica ## 4 BIRD RTLO RTLO Gavia stellata ## 5 BIRD UNLN UNLN Gavia sp. ## 6 BIRD YBLO YBLO Gavia adamsii ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 (Linnaeus) species Arctic Loon ## 2 (Brunnich) species Common Loon ## 3 (Lawrence) species Pacific Loon ## 4 (Pontoppidan) species Red-throated Loon ## 5 &lt;NA&gt; genus Unknown Loon ## 6 (G. R. Gray) species Yellow-billed Loon ## dwc:nameAccordingToID dwc:kingdom dwc:phylum ## 1 doi: 10.1642/AUK-15-73.1 Animalia Chordata ## 2 doi: 10.1642/AUK-15-73.1 Animalia Chordata ## 3 doi: 10.1642/AUK-15-73.1 Animalia Chordata ## 4 doi: 10.1642/AUK-15-73.1 Animalia Chordata ## 5 http://www.birdconservancy.org/ (accessed 5/16/2016) Animalia Chordata ## 6 doi: 10.1642/AUK-15-73.1 Animalia Chordata ## dwc:class dwc:order dwc:family dwc:genus gbif:subspecies gbif:variety ## 1 Aves Gaviiformes Gaviidae Gavia NA NA ## 2 Aves Gaviiformes Gaviidae Gavia NA NA ## 3 Aves Gaviiformes Gaviidae Gavia NA NA ## 4 Aves Gaviiformes Gaviidae Gavia NA NA ## 5 Aves Gaviiformes Gaviidae Gavia NA NA ## 6 Aves Gaviiformes Gaviidae Gavia NA NA To get the entire list for a particular taxonomic type, use the taxonTypeCode query. Be cautious with this query, the PLANT taxonomic list has several hundred thousand entries. For an example, lets look up the small mammal taxonomic list, which is one of the shorter ones, and use the verbose=true option to see a more extensive list of taxon data, including many taxon ranks that arent populated for these taxa. For space here, we display only the first 10 taxa: mam.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?taxonTypeCode=SMALL_MAMMAL&amp;offset=0&amp;limit=500&amp;verbose=true&quot;) mam.list &lt;- jsonlite::fromJSON(content(mam.req, as=&quot;text&quot;)) mam.list$data[1:10,] ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 SMALL_MAMMAL AMHA AMHA Ammospermophilus harrisii ## 2 SMALL_MAMMAL AMIN AMIN Ammospermophilus interpres ## 3 SMALL_MAMMAL AMLE AMLE Ammospermophilus leucurus ## 4 SMALL_MAMMAL AMLT AMLT Ammospermophilus leucurus tersus ## 5 SMALL_MAMMAL AMNE AMNE Ammospermophilus nelsoni ## 6 SMALL_MAMMAL AMSP AMSP Ammospermophilus sp. ## 7 SMALL_MAMMAL APRN APRN Aplodontia rufa nigra ## 8 SMALL_MAMMAL APRU APRU Aplodontia rufa ## 9 SMALL_MAMMAL ARAL ARAL Arborimus albipes ## 10 SMALL_MAMMAL ARLO ARLO Arborimus longicaudus ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Audubon and Bachman species Harriss Antelope Squirrel ## 2 Merriam species Texas Antelope Squirrel ## 3 Merriam species Whitetailed Antelope Squirrel ## 4 Goldman subspecies &lt;NA&gt; ## 5 Merriam species Nelsons Antelope Squirrel ## 6 &lt;NA&gt; genus &lt;NA&gt; ## 7 Taylor subspecies &lt;NA&gt; ## 8 Rafinesque species Sewellel ## 9 Merriam species Whitefooted Vole ## 10 True species Red Tree Vole ## taxonProtocolCategory dwc:nameAccordingToID ## 1 opportunistic isbn: 978 0801882210 ## 2 opportunistic isbn: 978 0801882210 ## 3 opportunistic isbn: 978 0801882210 ## 4 opportunistic isbn: 978 0801882210 ## 5 opportunistic isbn: 978 0801882210 ## 6 opportunistic isbn: 978 0801882210 ## 7 non-target isbn: 978 0801882210 ## 8 non-target isbn: 978 0801882210 ## 9 target isbn: 978 0801882210 ## 10 target isbn: 978 0801882210 ## dwc:nameAccordingToTitle ## 1 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 2 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 3 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 4 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 5 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 6 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 7 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 8 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 9 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 10 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## dwc:kingdom gbif:subkingdom gbif:infrakingdom gbif:superdivision ## 1 Animalia NA NA NA ## 2 Animalia NA NA NA ## 3 Animalia NA NA NA ## 4 Animalia NA NA NA ## 5 Animalia NA NA NA ## 6 Animalia NA NA NA ## 7 Animalia NA NA NA ## 8 Animalia NA NA NA ## 9 Animalia NA NA NA ## 10 Animalia NA NA NA ## gbif:division gbif:subdivision gbif:infradivision gbif:parvdivision ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## 7 NA NA NA NA ## 8 NA NA NA NA ## 9 NA NA NA NA ## 10 NA NA NA NA ## gbif:superphylum dwc:phylum gbif:subphylum gbif:infraphylum gbif:superclass ## 1 NA Chordata NA NA NA ## 2 NA Chordata NA NA NA ## 3 NA Chordata NA NA NA ## 4 NA Chordata NA NA NA ## 5 NA Chordata NA NA NA ## 6 NA Chordata NA NA NA ## 7 NA Chordata NA NA NA ## 8 NA Chordata NA NA NA ## 9 NA Chordata NA NA NA ## 10 NA Chordata NA NA NA ## dwc:class gbif:subclass gbif:infraclass gbif:superorder dwc:order ## 1 Mammalia NA NA NA Rodentia ## 2 Mammalia NA NA NA Rodentia ## 3 Mammalia NA NA NA Rodentia ## 4 Mammalia NA NA NA Rodentia ## 5 Mammalia NA NA NA Rodentia ## 6 Mammalia NA NA NA Rodentia ## 7 Mammalia NA NA NA Rodentia ## 8 Mammalia NA NA NA Rodentia ## 9 Mammalia NA NA NA Rodentia ## 10 Mammalia NA NA NA Rodentia ## gbif:suborder gbif:infraorder gbif:section gbif:subsection gbif:superfamily ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## 7 NA NA NA NA NA ## 8 NA NA NA NA NA ## 9 NA NA NA NA NA ## 10 NA NA NA NA NA ## dwc:family gbif:subfamily gbif:tribe gbif:subtribe dwc:genus ## 1 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 2 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 3 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 4 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 5 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 6 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 7 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 8 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 9 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## 10 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## dwc:subgenus gbif:subspecies gbif:variety gbif:subvariety gbif:form ## 1 &lt;NA&gt; NA NA NA NA ## 2 &lt;NA&gt; NA NA NA NA ## 3 &lt;NA&gt; NA NA NA NA ## 4 &lt;NA&gt; NA NA NA NA ## 5 &lt;NA&gt; NA NA NA NA ## 6 &lt;NA&gt; NA NA NA NA ## 7 &lt;NA&gt; NA NA NA NA ## 8 &lt;NA&gt; NA NA NA NA ## 9 &lt;NA&gt; NA NA NA NA ## 10 &lt;NA&gt; NA NA NA NA ## gbif:subform speciesGroup dwc:specificEpithet dwc:infraspecificEpithet ## 1 NA &lt;NA&gt; harrisii &lt;NA&gt; ## 2 NA &lt;NA&gt; interpres &lt;NA&gt; ## 3 NA &lt;NA&gt; leucurus &lt;NA&gt; ## 4 NA &lt;NA&gt; leucurus tersus ## 5 NA &lt;NA&gt; nelsoni &lt;NA&gt; ## 6 NA &lt;NA&gt; sp. &lt;NA&gt; ## 7 NA &lt;NA&gt; rufa nigra ## 8 NA &lt;NA&gt; rufa &lt;NA&gt; ## 9 NA &lt;NA&gt; albipes &lt;NA&gt; ## 10 NA &lt;NA&gt; longicaudus &lt;NA&gt; To get information about a single taxon, use the scientificname query. This query will not do a fuzzy match, so you need to query the exact name of the taxon in the NEON taxonomy. Because of this, the query will be most useful when you already have NEON data in hand and are looking for more information about a specific taxon. Querying on scientificname is unlikely to be an efficient way to figure out if NEON recognizes a particular taxon. In addition, scientific names contain spaces, which are not allowed in a URL. The spaces need to be replaced with the URL encoding replacement, %20. For an example, lets look up the little sand verbena, Abronia minor Standl. Searching for Abronia minor will fail, because the NEON taxonomy for this species includes the authority. The search will also fail with spaces. Search for Abronia%20minor%20Standl., and in this case we can omit offset and limit because we know there can only be a single result: am.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?scientificname=Abronia%20minor%20Standl.&quot;) am.list &lt;- jsonlite::fromJSON(content(am.req, as=&quot;text&quot;)) am.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 PLANT ABMI2 ABMI2 Abronia minor Standl. ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Standl. species little sand verbena ## dwc:nameAccordingToID dwc:kingdom dwc:phylum ## 1 http://plants.usda.gov (accessed 8/25/2014) Plantae Magnoliophyta ## dwc:class dwc:order dwc:family dwc:genus gbif:subspecies ## 1 Magnoliopsida Caryophyllales Nyctaginaceae Abronia NA ## gbif:variety ## 1 NA 2.13 Stacking NEON data At the top of this tutorial, we installed the neonUtilities package. This is a custom R package that stacks the monthly files provided by the NEON data portal into a single continuous file for each type of data table in the download. It currently handles files downloaded from the data portal, but not files pulled from the API. For a guide to using neonUtilities on data downloaded from the portal, look here. 2.14 Intro to NEON Exercises Part 2 2.14.1 Exercise 2.2: Written questions Suggested Timing: Complete this exercise before our next class meeting Question 1: How does NEON address dark data (Chapter 1)? Question 2: How might or does the NEON project intersect with your current research or future career goals? (1 paragraph) Question 3: Use the map in Chapter 2:Intro to NEON to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there NEON field sites that are in study regions of interest to you? What domains are the sites located in? What NEON field sites do your current research or Capstone Project ideas coincide with? Is the site or sites core or relocatable? Are they terrestrial or aquatic? Are there data available for the NEON field site(s) that you are most interested in? What kind of data are available? Question 4: Consider either your current or future research, or a question youd like to address durring this course and answer each of the following questions: Which types of NEON data may be more useful to address these questions? What non-NEON data resources could be combined with NEON data to help address your question? What challenges, if any, could you foresee when beginning to work with these data? Question 5: Use the Data Portal tools to investigate the data availability for the field sites youve already identified in the previous sections and answer each of the following questions: What types of aquatic or terrestrial data are currently available? Remote sensing data? Of these, what type of data are you most interested in working with for your project during this course? For what time period does the data cover? What format is the downloadable file available in? Where is the metadata to support this data? 2.14.2 Exercise 2.3: NEON Coding Lab - Further Exploration of NEON Data Suggested Timing: Complete this exercise a few days before your NEON clumination write up Use the answers that youve provided above in Exercise 2.2 to select a single NEON site. e.g. ONAQ Use the answers that youve provided above to select 3 NEON data products from either the TOS, TIS or ARS (AOP) collection methods. Sumarize each product with its NEON identifier, along with a sumarry. e.g.: **DP1.10055.001**: Plant phenology observations: phenophase status and insensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at **D15.ONAQ**. Using the NEON Ulitites package or the API pull in those data along with metadata. Organize your data into data.frames and produce summaries for each of your data: Filter your data based on metadata and quality flags: DP1.10055.001: Plant phenology observations: phenophase status and intensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at D15.ONAQ. Here I will focus on the phenophase intensity data, which is a measure of how prevalent that particular phenophase is in the sampled plants. Using the NEON Ulitites package or the API pull in those data along with metadata. sitesOfInterest &lt;- c(&quot;ONAQ&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) Organize your data into data.frames and produce summaries for each of your data: #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) Filter and format your data based on metadata and quality flags: #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) #Format dates (native format is &#39;factor&#39; silly R) phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) phe_statusintensity$year &lt;- substr(phe_statusintensity$date, 1, 4) phe_statusintensity$monthDay &lt;- format(phe_statusintensity$date, format=&quot;%m-%d&quot;) Now I want to remove NA values so I can see whats really going on: phe_statusintensity=phe_statusintensity%&gt;% filter(!is.na(phe_statusintensity$phenophaseIntensity)) Create minimum of 1 plot per data type (minimum of 3 plots total). These will vary based on that data that youve chosen. A non-exhastive list of ideas: 1. Your data as a function of height on the tower (FPAR example) 2. A map of the locations where your data is sampled (TOS tree example, bird example) 3. A model based on the data youre interested in working work (Coding lab 1 example) 4. A timeseries of your data (example below) What is the temporal frequency of observations in the data you decided was of interest? How do the data align to answer a central question? What challenges did you run into when investigating these data? How will you address these challenges and document your code? One to two paragraphs 2.14.3 Exercise 2.4: Intro to NEON Culmination Activity Due before we start Chapter 3: USA-NPN Write up a 1-page summary of a project that you might want to explore using NEON data over the duration of this course. Include: the types of NEON (and other data) that you will need to implement this project, including data product id numbers. If in your NEON coding lab part 2 you highlighted challenges to using these data, discuss methods to address those challenges. *e.g. If your site doesnt yet have a long data recocrd, is it located close to a longer lived site from another network? (LTER, Ameriflux, LTAR etc) One high-level summary graphic including all of your data from the NEON Coding Lab Part 2 Save this summary as you will be refining and adding to your ideas over the course of the semester. "],["introduction-to-usa-npn-its-data.html", "Chapter 3 Introduction to USA-NPN &amp; its Data 3.1 USA-NPN Learning Objectives 3.2 USA-NPN Project Mission &amp; Design: 3.3 Vision &amp; Mission 3.4 USA-NPNs Spatial design: 3.5 Types of USA-NPN Data: 3.6 How to Access USA-NPN Data: 3.7 USA-NPN Written Questions 3.8 Hands on: Accessing USA-NPN Data via rNPN 3.9 Accumulated Growing Degree Day Products 3.10 Extended Spring Indices 3.11 Putting it all together: 3.12 Combine Point and Raster Data 3.13 Live Demo Code with Lee Marsh of USA-NPN 3.14 USA-NPN Coding Lab 3.15 NEON TOS Phenology Data Lecture 3.16 Understanding Observation Biases and Censoring in Citizen Science Data 3.17 So then how can we model censored data? 3.18 Intro to USA-NPN Culmination Activity", " Chapter 3 Introduction to USA-NPN &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will build upon over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 3.1 USA-NPN Learning Objectives At the end of this activity, you will be able to: Understand the mission and purpose of the USA-National Phenology Network (USA-NPN) and the nature of the citizen science program from which the data is derived Access all of the various tools &amp; resources that are available to pull USA-NPN geospatial and observational data Effectively use the rNPN package to integrate and analyze NPN data with other similar datasets 3.2 USA-NPN Project Mission &amp; Design: The USA National Phenology Network (USA-NPN) collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. The USA National Phenology Network consists of a National Coordinating Office (NCO), thousands of volunteer observers and many partners, including research scientists, resource managers, educators, and policy-makers. Anyone who participates in Natures Notebook or collaborates with NCO staff to advance the science of phenology or to inform decisions is part of the USA-NPN. 3.3 Vision &amp; Mission USA-NPNs vision is to provide data and information on the timing of seasonal events in plants and animals to ensure the well-being of humans, ecosystems, and natural resources. To support this and its mission the USA-NPN collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. 3.3.1 Relevant documents &amp; background information: USA-NPN Strategic Plan USA-NPN Information Sheet: Tracking seasonal changes to support science, natural resource management, and society 2019 USA-NPN Annual Report 3.4 USA-NPNs Spatial design: Phenology datasets that are best suited for supporting scientific discovery and decision making are those that consist of observations of multiple life-cycle stages collected at regular intervals at the same locations over multiple years. The USA-NPN collects, stores, and shares high-quality observations of plant and animal phenology at a national scale by engaging observers in Natures Notebook, a national-scale, multi-taxon phenology observing program appropriate for both professional and volunteer participants. Because observations are entirely voluntary, the sampling design for observations is opportunistic. The Natures Notebook program has been adopted widely; data are collected at over 100 academic institutions, 78 National Ecological Observatory Network (NEON) sites, and by hundreds of researchers to contribute observations to support scientific discovery. The program is also used by tens of thousands of individual observers and members of federal, state, NGO, and private sector organizations as well as K-12 and higher-ed institutions. A unique aspect of Natures Notebook is that monitoring can be undertaken by individuals as well as by community or regionally-organized groups referred to as Local Phenology Programs (LPP). Organizations such as nature centers, arboreta, land conservancies, and National Wildlife Refuges use Natures Notebook to meet a diversity of outcomes, including asking and answering scientific questions about the impact of environmental change, informing natural resource management and decision-making, and educating and engaging the public. 3.5 Types of USA-NPN Data: 3.5.1 Observational Observational phenology data, consisting of observations made of phenological status on individual organisms, are collected and submitted by professional and citizen scientists, primarily through the USA-NPN plant and animal phenology observing program, Natures Notebook. These data are submitted to the USA-NPN and serve as the backbone of all USA-NPN observational data products. Observation protocols consist of status monitoring, in which observers visit a site at regular intervals to evaluate the phenological status of marked individual plants (or patches of plants) and animal species The protocols are described fully in Denny et al. (2014). In this system, phenological status is reported by yes or no answers to a series of questions, for example, Do you see leaves? or Do you see active individuals?. In addition to Yes or No, observers may also report ?, indicating that they are uncertain of the phenophase status. Observers are also invited to document the degree to which the phenophase is expressed on an individual plant, or for animals, at a site. This intensity or abundance question takes the form of a count or percentage - for example, 95100 percent of a beech trees canopy is full with Leaves, or 12 Active individual robins are seen. USA-NPN observational data and derivative products are described in USA National Phenology Network Observational Data Documentation (Rosemartin et al. 2018). The three formats in which the USA-NPN observational data are made available include: status and intensity data, individual phenometrics, and site-level phenometrics. Visual comparison of data collected by monitoring phenological events, phenophase status, and phenophase status plus intensity. Event monitoring captures onset of a given phenophase, whereas status monitoring captures onset and duration. Status monitoring with intensity (or abundance) captures onset, duration, and magnitude of a phenophase. Examples are derived from 2012 data submitted in Natures Notebook for (a) sugar maple (Acer saccharum) leafing for one individual plant in Maine, and (b) forsythia flowering (Forsythia sp.) for one individual plant in Massachusetts. Each point represents one observation; black points indicate presence of the phenophase while white points indicate absence. (a) illustrates the date on which the first leaf appears (event), the period during which leaves are present (status), and the period and rate at which the canopy fills from 0 to 100 % capacity and then, empties back to 0 with leaf fall (status+intensity, circles and solid line) using estimates of canopy fullness. Also illustrated is the period and rate at which the canopy fills and empties of autumn colored leaves (status+intensity, triangles and dashed line). (b) illustrates the date on which the first open flower appears (event), the periods during which open flowers are present on the plant (status), and an estimate of the number of open flowers on the plant over the periods in which they are present (status+intensity). In both examples, the event point is calculated as the first date of the year where the phenophase was reported as present. Note that in (b) there are two distinct periods of flowering, the second of which would not have been captured using event monitoring alone. (Denny et al., 2014) 3.5.2 Status &amp; Intensity Data Status and intensity data consist of presence/absence records for individual phenophases on individual plants or species of animals at a site on a single visit. These records also include intensity and abundance measures. Individual Phenometrics and Site Phenometrics, which are synthesized sequentially from Status and Intensity data, provide estimated phenophase onset and end dates. Individual Phenometrics are derived estimates of phenophase onset and end dates for organisms within a given period of interest. Site Phenometrics are summary metrics of the onset and end date of phenophase activity across multiple individuals of the same species at a site within a given period of interest. Magnitude Phenometrics provide measures of the extent to which a phenophase is expressed across multiple individuals or sites, for a given time interval. These metrics include several approaches for capturing the shape of seasonal activity curves. In Natures Notebook, plants are marked and tracked through time, while animals are not, resulting in several key differences between the phenometric data types for plants and for animals. Individual Phenometrics and Site Phenometrics are nearly identical for animals, while for plants the former provide data for individual plants and the latter aggregate data across plants of the same species at a site. Magnitude Phenometrics provide additional information on animals, including correcting abundance values by search time and search area, which is not relevant for plants. As additional observational phenology data types are created by the USA-NPN, they are described at www.usanpn.org/data/new_data_products. USA-NPN Animal Phenological Data by Type from Rosemartin et al.,2018 3.5.3 Gridded Raster Data The USA-NPN offers a growing suite of gridded (raster) maps of phenological events, patterns, and trends. These products include historical, real-time, and short-term forecasts and anomalies in the timing of events such as the start of the spring season, and growing degree days. These products are described in the USA National Phenology Network gridded products documentation (Crimmins et al. 2017) Accumulated Growing Degree Days anomaly in 2018 3.5.4 Pheno-Forecasts USA-NPN Pheno-Forecasts include real-time maps and short-term forecasts of insect pest activity at management-relevant spatial and temporal resolutions and are based on accumulated temperature thresholds associated with critical life-cycle stages of econmically important pests. Pheno Forecasts indicate, for a specified day, the status of the insects target life-cycle stage in real time across the contiguous United States. The maps are available for 12 insect pest species including the invasive emerald ash borer, hemlock woolly adelgid, and gypsy moth. These products are described in Short-term forecasts of insect phenology inform pest management (Crimmins et al. 2020) Example of USA-NPNs Hemlock Wolly Adelgid Pheno-Forecast for August, 2020. Pheno-Forecasts are also available for an invasive grasses, such as buffelgrass. The buffelgrass Pheno-Forecast is based on known precipitation thresholds for triggering green-up to a level where management actions are most effective. These maps are updated daily and predict green-up one to two weeks in the future. Land Surface Phenology products The USA-NPN offers maps derived from MODIS 6 land surface phenology data. Satellite observations can be linked to in-situ observations to help understand vegetation dynamics across large spatial scales. The MODIS Land Cover Dynamics Product (MLCD) provides global land surface phenology (LSP) data from 2001-present. MLCD serves a wide variety of applications and is currently the only source of operationally produced global LSP data. MLCD data have enabled important discoveries about the role of climate in driving seasonal vegetation changes, helped to create improved maps of land cover, and support ecosystem modeling efforts, among many other important applications. The LSP Climate Indicators (LSP-CI) dataset is a curated collection of the most relevant phenological indicators: a measure of spring and autumn timing and a measure of seasonal productivity. Statistically robust estimates of long-term normals (median and median absolute deviation, MAD), significance-screened trends (Theil-Sen slope magnitude where p&lt;=0.05), and interannual anomalies (in days as well as multiples of MAD) have been computed for these three phenological indicators. The data have been mosaiced across CONUS, reprojected and resampled to a more familiar spatial reference system that matches complementary datasets and delivered in the universally accessible GeoTIFF format. 3.6 How to Access USA-NPN Data: The USA-NPN makes the data they produce available through a number of different channels and tools. This is partly driven by the format of the data; GIS data, in many ways, can and should be managed differently than observational records, which can more easily be managed in a relational database. However, the need for these different venues is also driven by end-user need. The different tiers of tools makes the data accessible to anyone regardless of their level of technical experience. This is true from the casual observer that would like to use the visualization tool to see how their contributions to citizen science relate to the broader world, all the way to the data scientist that needs simple and standard APIs to integrate USA-NPN data into larger applications and analyses. 3.6.1 The USA-NPN Landing page A concise list of all available NPN data sets, tools, products. 3.6.2 APIs This is a set of standard web service calls that allows for programmatic access to NPN data independent of any particular programming language. *USA-NPN Web Service API Documentation *USA-NPN Geoserver Documentation *USA-NPN GeoServer API 3.6.3 Rnpn package This suite of R functions allows for programmatic access to both gridded and in-situ NPN data sets in an R environment. Full documentation available here: https://cran.r-project.org/web/packages/rnpn/rnpn.pdf 3.6.4 Phenology Observation Portal (for observational data) This tool allows users to download customized datasets of observational data from the National Phenology Database, which includes phenology data collected via the Natures Notebook phenology program (2009-present for the United States), and additional integrated datasets, such as historical lilac and honeysuckle data (1955-present). Filters are available to specify dates, regions, species and phenophases of interest. This provides access to all phenometrics, which represents varying degrees of data aggregation. 3.6.5 Geospatial Request Builder (for raster data and image files) This tool simplifies the process of accessing NPN gridded data through standard WMS and WCS services. WMS services provide the data as basic graphic images, such as PNGs or TIFFs, whereas WCS services provide the same data in formats accessible to GIS applications. 3.6.6 Visualization Tool The Visualization Tool provides an easier way to explore phenology data and maps. The user-friendly interface is intended to allow for searching for comparing general trends and quick-and-easy access to map data/products. 3.7 USA-NPN Written Questions Suggested timing: Complete before lecture 2 of USA-NPN Hands on Coding Exercises Question 1: How might or does USA-NPN intersect with your current research or future career goals? (1 paragraph) Question 2: Use the USA-NPN visualization tool ( &lt;www.usanpn.org/data/visualizations&gt; ) to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: · Are there species, regions, or phenophases of interest to you? · Is there geospatial phenology data that is useful for your work (e.g. Spring Indices or Growing Degree Days)? · What is the timeframe of data you will need to address your research interests? · What is the spatial extent of data you will need? Question 3: Consider either your current or future research, or a question youd like to address during this course: · What climate data or additional phenological datasets would be valuable to address your research interests? · What challenges, if any, could you foresee when beginning to work with these data? 3.8 Hands on: Accessing USA-NPN Data via rNPN 3.8.1 Introduction The USA National Phenology Network (USA-NPN) is a USGS funded organization that collects phenological observation records from volunteer and professional scientists to better understand the impact of changes in the environment on the timing of species life cycles. The USA-NPN also provides a number of raster-based climatological data sets and phenological models. These in-situ observation and geospatial, modeled datasets are available through a number of tools and data services. The USA-NPN R library, rnpn, is primarily a data access service for USA-NPN data products, serving as a wrapper to the USA-NPN REST based web services. This guide details how to use the library to access and work with all USA-NPN data types. install.packages(&quot;devtools&quot;) library(&#39;devtools&#39;) devtools::install_github(&quot;usa-npn/rnpn&quot;) library(&#39;rnpn&#39;) 3.8.2 Accessing USA-NPN Observational Data USA-NPN Observational data are collected on the ground by citizen and professional observers following standardized protocols, using the Natures Notebook platform. The data are available 2009 to present, and come in four formats or data types: Status &amp; Intensity, Individual Phenometrics, Site Phenometrics and Magnitude Phenometrics. An overview of the differences is provided in the figure below, and each type is detailed in the following sections. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Observational Data. In Natures Notebook, observers register a location, and then at each location they register any number of individual plants or animal species. The expectation is that the user then takes regular observations on each individual/species at a regular interval. Phenological status is reported by yes or no answers to a series of questions, for example, Do you see leaves? or Do you see active individuals?. In contrast to traditional monitoring of annual first events (for example, date of first leaf or first robin), this approach captures absence data when the phenophase is not occurring and repeat events. Each observation is comprised of a series of 1, 0 and -1 values, representing yes/no/uncertain for each possible phenophase for the plant on that date. To explore data in this native Status and Intensity format, see the vignette by the same name. A few considerations and functions apply across all USA-NPN Observational data types. 3.8.2.1 Basic format for for Observational data calls The basic format for an observational data call in the rnpn library is: npn_download_[NAME OF DATA TYPE] ( request_source = [NULL] year = [NULL] species_ID = [NULL] ) Request source should usually be populated with your full name or the name of the organization you represent. Species_ID is the unique identifier for all the available plants and animals in the USA-NPN database. You can create a table of all available species and their ID numbers: species &lt;- npn_species() Search for a species by common name from the full list: species[species$common_name==&quot;red maple&quot;,] ## # A tibble: 1 × 19 ## specie¹ commo² genus genus³ genus species kingdom itis_ funct class ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 3 red ma Acer 372 Maples rubrum Plantae 28728 Decidu 15 ## #  with 9 more variables: class_common_name &lt;chr&gt;, class_name &lt;chr&gt;, ## # order_id &lt;int&gt;, order_common_name &lt;chr&gt;, order_name &lt;chr&gt;, family_id &lt;int&gt;, ## # family_name &lt;chr&gt;, family_common_name &lt;chr&gt;, species_type &lt;list&gt;, and ## # abbreviated variable names ¹species_id, ²common_name, ³genus_id, ## # genus_common_name, itis_taxonomic_sn, functional_type, class_id There are many parameters which can be set beyond these basic ones, depending on the data type, and further detailed in the other vignettes featured in this package. 3.8.2.2 Required Parameters Note that specifying the year(s) of interest is a required parameter. Theres also another required field, request_source, which is a user-provided, self-identifying string. This allows the client to provide some information about who is accessing the data. Knowing who is using the data is very helpful for our staff to report the impact of the USA-NPN to the scientific community. The input provided here is entirely honor-based. 3.8.2.3 Find stations at which a species has been observed You can also now look up which stations have a registered plant for a particular species. In the example below, we use the species ID for red maple, which we were able to find through the npn_species() function, to find all stations with that species. npn_stations_with_spp (3) 3.8.3 Status and Intensity Data The Status and Intensity data type is the most direct presentation of the phenology data stored in the NPDb. Each row is comprised of a single record of the status (1/present/Yes, 0/absent/No or -1/uncertain/?) of a single phenophase on an individual plant or species of animal at a site on a single site visit, as well as the estimated intensity or abundance e.g., percent canopy fullness or number of individual robins observed respectively. Retrieving this kind of data using this package is easy, and heavily parameterized. Its possible to filter data using a number of including year, geographic extent and species. In this example we get all records of bird observations in the New England states from 2018. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), states = c(&quot;NY&quot;,&quot;PA&quot;,&quot;VT&quot;,&quot;MA&quot;), functional_types = &#39;Bird&#39; ) states is an example of an optional parameter that allows you to filter data based on geographic location. Another example is functional_types which allows you to get all available data for a group of similar species (e.g., all birds, shrubs or invasive species). The best place to review all available optional filters is the autogenerated package description. Another important optional parameter is called download_path. By default requests for data from the services are returned as a data frame that gets stored in memory as a variable. In some cases, it makes more sense to save the data to file for easy and fast retrieval later. The download_path parameter allows you to specify a file path to redirect the output from the service, without having to fuss with pesky I/O operations. Additionally, requests made this way streams the data returned, so if the dataset youre working with is particularly large, its possible to redirect the stream of data to file instead of loading it all into memory which can be useful if your environment doesnt have enough RAM to store the entire data set at once. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), functional_types = &#39;Bird&#39;, additional_fields = &#39;Site_Name&#39;, download_path =&#39;Bird_data_2018_SiteName.csv&#39; ) Using this function to get observational records is the most basic presentation of the data, and is the most robust for doing analysis, but there are a number of other products offered through the data service which provide additional value to data end users, outlined in the next vignettes. 3.8.4 Individual Phenometrics While Status and Intensity data provide a direct and complete look at the observational data, some analyses rely on more synthesized output. Individual Phenometrics are derived from phenophase status data and provide estimates of phenophase onset and end dates based on the first and last Yes status values for organisms within a specified season of interest. Each row in this data type is comprised of values that are derived from a string of consecutive Yes status reports without an intervening No status report for a single phenophase for an individual plant or animal species at a site, called a series. For plants, this data type provides information on the onset and end of a phenophase on an individual plant. For animals, it provides information on the onset and end of the presence of an animal species at a site. As animal presence at a site is much more likely to be interrupted by absence than the presence of a phenophase on a plant, Status and Intensity data or Site Phenometrics may be more appropriate for investigating animal phenology. However, we provide animal phenology in the same format as individual plants in this data type to allow users to readily compare individual plant phenology with animal activity. Note that more than one series may exist for a given phenophase in an individual plant or animal species within a single growing season or year, this might occur in the case of leaf bud break followed by a killing frost and second round of breaking leaf buds. It could also occur at group sites where two or more observers are reporting on the same plant on sequential days but are not in agreement on phenophase status. Any call for individual phenometrics requires chronological bounds, usually a calendar year, as determining onset and end depend on knowing what the time frame of interest is. If you query the services directly (without the benefit of this library) its possible to specify arbitrary dates, in contrast this library allows you to specify a series of calendar years as input. Heres an example of how to query the services for individual phenometrics data. Note that the overall structure and parameters are very similar to the call for status data. The biggest difference in this case is that start and end date parameters are now replaced with a years array, which predictably takes a set of year values with which to query the service. npn_download_individual_phenometrics( request_source=&#39;Your Name Here&#39;, years=c(2013,2014,2015,2016), species_id=c(210), download_path=&quot;saguaro_data_2013_2016.csv&quot; ) ## using a custom handler function. ## opening curl input connection. ## Found 96 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 136 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 145 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 102 records... ## closing curl input connection. ## NULL In this example, were able to see individual saguaro phenology for 2013 through 2016. The results returned from the service is a tabular set of records, giving start and end date by individual saguaro plant. By default, each record contains information about the location, species, phenophase, and start and end dates. Climate data from DayMet can also be acquired with Status &amp; Intensity, Individual Phenometrics and Site Phenometric data types, by setting the climate_data parameter to true. In this example, we are getting colored leaves (phenophase ID is 498) data for birches, using the four birch species IDs, for 2015: npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2015&#39;), species_ids = c(97, 98, 99, 430), phenophase_ids = c(498), climate_data = TRUE, download_path = &#39;Betula_data_2015.csv&#39; ) ## using a custom handler function. ## opening curl input connection. ## Found 146 records... ## closing curl input connection. ## NULL To show what this looks like, we can plot the day of year of the first observation of colored leaves in birches (genus Betula) against summer Tmax. BetulaLeaf &lt;-read.csv( &#39;Betula_data_2015.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( first_yes_doy~tmax_summer, data=BetulaLeaf, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Tmax Summer&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21 ) 3.8.5 Site Phenometrics Site Phenometrics, derived from Individual Phenometrics, provide summary metrics of the onset and end date of phenophase activity for a species at a site. Observers are directed to create sites that represent uniform habitat and are no larger than 15 acres. For plants, this metric is calculated as an average for all individuals of a species at the site. For animals, where individuals are not tracked, this metric represents the first and last recorded appearance of the species during the season of interest. For instance, if you asked for red maple leafing data, and there was a site with three red maple trees being observed, then the data would be the average onset date for all three of those red maple trees at that site. Heres an example of how to query the services for site phenometrics data, for cloned lilacs, breaking leaf buds, 2013. The call is very similar to the call for individual phenometrics data, however, in addition you can supply the quality control filter for the number of days between a yes record and preceding no record (also applies to the last yes and following no), for the observation to be included in the calculations. Typically this is set to 7, 14 or 30, as when downloading data using the USA-NPN Phenology Observation Portal. If you do not set this parameter, it defaults to 30 days. Note that in this example the results are stored in memory, rather than output as a file. LilacLeafPoints2013&lt;-npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2013&#39;), num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) In this example were able to see the date of the first observation of breaking leaf buds for cloned lilacs, averaged across individuals within sites. If any observation did not have a preceding no record within 30 days it was excluded from the calculations. We can now plot our cloned lilac site phenometric onset data by latitude. plot( mean_first_yes_doy~latitude, data=LilacLeafPoints2013, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Latitude&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=c(30,55), ylim=c(0,200) ) 3.8.6 Magnitude Phenometrics Magnitude Phenometrics are a suite of eight metrics derived from Status and Intensity data. This data type provides information on the extent to which a phenophase is expressed across multiple individuals or sites, for a given set of sequential time intervals. The data user may select a weekly, bi-weekly, monthly, or custom time interval to summarize the metrics. Two metrics are available for both plants and animals, one metric is available for plants alone and five metrics are available for animals alone (table 1). Three of the five animal metrics correct animal abundance values for observer effort in time and space. Heres an example of how to query for Magnitude Phenometrics, for the active individuals phenophase for black-capped chickadee data, in 2018. Requirements are similar to other data types. You must additionally specify the time interval by which the data should be summarized. Typically this is weekly, biweekly or monthly, as in the POP and Visualization Tool. The interval chosen in this example is 7 days. npn_download_magnitude_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, period_frequency = &quot;7&quot;, species_ids = &#39;245&#39;, phenophase_ids = &#39;292&#39;, download_path = &#39;MPM_BCC_ActInd_2018.csv&#39; ) In this example were able to see all of the magnitude phenometric fields, including proportion_yes_records, and mean_num_animals_in-phase. See the https://pubs.usgs.gov/of/2018/1060/ofr20181060.pdf for full field descriptions. From this dataset we can view the Proportion_Yes_Records (of all the records submitted on this species, what proportion are positive/yes records) by weekly interval: BCC_AI&lt;-read.csv( &#39;MPM_BCC_ActInd_2018.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( BCC_AI$proportion_yes_record~as.Date(BCC_AI$start_date,&quot;%Y-%m-%d&quot;), ylab=c(&quot;Proportion Yes Records&quot;), xlab=c(&quot;Date&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=as.Date(c(&quot;2018-01-01&quot;, &quot;2018-08-01&quot;)), ylim=c(0,1) ) 3.8.7 USA-NPN Geospatial Data USA-NPN provides phenology-relevant climate data in raster format. There are two main suites of products in this category: Accumulated Growing Degree Days and Extended Spring Indices. Accumulated Growing Degree Days and the Extended Spring Indices are both representations of accumulated temperature. As accumulated winter and spring heat drives many spring season phenological events in much of the country, these products can be used to better understand patterns in the current and historical timing of these events across the landscape. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Gridded Data. Both suites are available as: Current year value, with a 6-day forecast Current year anomaly, with a 6-day forecast Long-term (30 year) average Historical years AGDD - 2016-Prior Year Extended Spring Index - 1880-Prior Year All of these products can be downloaded using the npn_download_geospatial call. There is a number of other products and permutations of the above listed AGDD and Spring Index products, so you can get a complete list of available layers and additional details about them including resolution, extent and the abstract/layer description. layers &lt;- npn_get_layer_details() The following sections describe how to parameterize calls for both AGDD and Spring Index layers. These calls result in raster data sets for the contiguous United States. If you are interested in how many GDDs had accumulated when the red maple in your backyard leafed out, or what day the Spring Index requirements for leaf out were met for your location, you may wish to query the layers for these values, based on location and date. There are two ways to accomplish this, using the npn_get_point_data function which works for all layers and the npn_get_AGDD_point_data function, which only works for AGDD layers and provides a more precise result. npn_get_agdd_point_data( &#39;gdd:agdd_50f&#39;, &#39;38&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) This returns a value of 7.64098 GDD, base 50F, for the coordinates 38 north, -90 west on February 25th, 2019. npn_get_point_data( &#39;si-x:lilac_bloom_ncep&#39;, &#39;30&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) ## Downloading: 980 B Downloading: 980 B Downloading: 990 B Downloading: 990 B Downloading: 990 B Downloading: 990 B ## No encoding supplied: defaulting to UTF-8. ## [1] -9999 This returns a value for lilac bloom of day 48, for the coordinates 30 north, -90 west, as of February 25th, 2019. The above mentioned AGDD products use base temperatures of 32F or 50F and are managed through WCS services. There is also a function to get dynamic AGDD calculations based on a user defined base temperature and a number of other parameters. custom_agdd_raster &lt;- npn_get_custom_agdd_raster( method = &#39;double-sine&#39;, climate_data_source = &#39;NCEP&#39;, temp_unit = &#39;fahrenheit&#39;, start_date = &#39;2019-01-01&#39;, end_date = &#39;2019-05-10&#39;, base_temp = 20, upper_threshold = 90 ) 3.9 Accumulated Growing Degree Day Products Heat accumulation is commonly used as a way of predicting the timing of phenological transitions in plants and animals, including when plants exhibit leaf out, flowering, or fruit ripening, or when insects emerge from dormancy. This is typically expressed as accumulated heat units, either Growing Degree Hours or Growing Degree Days. Growing degree day thresholds have been established for many species, and are commonly used in agriculture, horticulture, and pest management to schedule activities such as harvesting, pesticide treatment, and flower collection. The USA-NPN is currently generating Accumulated Growing Degree Days (AGDD) rasters using a January 1 start date, calculated using simple averaging. These are available calculated using two base temperatures, 32 degrees Fahrenheit (F) and 50 F. When querying certain layers, the underlying data is agnostic about the specific year, and in these cases it makes sense to use the day of year to request data, since that will provide a standardized result, (i.e., April 1st is day 91 in some years and day 92 in others). npn_download_geospatial( &#39;gdd:30yr_avg_agdd_50f&#39;, 95 ) But if youre looking at a specific year, such as a current year layer, it makes sense to use a specific calendar date (formatted YYYY-MM-DD). Its also possible to save the raster directly to file instead of loading it into memory. npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-05-05&#39;, output_path=&#39;20180505-agdd-value.tiff&#39; ) In the case of the historic Spring Index layers, however, the product represents the overall outcome for the entire year, so while the year component of the date matters, the month and day do not. In this case, specify January 1 as the month and date. npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1995-01-01&quot; ) The dimension.range value, returned in the npn_get_layer_details() function, clarifies the full set of applicable dates for each layer. Of course, its also easy to grab raster data and load it into a visual plot as in this example, showing a map of AGDD base 50 on 2019-06-25: AGDDJun2019&lt;-npn_download_geospatial( &#39;gdd:agdd_50f&#39;, &#39;2019-06-25&#39; ) plot( AGDDJun2019, main = &quot;AGDD base 50 on June 25th, 2019&quot; ) An important layer to know of is the 30 year average for AGDD products. This is useful for many comparative analyses. This layer takes DOY as the date input, since its the average AGDD value for each day of year for 1981 - 2010. average_30yr &lt;- npn_download_geospatial( &quot;gdd:30yr_avg_agdd&quot;, 45 ) 3.10 Extended Spring Indices The Extended Spring Indices are mathematical models that predict the start of spring (timing of first leaf or first bloom) at a particular location. These models were constructed using historical observations of the timing of first leaf and first bloom in a cloned lilac cultivar (Syringa X chinensis Red Rothomagensis) and two cloned honeysuckle cultivars (Lonicera tatarica L. Arnold Red and Lonicera korolkowii Stapf, also known as Zabelii), which were selected based on the availability of historical observations from across a wide geographic area. Primary inputs to the model are temperature and weather events, beginning January 1 of each year. The model outputs are first leaf and first bloom date for a given location. Data for the Spring Index is available through an enumeration of layers that represents each of the three sub-models as well as an average model which represents the aggregation of the three sub-models. These layers are further enumerated by both of the represented phenophases, leaf and bloom. In the example below, first the layer representing only the Arnold Red model for 1987 is retrieved, while the second function call gets the model averaging all three of the models for the same year. npn_download_geospatial( &quot;si-x:arnoldred_bloom_prism&quot;, &quot;1987-01-01&quot; ) average_model &lt;- npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1987-01-01&quot; ) The Spring Indices are also unique in that the algorithm has been run against the BEST climate data set, so historic data going back to 1880 is available. BESTSIxData1905 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_best&#39;, &#39;1905-01-01&#39; ) NAvalue(BESTSIxData1905) &lt;- -9999 plot( BESTSIxData1905, main = &quot;Spring Index, 1905&quot; ) 3.10.1 Other Layers Besides the AGDD and Spring Index layers there are a number of other useful layers available through these services, including daily temperature minimum and maximums and aggregated MODISv6 phenometrics. The daily temperature minimum and maximum values are the underlying climate data used to generate current year AGDD and Spring Index maps. These data are generated by NOAAs National Centers for Environmental Prediction (NCEP) and are reserved through NPNs geospatial services. daily_max_20190505 &lt;- npn_download_geospatial( &#39;climate:tmax&#39;, &#39;2019-05-05&#39; ) plot( daily_max_20190505, main = &quot;Daily Temperature Max (C), May 5th, 2019&quot; ) The MODISv6 layers are aggregate values for remote sensing values from the MODISv6 data set, representing a subset of the following phenometrics, aggregated across 2001 - 2017: EVI Area, Mid-Greenup, Mid-Greendown. The available aggregate values for each layer are: median, TSslope, and mean absolute deviation. This example shows the median green up value, as DOY. Note that because this layer has a fixed date, the date parameter is input as a blank string. median_greenup &lt;- npn_download_geospatial( &#39;inca:midgup_median_nad83_02deg&#39;, &#39;&#39; ) plot( median_greenup, main = &quot;MODIS Median Mid-Greenup, 2001 - 2017&quot; ) 3.11 Putting it all together: 3.12 Combine Point and Raster Data Observational and gridded data can be visualized or analyzed together for a variety of purposes. Users may want to identify spatial patterns in the alignment of dogwood bloom and the Spring Index bloom model. The current years lilac leaf out observations may be compared to the 30 year average lilac sub-model of the spring index to see how well the model predicts the observations. This example shows several data access calls to assemble observational and gridded data. Option 1: You can add a parameter to an observational data call to additionally get a gridded layer value for each observation location/date. Note that if you dont specify which sub model of the Spring Index you want, you will get the SI-x Average layers. npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2013&#39;, num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39;, download_path = &#39;cl_lilac_data_2013_SIxLeaf.csv&#39;, six_leaf_layer = TRUE, six_sub_model = &#39;lilac&#39; ) If you want to append raster data other than Spring Index, Leaf values, theres alternative boolean flags that can be set, including six_bloom_layer for Spring Index, Bloom data, and agdd_layer. Instead of TRUE or FALSE agdd_layer takes 32 or 50 and will correlate each data point with the corresponding AGDD value for the given date using either 32 or 50 base temperature. Option 2: You can create a combined plot of observational data with modeled/raster data. Building on the approach for accessing point data from earlier vignettes describing Individual Phenometrics and getting raster data, we can access and plot these products together. In this example, we will look at how well cloned lilac leaf out observations in 2018 are predicted by the lilac leaf sub model of the Spring Index. 3.12.1 Step 1: Get the data LilacLeaf2018&lt;-npn_download_geospatial( &#39;si-x:lilac_leaf_ncep&#39;, &#39;2018-12-31&#39;, ) LilacLeaf2018Obs &lt;-npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) 3.12.2 Step 2: Preparing the data coords &lt;- LilacLeaf2018Obs[ , c(&quot;longitude&quot;, &quot;latitude&quot;)] data &lt;- as.data.frame(LilacLeaf2018Obs$first_yes_doy) crs &lt;- CRS(&quot;+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) LL_spdf &lt;- SpatialPointsDataFrame( coords = coords, data = data, proj4string = crs ) 3.12.3 Step 3: Define style options and create graph my.palette &lt;- brewer.pal(n=9,name=&quot;OrRd&quot;) plot( LilacLeaf2018, col = my.palette, main=&quot;2018 Observed and Predicted Lilac Leaf Out&quot; ) plot( LL_spdf, main=&quot;Lilac Obs&quot;, pch = 21, bg = my.palette, col = &#39;black&#39;, xlim=c(-125.0208,-66.47917), ylim=c(24.0625 ,49.9375), add = TRUE ) legend( &quot;bottomright&quot;, legend=c(&quot;Cloned Lilac Leaf Out Observations&quot;), pch = 21, bg = &#39;white&#39;, col = &#39;black&#39;, bty=&quot;n&quot;, cex=.8 ) 3.13 Live Demo Code with Lee Marsh of USA-NPN 3.13.1 Basic Utility Functions species &lt;- npn_species() phenophases &lt;- npn_phenophases() layer_details &lt;- npn_get_layer_details() quick_get_phenophase &lt;- function(species_id, date,phenophase_name){ phenophases&lt;-npn_phenophases_by_species(c(species_id),date=date) phenophases_species &lt;- phenophases[phenophases$species_id==species_id]$phenophases[[1]] phenophases_species[phenophases_species$phenophase_name==phenophase_name,]$phenophase_id } quick_get_species &lt;- function(species_name){ species[species$common_name==species_name,]$species_id } 3.13.2 Download Observational Data white_oak_id &lt;- quick_get_species(&quot;white oak&quot;) fruits_id&lt;- quick_get_phenophase(white_oak_id,&quot;2017-05-15&quot;,&quot;Fruits&quot;) # Raw data download s2017_white_oak_raw &lt;- npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id) ) 3.13.3 Magnitude Data m2017_white_oak_magnitude &lt;- npn_download_magnitude_phenometrics( request_source = &quot;INF550&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id), period_frequency = &quot;14&quot; ) datasets &lt;- npn_datasets() # NEON data, file download, additional fields npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2018:2020), states = c(&quot;CO&quot;), dataset_ids = c(16), additional_fields = c(&quot;Site_Name&quot;), download_path = &quot;NEON_CO_Data_2018-2010.csv&quot; ) 3.13.4 Downloading Geospatial Data SIXBloom2018 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_ncep&#39;, &#39;2018-12-31&#39; ) npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-04-15&#39;, output_path = &quot;20180415-32-agdd.tiff&quot; ) my_point &lt;- npn_get_point_data(&quot;gdd:agdd_50f&quot;, 33.649, -111.861, &quot;2017-05-15&quot;) 3.13.5 Putting it together dogwood_id &lt;- quick_get_species(&quot;flowering dogwood&quot;) dogwood_flowering_id &lt;- quick_get_phenophase(dogwood_id,&quot;2018-05-05&quot;,&quot;Flowers or flower buds&quot;) dogwood_data &lt;- npn_download_site_phenometrics( request_source = &#39;Demo&#39;, years = &#39;2018&#39;, species_ids = dogwood_id, phenophase_ids = dogwood_flowering_id, six_leaf_layer = TRUE, agdd=32 ) 3.13.6 Other Data Sources, e.g. Daymet, MODIS add_fields &lt;- npn_download_status_data( request_source = &quot;INF550&quot;, years = c(2014), species_id = c(4), additional_fields = c(&quot;tmaxf&quot;,&quot;Greenup_0&quot;,&quot;MidGreenup_0&quot;) ) 3.14 USA-NPN Coding Lab library(rnpn) library(ggplot2) library(neonUtilities) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union source(&#39;/Users/rohan/katharynduffy.github.io/neon_token_source.R&#39;) For the purposes of this exercise we will be focusing on two NEON sites: HARV and CPER. Save these two sites into your workplace so that you can feed them into functions and packages. Define AGDD and write the equation using LaTeX. What is an appropriate time interval over which we should calculate AGDD? This will be relevant for following questions Use the neonUtilities package to pull plant phenology observations (DP1.10055.001). We will work with the statusintensity data: Hints: #TOS Phenology Data sitesOfInterest &lt;- c(&quot;HARV&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union #Format dates phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) ## Warning in as.POSIXlt.POSIXct(x, tz = tz): unknown timezone &#39;%Y-%m-%d&#39; phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) ## Warning in as.POSIXlt.POSIXct(x, tz = tz): unknown timezone &#39;%Y-%m-%d&#39; phe_statusintensity$year &lt;- as.numeric(substr(phe_statusintensity$date, 1, 4)) phe_statusintensity$month &lt;- as.numeric(format(phe_statusintensity$date, format=&quot;%m&quot;)) In your phe_statusintensity data.frame pick a phenophase name of interest: unique(phe_statusintensity$phenophaseName) ## [1] &quot;Colored leaves&quot; &quot;Falling leaves&quot; &quot;Leaves&quot; ## [4] &quot;Breaking leaf buds&quot; &quot;Increasing leaf size&quot; &quot;Open flowers&quot; ## [7] &quot;Initial growth&quot; &quot;Young needles&quot; &quot;Open pollen cones&quot; ## [10] &quot;Breaking needle buds&quot; &quot;Emerging needles&quot; &quot;Young leaves&quot; And select a single taxon: unique(phe_perindividual$taxonID) ## [1] &quot;QURU&quot; &quot;ACRU&quot; &quot;ARNU2&quot; &quot;TRBOB&quot; &quot;UVSE&quot; &quot;MEVI&quot; &quot;MACA4&quot; &quot;MIRE&quot; &quot;MARAR&quot; ## [10] &quot;PRSES&quot; &quot;BEAL2&quot; &quot;PIST&quot; &quot;ACPE&quot; &quot;FAGR&quot; &quot;VACO&quot; &quot;VIAC&quot; &quot;BELE&quot; &quot;FRAM2&quot; ## [19] &quot;ACSAS&quot; &quot;TSCA&quot; Now create a new, filtered dataframe only including those observations and print a summary. Youll also want to filter for typical things like NA values, and think about how youll work with data that comes in factors or strings. Are there ways you could extract numerical values for plotting? Could you count data? Summarize your strategy. Using dpid DP1.00002.001 Single Aspirated Air Temperature calculate AGDD based on NEON tower data over the time period you decidided upon in question 1. To save you time and frustration Ive placed some mostly complete example code for one height on the tower just for Harvard. You will need to determine which height you think it best and conmplete these calculations for both sites. You will also need to consder things like filtering your temperature data for quality flags, and converting from GMT (Greenwich Mean Time) to your locations time: ##load libraries #library(tidyverse) library(neonUtilities) #install.packages(&#39;mgcv&#39;) library(mgcv) dpid &lt;- as.character(&#39;DP1.00002.001&#39;) ##single aspirated air temperature tempDat &lt;- loadByProduct(dpID=dpid, site = &quot;HARV&quot;, startdate = &quot;2017-01&quot;, enddate=&quot;2017-12&quot;, avg=30, package = &quot;basic&quot;, check.size = FALSE) df &lt;- tempDat$SAAT_30min # GDD typically reported in F # convert df temps df$meanTempF=df$tempSingleMean*1.8+32 #pull date value from dateTime df$date &lt;- substr(df$endDateTime, 1, 10) Group data and summarize values Here, I will group the 30-minute temperature averages by data (to get daily values) You will want to consider which vertical position is most appropriate to use for your analysis. You can view the sensor position data in the sensor_positions table downloaded above, where HOR.VER are the horizontal and vertical position indices (separated by a period),and zOffset is in meters above the ground: select(tempDat$sensor_positions_00002, c(HOR.VER, zOffset)) you can also view all of the sensor position info with the following line: head(tempDat$sensor_positions_00002) For example, the lowest position sensor (verticalPosition == 010) may be most appropriate for comparison with the phenological state of very short plants, while the highest verticalPosition may be better for comparison with canopy trees. Here Ill select level 1 for demonstration day_temp &lt;- df%&gt;% filter(verticalPosition==&quot;010&quot;)%&gt;% group_by(siteID, date)%&gt;% mutate(dayMaxTemp=max(meanTempF), dayMinTemp=min(meanTempF), dayMeanTemp=mean(meanTempF))%&gt;% select(siteID, date, dayMaxTemp, dayMinTemp, dayMeanTemp)%&gt;% distinct() ##alternative, simplified mean, consistent with many GDD calculations ### does accumulation differ for true mean vs. simplified mean? day_temp$mean2 &lt;- (day_temp$dayMinTemp + day_temp$dayMaxTemp)/2 Caluculate daily GDD for a true mean 50 degrees F is a common base temperature used to calculate plant specific GDD. When might you select a different base temp? How might you want to deal with different means of temperature? A couple of options below: day_temp$GDD1 &lt;- ifelse(day_temp$dayMeanTemp-50 &lt; 0, 0, round(day_temp$dayMeanTemp-50, 0)) day_temp$GDD2 &lt;- ifelse(day_temp$mean2-50 &lt; 0, 0, round(day_temp$mean2-50, 0)) day_temp$GDD3 &lt;- ifelse(day_temp$dayMeanTemp-50 &lt; 0, 0, round(day_temp$mean2-50, 0)) # define year day_temp$year &lt;- substr(day_temp$date, 1, 4) #function to add daily GDD values sumr.2 &lt;- function(x) { sapply(1:length(x), function(i) sum(x[1:i])) } #calculate Accumlated GDD day_temp$AGDD3 &lt;- sumr.2(x=day_temp$GDD3) day_temp$AGDD2 &lt;- sumr.2(x=day_temp$GDD2) day_temp$AGDD1 &lt;- sumr.2(x=day_temp$GDD1) day_temp &lt;- ungroup(day_temp) library(plotly) p = plot_ly() %&gt;% add_trace( x= ~day_temp$date, y = ~ day_temp$AGDD1, type= &#39;scatter&#39;, mode = &quot;lines&quot;, line = list(width = 1, color = &quot;rgb(120,120,120)&quot;), name = &quot;Calculated Mean Temp&quot;, showlegend = TRUE, opacity=.5 )%&gt;% add_trace( data = day_temp, x = ~ date, y = ~ AGDD2, name= &#39;Simplified Mean Temp&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;lines&#39;, line = list(width = 1), opacity=.5)%&gt;% add_trace( data = day_temp, x = ~ date, y = ~ AGDD3, name= &#39;Filtered Using Both&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;lines&#39;, line = list(width = 1), opacity=.2) p Plot your calculated AGDD and comment on your calculations. Do you need to revise your time horizon or sensor height? Now were going to build a model to see how AGDD impacts phenological status. But Wait. Is phenology all driven by temperature? Should you consider any other variables? What about AGDD and just plain temperature? Also, we have one very temperate site, and another that is a semi-arid grassland. Should water availability of any sort be considered? Any other variables or data? Create a GAM (Generalized Additive Model) for your phenological data including any variables you think might be relevant. One of the bonuses of a GAM is that it will tell you which variables are relevant and which arent so you can iterate a bit on your model and revise it. You might want to test a few positions on your asipirated air temperature, or a few other additional variables. Your selection is up to you, but you must document and justify your decision. Hints: library(mgcv) model &lt;- mgcv::gam(phenological_status_you_picked ~ AGDD + s(temp or maybe precip) + s(doy), data=your_data) mgcv::summary.gam(model) and plot your models for each site: Hint: mgcv::plot.gam(model, pages=1 ) Now that we have a model for NEON data, lets use the rnpn package to see how adding additional data could improve our fit. Use the taxonID that you selected at each NEON tower, and feed that to the rnpn package to grab observational data and increase your number of observations. Hints: Feeding a state or other region will make the data more congruent Youll likely need to either request the phenophase that you selected from NEON, or filter again. It might make your life easier to request the NEON data from rnpn as they host it as well. Pull AGDD from USA-NPN based on the observations you just pulled. Combine your NEON and USA-NPN data into the same data.frame and re-fit your GAM. Summarize your new model Plot your new model Comment on your new model: was it improved? If so how? 3.15 NEON TOS Phenology Data Lecture Please watch the recorded lecture with Dr. Katie Jones, lead plant ecologist with NEON Battelle 3.16 Understanding Observation Biases and Censoring in Citizen Science Data The following information was adapted from Zachmann et al. (in prep) and is not to be distributed or used beyond this course. One significant barrier to advancing our understanding and utilization of phenology information is wrapped up in the ability of existing models to represent drivers of phenology and phenological shifts appropriately. The specific timing of phenological events, such as first leaf or first flower in plants, is affected by multiple, potentially interacting climatic and geophysical factors. For example, soil moisture supply, atmospheric demand for water, frost events, day length, and terrain have all been shown to influence species phenology. Many of these factors vary over time as well as space. Additionally, some of their effects may themselves vary over time as a result of interactions among controls on phenology. For instance: Precipitation and temperature likely interact to initiate phenological events; Precipitation triggers a state change only after enough heat has accumulated at a given site over the course of the growing season. Phenological predictions, specifically, consist of estimates of the anticipated onset dates of key events: bud break and the arrival of spring, for example, or plant senescence in the fall. In order to track anomalies in real-time, we need forecasts of phenology. Further, we need forecasts of long-term shifts in phenology as a result of climate change. However, making any such predictions requires both reasonable and skillful models of phenology. By reasonable we refer to models that are appropriate for the data  including any nonignorable aspects of the sampling design, Gelman et al. (2013)  and the underlying ecological process, while by skillful we mean that the model has adequate predictive performance. 3.16.1 Many existing modeling approaches fail in one (or both) regards. Much of the existing work relies on models that implicitly or explicitly ignore key characteristics of phenological data and the ecological process of phenology. For example, many studies ignore censoring and other aspects of the sampling design underlying observations, including the location and nestedness of sampling units, or incompletely account for the dynamic nature of phenological drivers. Efforts that fail to account for these features of the data are liable to misrepresent the ecological process, thereby making predictions that are demonstrably wrong and potentially biased. Though it can be argued that some information is better than no information, scientific understanding and consequential management actions require the right information. In other words, it is better to be ignorant than misled. 3.16.2 Censoring Time-to-event data are rarely recorded accurately, in which case observations are said to be censored. There are several types of censoring we must consider. In the context of plant phenology, perhaps the most common is interval censoring, which occurs when a phenological event transpires between visits to a site. Illustration of the types of censoring that are relevant to analyses of phenological data. Although shown extending from zero to positive infinity, the x-axis for most phenological events spans the length of a growing season and starts on Jan. 1. The timing, ti, of the event (e.g., first leaf), in relation to interval boundaries li and ri, is indicated by the position of the leaf along the x- axis, while actual site visits are represented by the presence of the person. Note that - in the case of right-censoring  the event need not occur, hence the question mark. Phenological observations are almost never observed exactly, as depicted on the bottom-most timeline for uncensored observations. (Zachmann et al, in prep) Taking first leaf as an example, a deciduous shrub at site i might be completely leafless at the time of the first visit, li, but could have several emerging leaves at time of the second visit ri. The size of the interval depends on how frequently the site is visited, and may span days or weeks. All that is known is that the event of interest happened in (li,ri), rather than an exact time. Censoring is often ignored in analysis, or it is addressed, potentially inappropriately, using imputation techniques. When the intervals are small relative to the full timespan of interest, the bias introduced by interval censored observations may be small enough to be safely ignored. Either such assumptions must be tested or statistical methods that account for this feature of the sampling design must be used. 3.17 So then how can we model censored data? An extension of the ontology developed above, this figure shows a simulated set of observations for eight sites chosen at random from the N = 100 sites used to develop the concept figures. Three types of censoring can be seen in this figure. Not all phenological observations are created equally, nor systematically! (Zachmann et al, in prep) For logistical or other reasons, record-keeping at some sites starts earlier than others. Additionally, some of the observers make visits to sites more frequently than others. Analyses that do not account for this sampling process explicitly are somewhat impaired  they must filter the observations for known events (times represented by visits shown in purple) and must discard others (e.g., the data for sites 30 and 77), or must impute times using single-point imputation techniques, which have been shown to lead to invalid inference. The histogram seen in the upper panel is constructed on the basis of the true, unobserved leaf out dates. Metaphorically speaking, it is the pile of leaves that would be formed if each leaf (in addition to the other 92 sites not shown) were to fall straight to the bottom of the plot. The histogram on the bottom (in purple) is created using actual first leaf observations (the event dates corresponding to the purple botanists above). (Zachmann et al, in prep) Here the authors seek to model the latent distribution of event times (the green line overlaying the histogram in the upper panel) that gives rise to the observations seen in the lower panel. As they begin to introduce both greater complexity, realism, and utility, they can model the moments (or parameters) of the latent distribution using well-established deterministic models involving climatic and other phenological forcings. 3.18 Intro to USA-NPN Culmination Activity Note: Phenology data may not be relevant everyone. Here are a couple of suggestions: Be creative, example: Say you work with ground water hydrology, how could leaves on trees perhaps be relevant to groundwater recharge rate? Might there be a lag? Etc etc. I will fully accept alternate citizen-science-based datasets and project proposals based on those. The challenge of taking this option is that we have not covered that data. Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using USA-NPN data. Include the types of USA-NPN (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. Sugestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragaph summarizing how this data or analysis is useful to you and/or the infrastructure. "],["phenocam-digital-repeat-photography-networks-methods.html", "Chapter 4 PhenoCam: Digital Repeat Photography Networks &amp; Methods 4.1 Digital Repeat Photography Networks Learning Objectives 4.2 The PhenoCam Network Mission &amp; Design 4.3 PhenoCams Spatial design: 4.4 Digital Repeat Photography Written Questions 4.5 Introduction to Digital Repeat Photography Methods 4.6 Pulling Data via the phenocamapi Package 4.7 Exploring PhenoCam metadata 4.8 Download midday images 4.9 Detecting Foggy Images using the hazer R Package 4.10 Extracting Timeseries from Images using the xROI R Package 4.11 Documentation and Citation 4.12 Hands on: Digital Repeat Photography Computational 4.13 Digital Repeat Photography Coding Lab 4.14 PhenoCam Culmination Activity", " Chapter 4 PhenoCam: Digital Repeat Photography Networks &amp; Methods Estimated Time: 4 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 4.1 Digital Repeat Photography Networks Learning Objectives At the end of this activity, you will be able to: Understand how phenology is a large driver of biosphere-atmosphere interactions, and is a sensitive indicator of climate change. Summarize data which can be pulled out of digital repeat imagery Describe basic processing of digital repeat photography Perform basic image processing. Estimate image haziness as an indication of fog, cloud or other natural or artificial factors using the hazerR package. Define and use a Region of Interest, or ROI, for digitial repeat photography methods. Handle Field-of-View (FOV) shifts in digital repeat photography. Extract timeseries data from a stack of images using color-based metrics. 4.1.1 Guest Lectures 4.1.1.1 PhenoCam, marrying phenology and fluxes This is not the correct video, it is a placeholder (1 hour, 12 minutes) Andrew Richardson, Regents Professor at Northern Arizona Universitys Center of Ecosystem Science and Society (Ecoss) and School of Informatics, Computing and Cyber Systems (SICCS), and Principal Investigator of PhenoCam explains the background of PhenoCam, its conception, and its spacial design. 4.1.1.2 Digital repeat photography methods (1 hour, 11 minutes) Bijan Seyednasrollah, Post-doctoral scholar at Northern Arizona University, School of Informatics, Computing, and Cyber Systems, and lead developer of phenocamapi, covers digital repeat photography methods and the phenocamapi R package. 4.1.2 Assignments in this chapter Digital Repeat Photography Written Questions Demonstrate your knowledge of PhenoCams philosophy and practices, and identify how these intersect with your own research interests (4.4). Digital Repeat Photography Coding Lab Download and Manipulate digital repeat photography data using the R packages covered in this chapter (4.13). PhenoCam culmination activity Summarize a project you might explore using PhenoCam (4.14). 4.2 The PhenoCam Network Mission &amp; Design Since its inception, the objective of the PhenoCam network has been to serve as a repository for phenologically-relevant, digital, repeat (time-lapse) imagery, and to make that imagery, and derived data products, freely available to a wide array of third-party data end-users, including researchers, educators, and the general public. Thus, imagery from the PhenoCam archive is made publicly available, without restriction, and we encourage you to download imagery and datasets for use in your own research and teaching. In return, we ask that you acknowledge the source of the data and imagery, and abide by the terms of the Creative Commons CC BY Attribution License. 4.2.1 Relevant documents &amp; background information: The PhenoCam Gallery A map of PhenoCam locations A site table of all PhenoCams PhenoCam metadata 4.3 PhenoCams Spatial design: The PhenoCam Network is: A voluntary opt in network with collaborators who are varied, including: Individual research labs or field sites in North America, Europe, Asia, Africa, South America Individuals who think it would be cool to be part of a network like this The project is largely run by the Richardson Lab at NAU, with support from key collaborators at the University of New Hampshire who provide server and website management. Anyone can buy a relatively inexpensive camera, run some simple scripts to correct for things like auto-white balance (which we will cover later), and patch in to the netowrk. PhenoCam then retrieves, archieves and processess imagery for distribution. 4.3.1 PhenoCam as a Near Surface Remote Sensing Technique 6 years of PhenoCam imagery at Harvard Forest PhenoCam uses imagery from networked digital cameras for continuous monitoring of plant canopies Images are recorded approximately every 30 minutes (every 15 minutes for NEON), sunrise to sunset, 365 days a year The scale of observations is comparable to that of tower-based land-atmosphere flux measurements PhenoCams provide a direct link between what is happening on the ground and what is seen by satellites PhenoCams cover a wide array of: Plant Funtional Types (PFTs) Ecoregions Spatial distribution of PhenoCam data across ecological regions of North America. Background map illustrates USA Environmental Protection Agency Level I Ecoregions. Data counts have been aggregated to a spatial resolution of 4°, and the size of each circle corresponds to the number of site-years of data in the 4×4° grid cell. Sites in Hawaii, Puerto Rico, Central and South America, Europe, Asia and Africa (total of 88 site years) are not shown. (Seyednasrollah et al., 2019) Co-Located Networks Flux towers NEON LTER/LTAR Ambient vs. Experimental set ups SEGA sites SPRUCE Experiment 4.3.2 How PhenoCams Pull Data 4.3.3 Leveraging camera near-infrared (NIR) capabilities Petach et al., Agricultural &amp; Forest Meteorology 2014 CMOS sensor is sensitive to &gt; 700 nm A software-controlled filter enables sequential VIS (RGB color) and VIS+NIR (monochrome) images Potential applications: false color images camera NDVI as alternative to GCC 4.4 Digital Repeat Photography Written Questions Suggested completion: Before Digital Repeat Photography methods (day 2 PhenoCam) Question 1: What do you see as the value of the images themselves? The 1 or 3-day products, the transition dates? How could they be used for different applications? Question 2: Why does PhenoCam take photos every 15-30 minutes, but summarize to 1 or 3-day products? Question 3: Why does canopy color coordinate with photosynthesis in many ecosystems? Name an example of a situation where it wouldnt and explain why. Question 4: Why are there sometimes multiple Regions of Interest (ROIs) for a PhenoCam? Question 5: How might or does the PhenoCam project intersect with your current research or future career goals? (1 paragraph) Question 6: Use the map on the PhenoCam website to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there PhenoCams that are in study regions of interest to you? Which PhenoCam sites does your current research or final project ideas coincide with? Are they connected to other networks (e.g. LTAR, NEON, Fluxnet)? What is the data record length for the sites youre interested in? Question 7: Consider either your current or future research, or a question youd like to address during this course: Which types of PhenoCam data may be more useful to address these questions? What non-PhenoCam data resources could be combined to help address your question? What challenges, if any, could you foresee when beginning to work with these data? 4.5 Introduction to Digital Repeat Photography Methods The concept of repeat photography for studying environmental has been introduced to scientists long time ago (See Stephens et al., 1987). But in the past decade the idea has gained much popularity for monitoring environmental change (e.g., Sonnentag et al., 2012). One of the main applications of digital repeat photography is studying vegetation phenology for a diverse range of ecosystems and biomes (Richardson et al., 2019). The methods has also shown great applicability in other fields such as: assessing the seasonality of gross primary production, salt marsh restoration, monitoring tidal wetlands, investigating growth in croplands, and evaluating phenological data products derived from satellite remote sensing. Obtaining quantitative data from digital repeat photography images is usually performed by defining appropriate region of interest, also know as ROIs, and for the red (R), green (G) and blue (B) color channels, calculating pixel value (intensity) statistics across the pixels within each ROI. ROI boundaries are delineated by mask files which define which pixels are included and which are excluded from these calculations. The masks are then used to extract color-based time series from a stack of images. Following the time-series, statistical metrics are used to obtain 1-day and 3-day summary time series. From the summary product time series, phenological transition dates corresponding to the start and the end of green-up and green-down phenological phases are calculated. In this chapter we explain this process by starting from general image processing tools and then to phenocam-based software applications. For more details about digital repeat photogrpahy you can check out the following publications: - Seyednarollah, et al. 2019, Tracking vegetation phenology across diverse biomes using Version 2.0 of the PhenoCam Dataset. - Seyednarollah, et al. 2019, Data extraction from digital repeat photography using xROI: An interactive framework to facilitate the process. 4.6 Pulling Data via the phenocamapi Package The https://github.com/bnasr/phenocamapi phenocamapi R package is developed to simplify interacting with the PhenoCam network dataset and perform data wrangling steps on PhenoCam sites data and metadata. This tutorial will show you the basic commands for accessing PhenoCam data through the PhenoCam API. The phenocampapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available on GitHub (PhenocamAPI). Additional vignettes can be found on how to merge external time-series (e.g. Flux data) with the PhenoCam time-series. We begin with several useful skills and tools for extracting PhenoCam data directly from the server: Exploring the PhenoCam metadata Filtering the dataset by site attributes Extracting the list of midday images Downloading midday images for a given time range 4.7 Exploring PhenoCam metadata Each PhenoCam site has specific metadata including but not limited to how a site is set up and where it is located, what vegetation type is visible from the camera, and its climate regime. Each PhenoCam may have zero to several Regions of Interest (ROIs) per vegetation type. The phenocamapi package is an interface to interact with the PhenoCam server to extract those data and process them in an R environment. To explore the PhenoCam data, well use several packages for this tutorial. library(data.table) library(phenocamapi) ## Loading required package: rjson ## Loading required package: RCurl library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(jpeg) We can obtain an up-to-date data.frame of the metadata of the entire PhenoCam network using the get_phenos() function. The returning value would be a data.table in order to simplify further data exploration. # obtaining the phenocam site metadata from the server as data.table phenos &lt;- phenocamapi::get_phenos() # checking out the first few sites head(phenos$site) ## [1] &quot;aafcottawacfiaf14e&quot; &quot;aafcottawacfiaf14n&quot; &quot;aafcottawacfiaf14w&quot; ## [4] &quot;acadia&quot; &quot;admixpasture&quot; &quot;adrycpasture&quot; # checking out the columns colnames(phenos) ## [1] &quot;site&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;elev&quot; ## [5] &quot;active&quot; &quot;utc_offset&quot; ## [7] &quot;date_first&quot; &quot;date_last&quot; ## [9] &quot;infrared&quot; &quot;contact1&quot; ## [11] &quot;contact2&quot; &quot;site_description&quot; ## [13] &quot;site_type&quot; &quot;group&quot; ## [15] &quot;camera_description&quot; &quot;camera_orientation&quot; ## [17] &quot;flux_data&quot; &quot;flux_networks&quot; ## [19] &quot;flux_sitenames&quot; &quot;dominant_species&quot; ## [21] &quot;primary_veg_type&quot; &quot;secondary_veg_type&quot; ## [23] &quot;site_meteorology&quot; &quot;MAT_site&quot; ## [25] &quot;MAP_site&quot; &quot;MAT_daymet&quot; ## [27] &quot;MAP_daymet&quot; &quot;MAT_worldclim&quot; ## [29] &quot;MAP_worldclim&quot; &quot;koeppen_geiger&quot; ## [31] &quot;ecoregion&quot; &quot;landcover_igbp&quot; ## [33] &quot;dataset_version1&quot; &quot;site_acknowledgements&quot; ## [35] &quot;modified&quot; &quot;flux_networks_name&quot; ## [37] &quot;flux_networks_url&quot; &quot;flux_networks_description&quot; Now we have a better idea of the types of metadata that are available for the Phenocams. 4.7.1 Remove null values We may want to explore some of the patterns in the metadata before we jump into specific locations. Lets look at Mean Annual Precipitation (MAP) and Mean Annual Temperature (MAT) across the different field site and classify those by the primary vegetation type (primary_veg_type) for each site. We can find out what the abbreviations for the vegetation types mean from the following table: Abbreviation Description AG agriculture DB deciduous broadleaf DN deciduous needleleaf EB evergreen broadleaf EN evergreen needleleaf GR grassland MX mixed vegetation (generally EN/DN, DB/EN, or DB/EB) SH shrubs TN tundra (includes sedges, lichens, mosses, etc.) WT wetland NV non-vegetated RF reference panel XX unspecified To do this wed first want to remove the sites where there is not data and then plot the data. # removing the sites with unkown MAT and MAP values phenos &lt;- phenos[!((MAT_worldclim == -9999)|(MAP_worldclim == -9999))] # extracting the PhenoCam climate space based on the WorldClim dataset # and plotting the sites across the climate space different vegetation type as different symbols and colors phenos[primary_veg_type==&#39;DB&#39;, plot(MAT_worldclim, MAP_worldclim, pch = 19, col = &#39;green&#39;, xlim = c(-5, 27), ylim = c(0, 4000))] ## NULL phenos[primary_veg_type==&#39;DN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 1, col = &#39;darkgreen&#39;)] ## NULL phenos[primary_veg_type==&#39;EN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 17, col = &#39;brown&#39;)] ## NULL phenos[primary_veg_type==&#39;EB&#39;, points(MAT_worldclim, MAP_worldclim, pch = 25, col = &#39;orange&#39;)] ## NULL phenos[primary_veg_type==&#39;AG&#39;, points(MAT_worldclim, MAP_worldclim, pch = 12, col = &#39;yellow&#39;)] ## NULL phenos[primary_veg_type==&#39;SH&#39;, points(MAT_worldclim, MAP_worldclim, pch = 23, col = &#39;red&#39;)] ## NULL legend(&#39;topleft&#39;, legend = c(&#39;DB&#39;,&#39;DN&#39;, &#39;EN&#39;,&#39;EB&#39;,&#39;AG&#39;, &#39;SH&#39;), pch = c(19, 1, 17, 25, 12, 23), col = c(&#39;green&#39;, &#39;darkgreen&#39;, &#39;brown&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;red&#39; )) 4.7.2 Filtering using attributes Alternatively, we may want to only include Phenocams with certain attributes in our datasets. For example, we may be interested only in sites with a co-located flux tower. For this, wed want to filter for those with a flux tower using the flux_sitenames attribute in the metadata. # store sites with flux_data available and the FLUX site name is specified phenofluxsites &lt;- phenos[flux_data==TRUE&amp;!is.na(flux_sitenames)&amp;flux_sitenames!=&#39;&#39;, .(PhenoCam=site, Flux=flux_sitenames)] # return as table #and specify which variables to retain phenofluxsites &lt;- phenofluxsites[Flux!=&#39;&#39;] # see the first few rows head(phenofluxsites) ## PhenoCam Flux ## 1: admixpasture NZ-ADw ## 2: alligatorriver US-NC4 ## 3: arkansaswhitaker US-RGW ## 4: arsbrooks10 US-Br1: Brooks Field Site 10- Ames ## 5: arsbrooks11 US-Br3: Brooks Field Site 11- Ames ## 6: arscolesnorth LTAR We could further identify which of those Phenocams with a flux tower and in deciduous broadleaf forests (primary_veg_type=='DB'). #list deciduous broadleaf sites with flux tower DB.flux &lt;- phenos[flux_data==TRUE&amp;primary_veg_type==&#39;DB&#39;, site] # return just the site names as a list # see the first few rows head(DB.flux) ## [1] &quot;alligatorriver&quot; &quot;bartlett&quot; &quot;bartlettir&quot; &quot;bbc1&quot; ## [5] &quot;bbc2&quot; &quot;bbc3&quot; 4.8 Download midday images While PhenoCam sites may have many images in a given day, many simple analyses can use just the midday image when the sun is most directly overhead the canopy. Therefore, extracting a list of midday images (only one image a day) can be useful. # obtaining midday_images for dukehw duke_middays &lt;- get_midday_list(&#39;dukehw&#39;) # see the first few rows head(duke_middays) ## [1] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/05/dukehw_2013_05_31_150331.jpg&quot; ## [2] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/06/dukehw_2013_06_01_120111.jpg&quot; ## [3] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/06/dukehw_2013_06_02_120109.jpg&quot; ## [4] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/06/dukehw_2013_06_03_120110.jpg&quot; ## [5] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/06/dukehw_2013_06_04_120119.jpg&quot; ## [6] &quot;http://phenocam.nau.edu/data/archive/dukehw/2013/06/dukehw_2013_06_05_120110.jpg&quot; Now we have a list of all the midday images from this Phenocam. Lets download them and plot # download a file destfile &lt;- tempfile(fileext = &#39;.jpg&#39;) # download only the first available file # modify the `[1]` to download other images download.file(duke_middays[1], destfile = destfile, mode = &#39;wb&#39;) # plot the image img &lt;- try(readJPEG(destfile)) if(class(img)!=&#39;try-error&#39;){ par(mar= c(0,0,0,0)) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) } 4.8.1 Download midday images for a given time range Now we can access all the midday images and download them one at a time. However, we frequently want all the images within a specific time range of interest. Well learn how to do that next. # open a temporary directory tmp_dir &lt;- tempdir() # download a subset. Example dukehw 2017 download_midday_images(site = &#39;dukehw&#39;, # which site y = 2017, # which year(s) months = 1:12, # which month(s) days = 15, # which days on month(s) download_dir = tmp_dir) # where on your computer ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% ## [1] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o&quot; # list of downloaded files duke_middays_path &lt;- dir(tmp_dir, pattern = &#39;dukehw*&#39;, full.names = TRUE) head(duke_middays_path) ## [1] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_01_15_120109.jpg&quot; ## [2] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_02_15_120108.jpg&quot; ## [3] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_03_15_120151.jpg&quot; ## [4] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_04_15_120110.jpg&quot; ## [5] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_05_15_120108.jpg&quot; ## [6] &quot;C:\\\\Users\\\\rohan\\\\AppData\\\\Local\\\\Temp\\\\RtmpsfVW7o/dukehw_2017_06_15_120120.jpg&quot; We can demonstrate the seasonality of Duke forest observed from the camera. (Note this code may take a while to run through the loop). n &lt;- length(duke_middays_path) par(mar= c(0,0,0,0), mfrow=c(4,3), oma=c(0,0,3,0)) for(i in 1:n){ img &lt;- readJPEG(duke_middays_path[i]) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) mtext(month.name[i], line = -2) } mtext(&#39;Seasonal variation of forest at Duke Hardwood Forest&#39;, font = 2, outer = TRUE) The goal of this section was to show how to download a limited number of midday images from the PhenoCam server. However, more extensive datasets should be downloaded from the PhenoCam . The phenocamapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/phenocamapi. 4.9 Detecting Foggy Images using the hazer R Package 4.9.0.1 Read &amp; Plot an Image We will use several packages in this tutorial. All are available from CRAN. # load packages library(hazer) library(jpeg) library(data.table) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Before we start the image processing steps, lets read in and plot an image. This image is an example image that comes with the hazer package. # read the path to the example image jpeg_file &lt;- system.file(package = &#39;hazer&#39;, &#39;pointreyes.jpg&#39;) # read the image as an array rgb_array &lt;- jpeg::readJPEG(jpeg_file) # plot the RGB array on the active device panel # first set the margin in this order:(bottom, left, top, right) par(mar=c(0,0,3,0)) plotRGBArray(rgb_array, bty = &#39;n&#39;, main = &#39;Point Reyes National Seashore&#39;) When we work with images, all data we work with is generally on the scale of each individual pixel in the image. Therefore, for large images we will be working with large matrices that hold the value for each pixel. Keep this in mind before opening some of the matrices well be creating this tutorial as it can take a while for them to load. 4.9.0.2 Histogram of RGB channels A histogram of the colors can be useful to understanding what our image is made up of. Using the density() function from the base stats package, we can extract density distribution of each color channel. # color channels can be extracted from the matrix red_vector &lt;- rgb_array[,,1] green_vector &lt;- rgb_array[,,2] blue_vector &lt;- rgb_array[,,3] # plotting par(mar=c(5,4,4,2)) plot(density(red_vector), col = &#39;red&#39;, lwd = 2, main = &#39;Density function of the RGB channels&#39;, ylim = c(0,5)) lines(density(green_vector), col = &#39;green&#39;, lwd = 2) lines(density(blue_vector), col = &#39;blue&#39;, lwd = 2) In hazer we can also extract three basic elements of an RGB image : Brightness Darkness Contrast 4.9.0.3 Brightness The brightness matrix comes from the maximum value of the R, G, or B channel. We can extract and show the brightness matrix using the getBrightness() function. # extracting the brightness matrix brightness_mat &lt;- getBrightness(rgb_array) # unlike the RGB array which has 3 dimensions, the brightness matrix has only two # dimensions and can be shown as a grayscale image, # we can do this using the same plotRGBArray function par(mar=c(0,0,3,0)) plotRGBArray(brightness_mat, bty = &#39;n&#39;, main = &#39;Brightness matrix&#39;) Here the grayscale is used to show the value of each pixels maximum brightness of the R, G or B color channel. To extract a single brightness value for the image, depending on our needs we can perform some statistics or we can just use the mean of this matrix. # the main quantiles quantile(brightness_mat) ## 0% 25% 50% 75% 100% ## 0.09019608 0.43529412 0.62745098 0.80000000 0.91764706 # create histogram par(mar=c(5,4,4,2)) hist(brightness_mat) Question for the class: Why are we getting so many images up in the high range of the brightness? Where does this correlate to on the RGB image? 4.9.0.4 Darkness Darkness is determined by the minimum of the R, G or B color channel. In the Similarly, we can extract and show the darkness matrix using the getDarkness() function. # extracting the darkness matrix darkness_mat &lt;- getDarkness(rgb_array) # the darkness matrix has also two dimensions and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(darkness_mat, bty = &#39;n&#39;, main = &#39;Darkness matrix&#39;) # main quantiles quantile(darkness_mat) ## 0% 25% 50% 75% 100% ## 0.03529412 0.23137255 0.36470588 0.47843137 0.83529412 # histogram par(mar=c(5,4,4,2)) hist(darkness_mat) 4.9.0.5 Contrast The contrast of an image is the difference between the darkness and brightness of the image. The contrast matrix is calculated by difference between the darkness and brightness matrices. The contrast of the image can quickly be extracted using the getContrast() function. # extracting the contrast matrix contrast_mat &lt;- getContrast(rgb_array) # the contrast matrix has also 2D and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(contrast_mat, bty = &#39;n&#39;, main = &#39;Contrast matrix&#39;) # main quantiles quantile(contrast_mat) ## 0% 25% 50% 75% 100% ## 0.0000000 0.1450980 0.2470588 0.3333333 0.4509804 # histogram par(mar=c(5,4,4,2)) hist(contrast_mat) 4.9.0.6 Image fogginess &amp; haziness Haziness of an image can be estimated using the getHazeFactor() function. This function is based on the method described in Mao et al. (2014). The technique was originally developed to for detecting foggy images and estimating the haze degree factor for a wide range of outdoor conditions. The function returns a vector of two numeric values: haze as the haze degree and A0 as the global atmospheric light, as it is explained in the original paper. The PhenoCam standards classify any image with the haze degree greater than 0.4 as a significantly foggy image. # extracting the haze matrix haze_degree &lt;- getHazeFactor(rgb_array) print(haze_degree) ## $haze ## [1] 0.2251633 ## ## $A0 ## [1] 0.7105258 Here we have the haze values for our image. Note that the values might be slightly different due to rounding errors on different platforms. 4.9.0.7 Process sets of images We can use for loops or the lapply functions to extract the haze values for a stack of images. You can download the related datasets from here (direct download). Download and extract the zip file to be used as input data for the following step. #pointreyes_url &lt;- &#39;http://bit.ly/2F8w2Ia&#39; # set up the input image directory data_dir &lt;- &#39;data/&#39; #dir.create(data_dir, showWarnings = F) #pointreyes_zip &lt;- paste0(data_dir, &#39;pointreyes.zip&#39;) pointreyes_dir &lt;- paste0(data_dir, &#39;pointreyes&#39;) #download zip file #download.file(pointreyes_url, destfile = pointreyes_zip) #unzip(pointreyes_zip, exdir = data_dir) # get a list of all .jpg files in the directory pointreyes_images &lt;- dir(path = &#39;data/pointreyes&#39;, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) Now we can use a for loop to process all of the images to get the haze and A0 values. # number of images n &lt;- length(pointreyes_images) # create an empty matrix to fill with haze and A0 values haze_mat &lt;- data.frame() # the process takes a bit, a progress bar lets us know it is working. pb &lt;- txtProgressBar(0, n, style = 3) ## | | | 0% for(i in 1:n) { image_path &lt;- pointreyes_images[i] img &lt;- jpeg::readJPEG(image_path) hz &lt;- getHazeFactor(img) haze_mat &lt;- rbind(haze_mat, data.frame(file = as.character(image_path), haze = hz[1], A0 = hz[2])) setTxtProgressBar(pb, i) } ## | |= | 1% | |== | 3% | |=== | 4% | |==== | 6% | |===== | 7% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 15% | |============ | 17% | |============= | 18% | |============== | 20% | |=============== | 21% | |================ | 23% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 31% | |======================= | 32% | |======================== | 34% | |========================= | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 39% | |============================= | 41% | |============================== | 42% | |=============================== | 44% | |================================ | 45% | |================================= | 46% | |================================== | 48% | |=================================== | 49% | |=================================== | 51% | |==================================== | 52% | |===================================== | 54% | |====================================== | 55% | |======================================= | 56% | |======================================== | 58% | |========================================= | 59% | |========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================= | 65% | |============================================== | 66% | |=============================================== | 68% | |================================================ | 69% | |================================================= | 70% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |======================================================= | 79% | |======================================================== | 80% | |========================================================= | 82% | |========================================================== | 83% | |=========================================================== | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 94% | |=================================================================== | 96% | |==================================================================== | 97% | |===================================================================== | 99% | |======================================================================| 100% Now we have a matrix with haze and A0 values for all our images. Lets compare top three images with low and high haze values. top10_high_haze &lt;- haze_mat %&gt;% dplyr::arrange(desc(haze)) %&gt;% slice(1:3) top10_low_haze &lt;- haze_mat %&gt;% arrange(haze)%&gt;% slice(1:3) par(mar= c(0,0,0,0), mfrow=c(3,2), oma=c(0,0,3,0)) for(i in 1:3){ img &lt;- readJPEG(as.character(top10_low_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) img &lt;- readJPEG(as.character(top10_high_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) } mtext(&#39;Top images with low (left) and high (right) haze values at Point Reyes&#39;, font = 2, outer = TRUE) Lets classify those into hazy and non-hazy as per the PhenoCam standard of 0.4. # classify image as hazy: T/F haze_mat=haze_mat%&gt;% mutate(haze_mat, foggy=ifelse(haze&gt;.4, TRUE, FALSE)) head(haze_mat) ## file haze A0 foggy ## 1 data/pointreyes/pointreyes_2017_01_01_120056.jpg 0.2249810 0.6970257 FALSE ## 2 data/pointreyes/pointreyes_2017_01_06_120210.jpg 0.2339372 0.6826148 FALSE ## 3 data/pointreyes/pointreyes_2017_01_16_120105.jpg 0.2312940 0.7009978 FALSE ## 4 data/pointreyes/pointreyes_2017_01_21_120105.jpg 0.4536108 0.6209055 TRUE ## 5 data/pointreyes/pointreyes_2017_01_26_120106.jpg 0.2297961 0.6813884 FALSE ## 6 data/pointreyes/pointreyes_2017_01_31_120125.jpg 0.4206842 0.6315728 TRUE Now we can save all the foggy images to a new folder that will retain the foggy images but keep them separate from the non-foggy ones that we want to analyze. # identify directory to move the foggy images to foggy_dir &lt;- paste0(pointreyes_dir, &#39;foggy&#39;) clear_dir &lt;- paste0(pointreyes_dir, &#39;clear&#39;) # if a new directory, create new directory at this file path dir.create(foggy_dir, showWarnings = FALSE) dir.create(clear_dir, showWarnings = FALSE) # copy the files to the new directories #file.copy(haze_mat[foggy==TRUE,file], to = foggy_dir) #file.copy(haze_mat[foggy==FALSE,file], to = clear_dir) Now that we have our images separated, we can get the full list of haze values only for those images that are not classified as foggy. # this is an alternative approach instead of a for loop # loading all the images as a list of arrays pointreyes_clear_images &lt;- dir(path = clear_dir, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) img_list &lt;- lapply(pointreyes_clear_images, FUN = jpeg::readJPEG) # getting the haze value for the list # patience - this takes a bit of time haze_list &lt;- t(sapply(img_list, FUN = getHazeFactor)) # view first few entries head(haze_list) ## haze A0 ## [1,] 0.224981 0.6970257 ## [2,] 0.2339372 0.6826148 ## [3,] 0.231294 0.7009978 ## [4,] 0.2297961 0.6813884 ## [5,] 0.2152078 0.6949932 ## [6,] 0.345584 0.6789334 We can then use these values for further analyses and data correction. The hazer R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/hazer. 4.10 Extracting Timeseries from Images using the xROI R Package In this section, well learn how to use an interactive open-source toolkit, the xROI R package that facilitates the process of time series extraction and improves the quality of the final data. The xROI package provides a responsive environment for scientists to interactively: delineate regions of interest (ROIs), handle field of view (FOV) shifts, and extract and export time series data characterizing color-based metrics. Using the xROI R package, the user can detect FOV shifts with minimal difficulty. The software gives user the opportunity to re-adjust mask files or redraw new ones every time an FOV shift occurs. 4.10.1 xROI Design The R language and Shiny package were used as the main development tool for xROI, while Markdown, HTML, CSS and JavaScript languages were used to improve the interactivity. While Shiny apps are primarily used for web-based applications to be used online, the package authors used Shiny for its graphical user interface capabilities. In other words, both the User Interface (UI) and server modules are run locally from the same machine and hence no internet connection is required (after installation). The xROIs UI element presents a side-panel for data entry and three main tab-pages, each responsible for a specific task. The server-side element consists of R and bash scripts. Image processing and geospatial features were performed using the Geospatial Data Abstraction Library (GDAL) and the rgdal and raster R packages. 4.10.2 Install xROI The xROI R package has been published on The Comprehensive R Archive Network (CRAN). The latest tested xROI package can be installed from the CRAN packages repository by running the following command in an R environment. utils::install.packages(&#39;xROI&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) Alternatively, the latest beta release of xROI can be directly downloaded and installed from the development GitHub repository. # install devtools first utils::install.packages(&#39;devtools&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) # use devtools to install from GitHub devtools::install_github(&quot;bnasr/xROI&quot;) xROI depends on many R packages including: raster, rgdal, sp, jpeg, tiff, shiny, shinyjs, shinyBS, shinyAce, shinyTime, shinyFiles, shinydashboard, shinythemes, colourpicker, rjson, stringr, data.table, lubridate, plotly, moments, and RCurl. All the required libraries and packages will be automatically installed with installation of xROI. The package offers a fully interactive high-level interface as well as a set of low-level functions for ROI processing. 4.10.3 Launch xROI A comprehensive user manual for low-level image processing using xROI is available from CRAN xROI.pdf. While the user manual includes a set of examples for each function; here we will learn to use the graphical interactive mode. Calling the Launch() function, as well do below, opens up the interactive mode in your operating systems default web browser. The landing page offers an example dataset to explore different modules or upload a new dataset of images. You can launch the interactive mode can be launched from an interactive R environment. # load xROI library(xROI) # launch xROI Launch() Or from the command line (e.g. bash in Linux, Terminal in macOS and Command Prompt in Windows machines) where an R engine is already installed. Rscript -e xROI::Launch(Interactive = TRUE) 4.10.4 End xROI When you are done with the xROI interface you can close the tab in your browser and end the session in R by using one of the following options In RStudio: Press the key on your keyboard. In R Terminal: Press &lt;Ctrl + C&gt; on your keyboard. 4.10.5 Use xROI To get some hands-on experience with xROI, we can analyze images from the dukehw of the PhenoCam network. You can download the data set from this link (direct download). Follow the steps below: First,save and extract (unzip) the file on your computer. Second, open the data set in xROI by setting the file path to your data # launch data in ROI # first edit the path below to the dowloaded directory you just extracted xROI::Launch(&#39;/path/to/extracted/directory&#39;) # alternatively, you can run without specifying a path and use the interface to browse Now, draw an ROI and the metadata. Then, save the metadata and explore its content. Now we can explore if there is any FOV shift in the dataset using the CLI processer tab. Finally, we can go to the Time series extraction tab. Extract the time-series. Save the output and explore the dataset in R. 4.11 Documentation and Citation More documentation about xROI can be found from: Seyednarollah, et al. 2019. knitr::include_graphics(&#39;docs/images/xROI-ms2019.png&#39;) &gt;xROI published in ISPRS Journal of Photogrammetry and Remote Sensing, 2019 4.11.1 Challenge: Use xROI Lets use xROI on a little more challenging site with field of view shifts. Download and extract the data set from this link (direct download, 218 MB) and follow the above steps to extract the time-series. The xROI R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/xROI. 4.12 Hands on: Digital Repeat Photography Computational First lets load some packages: library(jsonlite) ## ## Attaching package: &#39;jsonlite&#39; ## The following objects are masked from &#39;package:rjson&#39;: ## ## fromJSON, toJSON library(phenocamapi) library(plotly) ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(phenocamr) library(dplyr) As a refresher, there are two main ways to pull in PhenoCam data. First, directly via the API: c = jsonlite::fromJSON(&#39;https://phenocam.sr.unh.edu/api/cameras/?format=json&amp;limit=2000&#39;) c = c$results c_m=c$sitemetadata c$sitemetadata=NULL cams_=cbind(c, c_m) cams_[is.na(cams_)] = &#39;N&#39; cams_[, 2:4] &lt;- sapply(cams_[, 2:4], as.numeric) #changing lat/lon/elev from string values into numeric head(cams_) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2 aafcottawacfiaf14n 45.29290 -75.76700 90 TRUE -5 2021-09-03 ## 3 aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 4 acadia 44.37694 -68.26083 158 FALSE -5 2007-03-15 ## 5 admixpasture -43.64930 172.34950 33 TRUE 12 2021-03-04 ## 6 adrycpasture -43.65130 172.35010 31 TRUE 12 2021-03-02 ## date_last infrared ## 1 2022-09-26 N ## 2 2022-09-26 Y ## 3 2022-09-26 N ## 4 2022-01-07 N ## 5 2022-09-08 Y ## 6 2022-09-08 Y ## contact1 ## 1 Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 2 Elizabeth Pattey &lt;elizabeth DOT pattey AT agr DOT gc DOT ca&gt; ## 3 Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 4 Dee Morse &lt;dee_morse AT nps DOT gov&gt; ## 5 John Hunt &lt;huntj AT landcareresearch DOT co DOT nz&gt; ## 6 John Hunt &lt;huntj AT landcareresearch DOT co DOT nz&gt; ## contact2 ## 1 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 2 Luc Pelletier &lt;luc DOT pelletier3 AT agr DOT gc DOT ca&gt; ## 3 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 4 John Gross &lt;John_Gross AT nps DOT gov&gt; ## 5 Scott Graham &lt;grahams AT landcareresearch DOT co DOT nz&gt; ## 6 Scott Graham &lt;grahams AT landcareresearch DOT co DOT nz&gt; ## site_description ## 1 AAFC Site - Ottawa (On) - CFIA - Field F14 - East Flux Tower ## 2 AAFC Site - Ottawa (On) - CFIA - Field F14 - North Section ## 3 AAFC Site - Ottawa (On) - CFIA - Field F14 - West Flux Tower ## 4 Acadia National Park, McFarland Hill, near Bar Harbor, Maine ## 5 Eddy site, mixed species irrigated dairy pasture, Ashley Dene Research &amp; Development Station, South Island, New Zealand ## 6 Ryegrass-clover irrigated dairy pasture, Ashley Dene Research &amp; Development Station, South Island, New Zealand ## site_type group camera_description camera_orientation ## 1 II N Campbell Scientific CCFC NE ## 2 I N StarDot NetCam SC NNW ## 3 II N Campbell Scientific CCFC WNW ## 4 III National Park Service unknown NE ## 5 I N StarDot NetCam SC ## 6 I N StarDot NetCamSC ## flux_data flux_networks flux_sitenames ## 1 TRUE NULL N ## 2 TRUE NULL ## 3 TRUE OTHER, , Other/Unaffiliated N ## 4 FALSE NULL ## 5 TRUE NULL NZ-ADw ## 6 TRUE NULL ## dominant_species ## 1 Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 2 Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 3 Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 4 ## 5 Lolium perenne, Lolium multiflorum, Trifolium pratense, Trifolium repens, Plantago lanceolata ## 6 Lolium perenne, Trifolium repens ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 1 AG AG TRUE 6.4 943 ## 2 AG AG TRUE 6.4 943 ## 3 AG AG TRUE 6.4 943 ## 4 DB EN FALSE N N ## 5 AG TRUE N N ## 6 AG TRUE N N ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 1 6.3 952 6 863 Dfb 8 ## 2 6.3 953 5.9 863 Dfb 8 ## 3 6.3 952 6 863 Dfb 8 ## 4 7.05 1439 6.5 1303 Dfb 8 ## 5 N N 11.6 640 Cfb N ## 6 N N 11.6 639 Cfb N ## landcover_igbp ## 1 12 ## 2 12 ## 3 12 ## 4 5 ## 5 N ## 6 N ## site_acknowledgements ## 1 Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth  Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2 Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3 Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth  Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 4 Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 5 Ministry of Business, Innovation and Employment; New Zealand Agricultural Greenhouse Gas Research Centre; Manaaki Whenua ## 6 Ministry of Business, Innovation and Employment; New Zealand Agricultural Greenhouse Gas Research Centre; Manaaki Whenua ## modified ## 1 2021-08-23T12:12:59.295391-04:00 ## 2 2021-08-13T10:49:30.502306-04:00 ## 3 2021-08-23T12:13:11.193254-04:00 ## 4 2016-11-01T15:42:15.016778-04:00 ## 5 2021-03-09T17:03:10.816039-05:00 ## 6 2021-03-09T17:03:11.773814-05:00 And second, via the phenocamapi package: phenos=get_phenos() head(phenos) ## site lat lon elev active utc_offset date_first ## 1: aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2: aafcottawacfiaf14n 45.29290 -75.76700 90 TRUE -5 2021-09-03 ## 3: aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 4: acadia 44.37694 -68.26083 158 FALSE -5 2007-03-15 ## 5: admixpasture -43.64930 172.34950 33 TRUE 12 2021-03-04 ## 6: adrycpasture -43.65130 172.35010 31 TRUE 12 2021-03-02 ## date_last infrared contact1 ## 1: 2022-09-26 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 2: 2022-09-26 Y Elizabeth Pattey &lt;elizabeth.pattey@agr.gc.ca&gt; ## 3: 2022-09-26 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 4: 2022-01-07 N Dee Morse &lt;dee_morse@nps.gov&gt; ## 5: 2022-09-08 Y John Hunt &lt;huntj@landcareresearch.co.nz&gt; ## 6: 2022-09-08 Y John Hunt &lt;huntj@landcareresearch.co.nz&gt; ## contact2 ## 1: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 2: Luc Pelletier &lt;luc.pelletier3@agr.gc.ca&gt; ## 3: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 4: John Gross &lt;John_Gross@nps.gov&gt; ## 5: Scott Graham &lt;grahams@landcareresearch.co.nz&gt; ## 6: Scott Graham &lt;grahams@landcareresearch.co.nz&gt; ## site_description ## 1: AAFC Site - Ottawa (On) - CFIA - Field F14 - East Flux Tower ## 2: AAFC Site - Ottawa (On) - CFIA - Field F14 - North Section ## 3: AAFC Site - Ottawa (On) - CFIA - Field F14 - West Flux Tower ## 4: Acadia National Park, McFarland Hill, near Bar Harbor, Maine ## 5: Eddy site, mixed species irrigated dairy pasture, Ashley Dene Research &amp; Development Station, South Island, New Zealand ## 6: Ryegrass-clover irrigated dairy pasture, Ashley Dene Research &amp; Development Station, South Island, New Zealand ## site_type group camera_description camera_orientation ## 1: II &lt;NA&gt; Campbell Scientific CCFC NE ## 2: I &lt;NA&gt; StarDot NetCam SC NNW ## 3: II &lt;NA&gt; Campbell Scientific CCFC WNW ## 4: III National Park Service unknown NE ## 5: I &lt;NA&gt; StarDot NetCam SC ## 6: I &lt;NA&gt; StarDot NetCamSC ## flux_data flux_networks flux_sitenames ## 1: TRUE &lt;list[0]&gt; &lt;NA&gt; ## 2: TRUE &lt;list[0]&gt; ## 3: TRUE &lt;list[1]&gt; &lt;NA&gt; ## 4: FALSE &lt;list[0]&gt; ## 5: TRUE &lt;list[0]&gt; NZ-ADw ## 6: TRUE &lt;list[0]&gt; ## dominant_species ## 1: Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 2: Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 3: Zea mays, Triticum aestivum, Brassica napus, Glycine max ## 4: ## 5: Lolium perenne, Lolium multiflorum, Trifolium pratense, Trifolium repens, Plantago lanceolata ## 6: Lolium perenne, Trifolium repens ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 1: AG AG TRUE 6.4 943 ## 2: AG AG TRUE 6.4 943 ## 3: AG AG TRUE 6.4 943 ## 4: DB EN FALSE NA NA ## 5: AG TRUE NA NA ## 6: AG TRUE NA NA ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 1: 6.30 952 6.0 863 Dfb 8 ## 2: 6.30 953 5.9 863 Dfb 8 ## 3: 6.30 952 6.0 863 Dfb 8 ## 4: 7.05 1439 6.5 1303 Dfb 8 ## 5: NA NA 11.6 640 Cfb NA ## 6: NA NA 11.6 639 Cfb NA ## landcover_igbp dataset_version1 ## 1: 12 NA ## 2: 12 NA ## 3: 12 NA ## 4: 5 NA ## 5: NA NA ## 6: NA NA ## site_acknowledgements ## 1: Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth  Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2: Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3: Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth  Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 4: Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 5: Ministry of Business, Innovation and Employment; New Zealand Agricultural Greenhouse Gas Research Centre; Manaaki Whenua ## 6: Ministry of Business, Innovation and Employment; New Zealand Agricultural Greenhouse Gas Research Centre; Manaaki Whenua ## modified flux_networks_name flux_networks_url ## 1: 2021-08-23T12:12:59.295391-04:00 &lt;NA&gt; &lt;NA&gt; ## 2: 2021-08-13T10:49:30.502306-04:00 &lt;NA&gt; &lt;NA&gt; ## 3: 2021-08-23T12:13:11.193254-04:00 OTHER ## 4: 2016-11-01T15:42:15.016778-04:00 &lt;NA&gt; &lt;NA&gt; ## 5: 2021-03-09T17:03:10.816039-05:00 &lt;NA&gt; &lt;NA&gt; ## 6: 2021-03-09T17:03:11.773814-05:00 &lt;NA&gt; &lt;NA&gt; ## flux_networks_description ## 1: &lt;NA&gt; ## 2: &lt;NA&gt; ## 3: Other/Unaffiliated ## 4: &lt;NA&gt; ## 5: &lt;NA&gt; ## 6: &lt;NA&gt; To familiarize yourself with the phenocam API, lets explore the structure: https://phenocam.sr.unh.edu/api/ Explore the options for filtering, file type and so forth. 4.12.1 PhenoCam time series PhenoCam time series are extracted time series data obtained from ROIs for a given site. 4.12.2 Obtain ROIs To download the phenological time series from the PhenoCam, we need to know the site name, vegetation type and ROI ID. This information can be obtained from each specific PhenoCam page on the PhenoCam website or by using the get_rois() function. # obtaining the list of all the available ROI&#39;s on the PhenoCam server rois &lt;- get_rois() # view what information is returned colnames(rois) ## [1] &quot;roi_name&quot; &quot;site&quot; &quot;lat&quot; ## [4] &quot;lon&quot; &quot;roitype&quot; &quot;active&quot; ## [7] &quot;show_link&quot; &quot;show_data_link&quot; &quot;sequence_number&quot; ## [10] &quot;description&quot; &quot;first_date&quot; &quot;last_date&quot; ## [13] &quot;site_years&quot; &quot;missing_data_pct&quot; &quot;roi_page&quot; ## [16] &quot;roi_stats_file&quot; &quot;one_day_summary&quot; &quot;three_day_summary&quot; ## [19] &quot;data_release&quot; # view first few locations head(rois$roi_name) ## [1] &quot;aafcottawacfiaf14n_AG_1000&quot; &quot;admixpasture_AG_1000&quot; ## [3] &quot;adrycpasture_AG_1000&quot; &quot;alligatorriver_DB_1000&quot; ## [5] &quot;arbutuslake_DB_1000&quot; &quot;arbutuslakeinlet_DB_1000&quot; 4.12.3 Download time series The get_pheno_ts() function can download a time series and return the result as a data.table. Lets work with the Duke Forest Hardwood Stand (dukehw) PhenoCam and specifically the ROI DB_1000 we can run the following code. # list ROIs for dukehw rois[site==&#39;dukehw&#39;,] ## roi_name site lat lon roitype active show_link ## 1: dukehw_DB_1000 dukehw 35.97358 -79.10037 DB TRUE TRUE ## show_data_link sequence_number description ## 1: TRUE 1000 canopy level DB forest at awesome Duke forest ## first_date last_date site_years missing_data_pct ## 1: 2013-06-01 2022-09-29 8.4 9.0 ## roi_page ## 1: https://phenocam.nau.edu/webcam/roi/dukehw/DB_1000/ ## roi_stats_file ## 1: https://phenocam.nau.edu/data/archive/dukehw/ROI/dukehw_DB_1000_roistats.csv ## one_day_summary ## 1: https://phenocam.nau.edu/data/archive/dukehw/ROI/dukehw_DB_1000_1day.csv ## three_day_summary ## 1: https://phenocam.nau.edu/data/archive/dukehw/ROI/dukehw_DB_1000_3day.csv ## data_release ## 1: NA # to obtain the DB 1000 from dukehw dukehw_DB_1000 &lt;- get_pheno_ts(site = &#39;dukehw&#39;, vegType = &#39;DB&#39;, roiID = 1000, type = &#39;3day&#39;) # what data are available str(dukehw_DB_1000) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 1139 obs. of 35 variables: ## $ date : chr &quot;2013-06-01&quot; &quot;2013-06-04&quot; &quot;2013-06-07&quot; &quot;2013-06-10&quot; ... ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ doy : int 152 155 158 161 164 167 170 173 176 179 ... ## $ image_count : int 57 76 77 77 77 78 21 0 0 0 ... ## $ midday_filename : chr &quot;dukehw_2013_06_01_120111.jpg&quot; &quot;dukehw_2013_06_04_120119.jpg&quot; &quot;dukehw_2013_06_07_120112.jpg&quot; &quot;dukehw_2013_06_10_120108.jpg&quot; ... ## $ midday_r : num 91.3 76.4 60.6 76.5 88.9 ... ## $ midday_g : num 97.9 85 73.2 82.2 95.7 ... ## $ midday_b : num 47.4 33.6 35.6 37.1 51.4 ... ## $ midday_gcc : num 0.414 0.436 0.432 0.42 0.406 ... ## $ midday_rcc : num 0.386 0.392 0.358 0.391 0.377 ... ## $ r_mean : num 87.6 79.9 72.7 80.9 83.8 ... ## $ r_std : num 5.9 6 9.5 8.23 5.89 ... ## $ g_mean : num 92.1 86.9 84 88 89.7 ... ## $ g_std : num 6.34 5.26 7.71 7.77 6.47 ... ## $ b_mean : num 46.1 38 39.6 43.1 46.7 ... ## $ b_std : num 4.48 3.42 5.29 4.73 4.01 ... ## $ gcc_mean : num 0.408 0.425 0.429 0.415 0.407 ... ## $ gcc_std : num 0.00859 0.0089 0.01318 0.01243 0.01072 ... ## $ gcc_50 : num 0.408 0.427 0.431 0.416 0.407 ... ## $ gcc_75 : num 0.414 0.431 0.435 0.424 0.415 ... ## $ gcc_90 : num 0.417 0.434 0.44 0.428 0.421 ... ## $ rcc_mean : num 0.388 0.39 0.37 0.381 0.38 ... ## $ rcc_std : num 0.01176 0.01032 0.01326 0.00881 0.00995 ... ## $ rcc_50 : num 0.387 0.391 0.373 0.383 0.382 ... ## $ rcc_75 : num 0.391 0.396 0.378 0.388 0.385 ... ## $ rcc_90 : num 0.397 0.399 0.382 0.391 0.389 ... ## $ max_solar_elev : num 76 76.3 76.6 76.8 76.9 ... ## $ snow_flag : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_mean: logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_50 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_75 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_90 : logi NA NA NA NA NA NA ... ## $ YEAR : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ DOY : int 152 155 158 161 164 167 170 173 176 179 ... ## $ YYYYMMDD : chr &quot;2013-06-01&quot; &quot;2013-06-04&quot; &quot;2013-06-07&quot; &quot;2013-06-10&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; We now have a variety of data related to this ROI from the Hardwood Stand at Duke Forest. Green Chromatic Coordinate (GCC) is a measure of greenness of an area and is widely used in Phenocam images as an indicator of the green pigment in vegetation. Lets use this measure to look at changes in GCC over time at this site. Looking back at the available data, we have several options for GCC. gcc90 is the 90th quantile of GCC in the pixels across the ROI (for more details, PhenoCam v1 description). Well use this as it tracks the upper greenness values while not including many outliners. Before we can plot gcc-90 we do need to fix our dates and convert them from Factors to Date to correctly plot. # date variable into date format dukehw_DB_1000[,date:=as.Date(date)] # plot gcc_90 dukehw_DB_1000[,plot(date, gcc_90, col = &#39;green&#39;, type = &#39;b&#39;)] ## NULL mtext(&#39;Duke Forest, Hardwood&#39;, font = 2) Now, based on either direct API access or via the phenocamapi package, generate a dataframe of phenocam sites. Select two phenocam sites from different plant functional types to explore (e.g. one grassland site and one evergreen needleleaf site) #example GrassSites=cams_%&gt;% filter(cams_$primary_veg_type==&#39;GR&#39;) head(GrassSites) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 archboldbahia 27.16560 -81.21611 8 TRUE -5 2017-03-21 ## 2 arsgacp2 31.43950 -83.59146 101 FALSE 2 2016-04-27 ## 3 bitterbrush001 48.15400 -119.94560 667 TRUE -8 2020-09-20 ## 4 blueoakheadquarters 37.38270 -121.73930 572 TRUE -8 2022-01-24 ## 5 bozeman 45.78306 -110.77778 2332 FALSE -7 2015-08-16 ## 6 butte 45.95304 -112.47964 1682 TRUE -7 2008-04-01 ## date_last infrared contact1 ## 1 2022-09-28 Y Amartya Saha &lt;asaha AT archbold-station DOT org&gt; ## 2 2018-01-23 Y David Bosch &lt;David DOT Bosch AT ars DOT usda DOT gov&gt; ## 3 2022-09-03 Y Brandon Sackmann &lt;bssackmann AT gsi-net DOT com&gt; ## 4 2022-09-28 Y Zachariah Tuhtill &lt;ztuthill AT berkeley DOT edu&gt; ## 5 2019-12-18 Y Paul Stoy &lt;paul DOT stoy AT gmail DOT com&gt; ## 6 2022-09-02 N James Gallagher &lt;jgallagher AT opendap DOT org&gt; ## contact2 ## 1 Elizabeth Boughton &lt;eboughton AT archbold-station DOT org&gt; ## 2 ## 3 Brandon Sackmann &lt;bssackmann AT gsienv DOT com&gt; ## 4 Zachary Harlow &lt;harlow AT berkeley DOT edu&gt; ## 5 ## 6 Martha Apple &lt;MApple AT mtech DOT edu&gt; ## site_description ## 1 Archbold Biological Station, Florida, USA ## 2 Southeast Watershed Research Laboratory EC2 Tifton, Georgia ## 3 Parcel 1 East at Midpoint (Shrub-Steppe - Post Wildfire ca. 2014), Methow, Washington ## 4 Residence Meadow, Blue Oak Ranch Reserve, Santa Clara County, CA ## 5 Bangtail Study Area, Montana State University, Montana ## 6 Continental Divide, Butte, Montana ## site_type group camera_description camera_orientation ## 1 I LTAR StarDot NetCam SC N ## 2 I LTAR N N ## 3 I N StarDot NetCam SC W ## 4 I N StarDot NetCam SC N ## 5 I AmericaView AMERIFLUX StarDot NetCam SC N ## 6 I PhenoCam StarDot NetCam SC E ## flux_data flux_networks ## 1 FALSE NULL ## 2 FALSE NULL ## 3 FALSE NULL ## 4 FALSE NULL ## 5 TRUE AMERIFLUX, http://ameriflux.lbl.gov, AmeriFlux Network ## 6 FALSE NULL ## flux_sitenames ## 1 N ## 2 N ## 3 ## 4 ## 5 US-MTB (forthcoming) ## 6 ## dominant_species ## 1 ## 2 ## 3 ## 4 Avena spp., Quercus lobata, Elymus trachycaulus ## 5 Festuca idahoensis ## 6 Agropyron cristatum, Poa pratensis, Phalaris arundinaceae, Carex. sp., Geum triflorum, Ericameria nauseousa, Centaurea macula, Achillea millefolium, Senecio sp., Lupinus sp., Penstemon sp., Linaria vulgaris, Cirsium arvense; Alnus incana, Salix sp., Populus tremuloides ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 1 GR N FALSE N N ## 2 GR N FALSE N N ## 3 GR SH FALSE 8.7 323 ## 4 GR DB FALSE 13.4 600 ## 5 GR EN FALSE 5 850 ## 6 GR SH FALSE N N ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 1 22.85 1302 22.5 1208 Cfa 8 ## 2 19 1251 18.7 1213 Cfa 8 ## 3 8.35 420 7.3 340 Dsb 10 ## 4 14.35 501 14.3 646 Csb 11 ## 5 2.3 981 0.9 728 Dfb 6 ## 6 5.05 365 4.3 311 BSk 6 ## landcover_igbp ## 1 14 ## 2 14 ## 3 1 ## 4 8 ## 5 10 ## 6 10 ## site_acknowledgements ## 1 ## 2 ## 3 Research at this site is supported by GSI Environmental Inc. (http://www.gsi-net.com), Olympia, WA. ## 4 Blue Oak Ranch Reserve is part of the University of California Natural Reserve System. Blue Oak Ranch is administered bu the University of California, Berkeley ## 5 Research at the Bozeman site is supported by Colorado State University and the AmericaView program (grants G13AC00393, G11AC20461, G15AC00056) with phenocam equipment and deployment sponsored by the Department of Interior North Central Climate Science Center. ## 6 Research at the Continental Divide PhenoCam Site in Butte, Montana is supported by the National Science Foundation-EPSCoR (grant NSF-0701906), OpenDap, Inc., and Montana Tech of the University of Montana ## modified ## 1 2019-01-07T18:36:07.631244-05:00 ## 2 2018-06-18T20:27:17.280524-04:00 ## 3 2020-10-30T10:48:53.990761-04:00 ## 4 2021-04-12T09:54:24.431967-04:00 ## 5 2016-11-01T15:42:19.771057-04:00 ## 6 2016-11-01T15:42:19.846100-04:00 FirstSite=GrassSites[5, ] #randomly chose the fifth site in the table FirstSite ## Sitename Lat Lon Elev active utc_offset date_first date_last ## 5 bozeman 45.78306 -110.7778 2332 FALSE -7 2015-08-16 2019-12-18 ## infrared contact1 contact2 ## 5 Y Paul Stoy &lt;paul DOT stoy AT gmail DOT com&gt; ## site_description site_type ## 5 Bangtail Study Area, Montana State University, Montana I ## group camera_description camera_orientation flux_data ## 5 AmericaView AMERIFLUX StarDot NetCam SC N TRUE ## flux_networks flux_sitenames ## 5 AMERIFLUX, http://ameriflux.lbl.gov, AmeriFlux Network US-MTB (forthcoming) ## dominant_species primary_veg_type secondary_veg_type site_meteorology ## 5 Festuca idahoensis GR EN FALSE ## MAT_site MAP_site MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim ## 5 5 850 2.3 981 0.9 728 ## koeppen_geiger ecoregion landcover_igbp ## 5 Dfb 6 10 ## site_acknowledgements ## 5 Research at the Bozeman site is supported by Colorado State University and the AmericaView program (grants G13AC00393, G11AC20461, G15AC00056) with phenocam equipment and deployment sponsored by the Department of Interior North Central Climate Science Center. ## modified ## 5 2016-11-01T15:42:19.771057-04:00 Chose your own sites and build out your code chunk here: print(&#39;build your code here&#39;) ## [1] &quot;build your code here&quot; Koen Huffkens developed the phenocamr package, which streamlines access to quality controlled data. We will now use this package to download and process site based data according to a standardized methodology. A full description of the methodology is provided in Scientific Data: Tracking vegetation phenology across diverse North American biomes using PhenoCam imagery (Richardson et al. 2018). #uncomment if you need to install via devtools #if(!require(devtools)){install.package(devtools)} #devtools::install_github(&quot;khufkens/phenocamr&quot;) library(phenocamr) Use the dataframe of sites that you want to analyze to feed the phenocamr package. Note: you can choose either a 1 or 3 day product dir.create(&#39;data/&#39;, showWarnings = F) phenocamr::download_phenocam( frequency = 3, veg_type = &#39;DB&#39;, roi_id = 1000, site = &#39;harvard&#39;, phenophase = TRUE, out_dir = &quot;data/&quot; ) ## Downloading: harvard_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardbarn_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardbarn2_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardems2_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardfarmsouth_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardhemlock_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardlph_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! #&gt; Downloading: harvard_DB_1000_3day.csv #&gt; -- Flagging outliers! #&gt; -- Smoothing time series! #&gt; -- Estimating transition dates! Now look in your working directory. You have data! Read it in: # load the time series data but replace the csv filename with whatever you downloaded df &lt;- read.table(&quot;data/harvard_DB_1000_3day.csv&quot;, header = TRUE, sep = &quot;,&quot;) # read in the transition date file td &lt;- read.table(&quot;data/harvard_DB_1000_3day_transition_dates.csv&quot;, header = TRUE, sep = &quot;,&quot;) Lets take a look at the data: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;Smoothed GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data=df, x ~ as.Date(date), y = ~gcc_90, name = &#39;GCC&#39;, type = &#39;scatter&#39;, color =&#39;#07A4B5&#39;, opacity=.5 ) p ## Warning: Ignoring 3716 observations ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels What patterns do you notice? How would we go about determining say the start of spring? (SOS) 4.12.4 Threshold values Lets subset the transition date (td) for each year when 25% of the greenness amplitude (of the 90^th) percentile is reached (threshold_25). # select the rising (spring dates) for 25% threshold of Gcc 90 spring &lt;- td[td$direction == &quot;rising&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] Lets create a simple plot_ly line graph of the smooth Green Chromatic Coordinate (Gcc) and add points for transition dates: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;PhenoCam GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data= spring, x = ~ as.Date(spring$transition_25, origin = &quot;1970-01-01&quot;), y = ~ spring$threshold_25, type = &#39;scatter&#39;, mode = &#39;marker&#39;, name = &#39;Spring Dates&#39;) p Now we can see the transition date for each year of interest and the annual patterns of Gcc. However, if you want more control over the parameters used during processing, you can run through the three default processing steps as implemented in download_phenocam() and set parameters manually. Of particular interest is the option to specify your own threshold used in determining transition dates. What would be a reasonable threshold for peak greenness? Or autumn onset? Look at the ts dataset and phenocamr package and come up with a threshold. Use the same code to plot it here: # #some hint code # #what does &#39;rising&#39; versus &#39;falling&#39; denote? # #what threshold should you choose? # #td &lt;- phenophases(&quot;butte_GR_1000_3day.csv&quot;, # # internal = TRUE, # # upper_thresh = 0.8) fall &lt;- td[td$direction == &quot;falling&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] #Now generate a fall dataframe, what metrics should you use? 4.12.5 Comparing phenology across vegetation types Lets load in a function to make plotting smoother. Ive dropped it here in the markdown so that you can edit it and re-run it as you see fit: gcc_plot = function(gcc, spring, fall){ unix = &quot;1970-01-01&quot; p = plot_ly( data = gcc, x = ~ date, y = ~ gcc_90, showlegend = FALSE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_trace( y = ~ smooth_gcc_90, mode = &quot;lines&quot;, line = list(width = 2, color = &quot;rgb(120,120,120)&quot;), name = &quot;Gcc loess fit&quot;, showlegend = TRUE ) %&gt;% # SOS spring # 10% add_trace( data = spring, x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#7FFF00&quot;, symbol = &quot;circle&quot;), name = &quot;SOS (10%)&quot;, showlegend = TRUE ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), # y = ~ 0, # yend = ~ 1, y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#7FFF00&quot;), name = &quot;SOS (10%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#66CD00&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;SOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#66CD00&quot;), name = &quot;SOS (25%) - CI&quot; ) %&gt;% # 50 % add_trace( x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#458B00&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;SOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#458B00&quot;), name = &quot;SOS (50%) - CI&quot; ) %&gt;% # EOS fall # 50% add_trace( data = fall, x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#FFB90F&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;EOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#FFB90F&quot;), name = &quot;EOS (50%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#CD950C&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;EOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#CD950C&quot;), name = &quot;EOS (25%) - CI&quot; ) %&gt;% # 10 % add_trace( x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, marker = list(color = &quot;#8B6508&quot;, symbol = &quot;circle&quot;), showlegend = TRUE, name = &quot;EOS (10%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#8B6508&quot;), name = &quot;EOS (10%) - CI&quot; ) return (p) } gcc_p = gcc_plot(df, spring, fall) gcc_p &lt;- gcc_p %&gt;% layout( legend = list(x = 0.9, y = 0.9), xaxis = list( type = &#39;date&#39;, tickformat = &quot; %B&lt;br&gt;%Y&quot;, title=&#39;Year&#39;), yaxis = list( title = &#39;PhenoCam GCC&#39; )) gcc_p ## Warning: Ignoring 3716 observations ## Warning: Can&#39;t display both discrete &amp; non-discrete data on same axis What is the difference in 25% greenness onset for your first site? #hint, look at the spring dataframe you just generated #some hints to get you started # d=spring$transition_25 # d=as.Date(d) # d #more code hints # dates_split &lt;- data.frame(date = d, # year = as.numeric(format(d, format = &quot;%Y&quot;)), # month = as.numeric(format(d, format = &quot;%m&quot;)), # day = as.numeric(format(d, format = &quot;%d&quot;))) Generate a plot of smoothed gcc and transition dates for your two sites and subplot them. What do you notice? #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p 4.12.6 In Class Hands-on Coding: Comparing phenology of the same plant function type (PFT) across climate space As Dr. Richardson mentioned in his introduction lecture, the same plant functional types (e.g. grasslands) can have very different phenologogical cycles. Lets pick two phenocam grassland sites: one from a tropical climate (kamuela), and one from an arid climate (konza): GrassSites=GrassSites[&#39;filter for your sites&#39;] Now pull data for those sites via phenocamr or the phenocamapi print(&#39;code here&#39;) ## [1] &quot;code here&quot; Now lets create a subplot of your grasslands to compare phenology, some hint code below: #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p Once you have a subplot of grassland phenology across 2 climates answer the following questions in your markdown: 1. What seasonal patterns do you see? 2. Do you think you set your thresholds correctly for transition dates/phenophases? How might that very as a function of climate? 3. What are the challenges of forecasting or modeling tropical versus arid grasslands? 4.13 Digital Repeat Photography Coding Lab 4.13.1 Quantifying haze and redness to evaluate California wildfires Pull mid-day imagery for September 1-7th, 2019 and 2020 for the canopy-level camera NEON.D17.SOAP.DP1.00033. Create a 2-panel plot showing those images in 2019 (left) and 2020 (right). Use the hazeR package to quantify the haze in each of those images. Print a summary of your results. Generate a density function RGB plot for your haziest image in 2020, and one for the same date in 2019. Create a 2-panel plot showing 2019 on the left and 2020 on the right. Pull timesseries data via the phenocamapi package. Calculate the difference in the rcc90 between 2019 and 2020 over the same time period as your images. Create a summary plot showing haze as a bar and the differenece in rcc90 from question 4 as a timersies. Answer the following questions: Does the hazeR package pick up smokey images? If you were to use color coordinates, which color band would be most useful to highlight smoke and why? Optional Bonus Points: Repeat the above calculations for the understory camera on the same tower NEON.D17.SOAP.DP1.00042 and produce the same plots. Which camera is better and capturing wildfire? Why do you think that is so? 4.14 PhenoCam Culmination Activity Write up a 1-page summary of a project that you might want to explore using PhenoCam data over the duration of this course. Include the types of PhenoCam (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. "],["flux-measurements-inter-operability.html", "Chapter 5 Flux Measurements &amp; Inter-Operability 5.1 Learning Objectives 5.2 Eddy Co_variance Data: What does it actually measure? 5.3 QA/QC Flags 5.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET 5.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond 5.6 Hands On: Introduction to working with NEON eddy flux data 5.7 Exercises", " Chapter 5 Flux Measurements &amp; Inter-Operability Estimated Time: 3 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 5.1 Learning Objectives At the end of this activity, you will be able to: Define the eddy covariance method Articulate various carbon storage and flux terms Understand the structure of bundled eddy covariance data Be able to process NEON flux data such that it is comparable with AmeriFlux 5.2 Eddy Co_variance Data: What does it actually measure? 5.2.1 Example Eddy Site 5.3 QA/QC Flags 5.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET AmeriFlux is a network of PI-managed sites measuring ecosystem CO2, water, and energy fluxes in North, Central and South America. It was established to connect research on field sites representing major climate and ecological biomes, including tundra, grasslands, savanna, crops, and conifer, deciduous, and tropical forests. As a grassroots, investigator-driven network, the AmeriFlux community has tailored instrumentation to suit each unique ecosystem. This coalition of the willing is diverse in its interests, use of technologies and collaborative approaches. As a result, the AmeriFlux Network continually pioneers new ground. The network was launched in 1996, after an international workshop on flux measurements in La Thuile, Italy, in 1995, where some of the first year-long flux measurements were presented. Early support for the network came from many sources, including the U.S. Department of Energys Terrestrial Carbon Program, the DOEs National Institute of Global Environmental Change (NIGEC), NASA, NOAA and the US Forest Service. The network grew from about 15 sites in 1997 to more than 110 active sites registered today. Sixty-one other sites, now inactive, have flux data stored in the networks database. In 2012, the U.S. DOE established the AmeriFlux Management Project (AMP) at Lawrence Berkeley National Laboratory (LBNL) to support the broad AmeriFlux community and the AmeriFlux sites. View the AMERIFLUX Network-at-a-Glance AmeriFlux is now one of the DOE Office of Biological and Environmental Researchs (BER) best-known and most highly regarded brands in climate and ecological research. AmeriFlux datasets, and the understanding derived from them, provide crucial linkages between terrestrial ecosystem processes and climate-relevant responses at landscape, regional, and continental scales. 5.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond Given that AmeriFlux has been collecting and coordinating eddy covariance data across the Americas since 1996. The network provides a common platform for data sharing and collaboration for organizations and individual private investigators collecting flux tower data. There are now &gt;470 registered flux tower sites in North, Central, and South America in the AmeriFlux network, many operated by individual researchers or universities. The towers collect eddy covariance data across a broad range of climate zones and ecosystem types, from Chile to Alaska and everywhere in between. Now, data from the NEON project is available through the AmeriFlux data portal. The NEON team has formatted data from the NEON flux towers to make it fully compatible with AmeriFlux data. This allows researchers to view, download and analyze data from the NEON flux towers alongside data from all of the other flux towers in the AmeriFlux network. With 47 flux towers at terrestrial field sites across the U.S., the NEON program is now the largest single contributor of flux tower data to the AmeriFlux network. NEON field sites are located in 20 ecoclimatic zones across the U.S., representing many distinct ecosystems. Eddy covariance data will be served using the same methods at each site for the entire 30-year life of the Observatory, allowing for unprecedented comparability across both time and space. 5.6 Hands On: Introduction to working with NEON eddy flux data 5.6.1 Setup Start by installing and loading packages and setting options. To work with the NEON flux data, we need the rhdf5 package, which is hosted on Bioconductor, and requires a different installation process than CRAN packages: install.packages(&#39;BiocManager&#39;) BiocManager::install(&#39;rhdf5&#39;) options(stringsAsFactors=F) library(neonUtilities) Use the zipsByProduct() function from the neonUtilities package to download flux data from two sites and two months. The transformations and functions below will work on any time range and site(s), but two sites and two months allows us to see all the available functionality while minimizing download size. Inputs to the zipsByProduct() function: dpID: DP4.00200.001, the bundled eddy covariance product package: basic (the expanded package is not covered in this tutorial) site: NIWO = Niwot Ridge and HARV = Harvard Forest startdate: 2018-06 (both dates are inclusive) enddate: 2018-07 (both dates are inclusive) savepath: modify this to something logical on your machine check.size: T if you want to see file size before downloading, otherwise F The download may take a while, especially if youre on a slow network. zipsByProduct(dpID=&quot;DP4.00200.001&quot;, package=&quot;basic&quot;, site=c(&quot;NIWO&quot;, &quot;HARV&quot;), startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, savepath=&quot;./data&quot;, check.size=F) ## Finding available files ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## ## Downloading files totaling approximately 345.260177 MB ## ./data/filesToStack00200 already exists. Download will proceed, but check for duplicate files. ## Downloading 4 files ## | | | 0% | |======================= | 33% | |=============================================== | 67% | |======================================================================| 100% ## 4 files successfully downloaded to ./data/filesToStack00200 5.6.2 Data Levels There are five levels of data contained in the eddy flux bundle. For full details, refer to the NEON algorithm document. Briefly, the data levels are: Level 0 (dp0p): Calibrated raw observations Level 1 (dp01): Time-aggregated observations, e.g. 30-minute mean gas concentrations Level 2 (dp02): Time-interpolated data, e.g. rate of change of a gas concentration Level 3 (dp03): Spatially interpolated data, i.e. vertical profiles Level 4 (dp04): Fluxes The dp0p data are available in the expanded data package and are beyond the scope of this tutorial. The dp02 and dp03 data are used in storage calculations, and the dp04 data include both the storage and turbulent components. Since many users will want to focus on the net flux data, well start there. 5.6.3 Extract Level 4 data (Fluxes!) To extract the Level 4 data from the HDF5 files and merge them into a single table, well use the stackEddy() function from the neonUtilities package. stackEddy() requires two inputs: filepath: Path to a file or folder, which can be any one of: A zip file of eddy flux data downloaded from the NEON data portal A folder of eddy flux data downloaded by the zipsByProduct() function The folder of files resulting from unzipping either of 1 or 2 A single HDF5 file of NEON eddy flux data level: dp01-4 Input the filepath you downloaded to using zipsByProduct() earlier, including the filestoStack00200 folder created by the function, and dp04: flux &lt;- stackEddy(filepath=&quot;./data/filesToStack00200&quot;, level=&quot;dp04&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |== | 2% | |=== | 5% | |===== | 7% | |====== | 9% | |======== | 11% | |========== | 14% | |=========== | 16% | |============= | 18% | |============== | 20% | |================ | 23% | |================== | 25% | |=================== | 27% | |===================== | 30% | |====================== | 32% | |======================== | 34% | |========================= | 36% | |=========================== | 39% | |============================= | 41% | |============================== | 43% | |================================ | 45% | |================================= | 48% | |=================================== | 50% | |===================================== | 52% | |====================================== | 55% | |======================================== | 57% | |========================================= | 59% | |=========================================== | 61% | |============================================= | 64% | |============================================== | 66% | |================================================ | 68% | |================================================= | 70% | |=================================================== | 73% | |==================================================== | 75% | |====================================================== | 77% | |======================================================== | 80% | |========================================================= | 82% | |=========================================================== | 84% | |============================================================ | 86% | |============================================================== | 89% | |================================================================ | 91% | |================================================================= | 93% | |=================================================================== | 95% | |==================================================================== | 98% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |== | 2% | |=== | 5% | |===== | 7% | |====== | 9% | |======== | 11% | |========== | 14% | |=========== | 16% | |============= | 18% | |============== | 20% | |================ | 23% | |================== | 25% | |=================== | 27% | |===================== | 30% | |====================== | 32% | |======================== | 34% | |========================= | 36% | |=========================== | 39% | |============================= | 41% | |============================== | 43% | |================================ | 45% | |================================= | 48% | |=================================== | 50% | |===================================== | 52% | |====================================== | 55% | |======================================== | 57% | |========================================= | 59% | |=========================================== | 61% | |============================================= | 64% | |============================================== | 66% | |================================================ | 68% | |================================================= | 70% | |=================================================== | 73% | |==================================================== | 75% | |====================================================== | 77% | |======================================================== | 80% | |========================================================= | 82% | |=========================================================== | 84% | |============================================================ | 86% | |============================================================== | 89% | |================================================================ | 91% | |================================================================= | 93% | |=================================================================== | 95% | |==================================================================== | 98% | |======================================================================| 100% We now have an object called flux. Its a named list containing four tables: one table for each sites data, and variables and objDesc tables. names(flux) ## [1] &quot;HARV&quot; &quot;NIWO&quot; &quot;variables&quot; &quot;objDesc&quot; &quot;issueLog&quot; Lets look at the contents of one of the site data files: knitr::kable(head(flux$NIWO)) timeBgn timeEnd data.fluxCo2.nsae.flux data.fluxCo2.stor.flux data.fluxCo2.turb.flux data.fluxH2o.nsae.flux data.fluxH2o.stor.flux data.fluxH2o.turb.flux data.fluxMome.turb.veloFric data.fluxTemp.nsae.flux data.fluxTemp.stor.flux data.fluxTemp.turb.flux data.foot.stat.angZaxsErth data.foot.stat.distReso data.foot.stat.veloYaxsHorSd data.foot.stat.veloZaxsHorSd data.foot.stat.veloFric data.foot.stat.distZaxsMeasDisp data.foot.stat.distZaxsRgh data.foot.stat.distZaxsAbl data.foot.stat.distXaxs90 data.foot.stat.distXaxsMax data.foot.stat.distYaxs90 qfqm.fluxCo2.nsae.qfFinl qfqm.fluxCo2.stor.qfFinl qfqm.fluxCo2.turb.qfFinl qfqm.fluxH2o.nsae.qfFinl qfqm.fluxH2o.stor.qfFinl qfqm.fluxH2o.turb.qfFinl qfqm.fluxMome.turb.qfFinl qfqm.fluxTemp.nsae.qfFinl qfqm.fluxTemp.stor.qfFinl qfqm.fluxTemp.turb.qfFinl qfqm.foot.turb.qfFinl 2018-06-01 00:00:00 2018-06-01 00:29:59 0.1713858 -0.0634816 0.2348674 15.876622 3.3334970 12.543125 0.2047081 4.7565505 -1.4575094 6.2140599 94.2262 8.34 0.7955893 0.2713232 0.2025427 8.34 0.0410571 1000 325.26 133.44 25.02 1 1 1 1 1 1 0 0 0 0 0 2018-06-01 00:30:00 2018-06-01 00:59:59 0.9251711 0.0874815 0.8376896 8.089274 -1.2063258 9.295600 0.1923735 -0.2717454 0.3403877 -0.6121331 355.4252 8.34 0.8590177 0.2300000 0.2000000 8.34 0.2799194 1000 266.88 108.42 50.04 1 1 0 1 0 1 0 1 0 1 0 2018-06-01 01:00:00 2018-06-01 01:29:59 0.5005812 0.0223170 0.4782642 5.290594 -4.4190781 9.709672 0.1200918 -4.2055147 0.1870677 -4.3925824 359.8013 8.34 1.2601763 0.2300000 0.2000000 8.34 0.2129323 1000 275.22 116.76 66.72 1 1 0 0 0 0 1 0 0 0 0 2018-06-01 01:30:00 2018-06-01 01:59:59 0.8032820 0.2556931 0.5475889 9.190214 0.2030371 8.987177 0.1177545 -13.3834484 -2.4904300 -10.8930185 137.7743 8.34 0.7332641 0.2300000 0.2000000 8.34 0.8340000 1000 208.50 83.40 75.06 1 1 0 0 0 0 1 0 0 0 0 2018-06-01 02:00:00 2018-06-01 02:29:59 0.4897685 0.2309047 0.2588638 3.111909 0.1349363 2.976973 0.1589189 -5.1854815 -0.7514531 -4.4340284 188.4799 8.34 0.7096286 0.2300000 0.2000000 8.34 0.8340000 1000 208.50 83.40 66.72 1 1 0 0 0 0 0 0 0 0 0 2018-06-01 02:30:00 2018-06-01 02:59:59 0.9223979 0.0622858 0.8601121 4.613676 -0.3929445 5.006621 0.1114406 -7.7365481 -1.9046775 -5.8318707 183.1920 8.34 0.3789859 0.2300000 0.2000000 8.34 0.8340000 1000 208.50 83.40 41.70 1 1 0 1 1 0 0 0 0 0 0 The variables and objDesc tables can help you interpret the column headers in the data table. The objDesc table contains definitions for many of the terms used in the eddy flux data product, but it isnt complete. To get the terms of interest, well break up the column headers into individual terms and look for them in the objDesc table: term &lt;- unlist(strsplit(names(flux$NIWO), split=&quot;.&quot;, fixed=T)) flux$objDesc[which(flux$objDesc$Object %in% term),] ## Object ## 138 angZaxsErth ## 171 data ## 343 qfFinl ## 420 qfqm ## 604 timeBgn ## 605 timeEnd ## Description ## 138 Wind direction ## 171 Represents data fields ## 343 The final quality flag indicating if the data are valid for the given aggregation period (1=fail, 0=pass) ## 420 Quality flag and quality metrics, represents quality flags and quality metrics that accompany the provided data ## 604 The beginning time of the aggregation period ## 605 The end time of the aggregation period knitr::kable(term) x timeBgn timeEnd data fluxCo2 nsae flux data fluxCo2 stor flux data fluxCo2 turb flux data fluxH2o nsae flux data fluxH2o stor flux data fluxH2o turb flux data fluxMome turb veloFric data fluxTemp nsae flux data fluxTemp stor flux data fluxTemp turb flux data foot stat angZaxsErth data foot stat distReso data foot stat veloYaxsHorSd data foot stat veloZaxsHorSd data foot stat veloFric data foot stat distZaxsMeasDisp data foot stat distZaxsRgh data foot stat distZaxsAbl data foot stat distXaxs90 data foot stat distXaxsMax data foot stat distYaxs90 qfqm fluxCo2 nsae qfFinl qfqm fluxCo2 stor qfFinl qfqm fluxCo2 turb qfFinl qfqm fluxH2o nsae qfFinl qfqm fluxH2o stor qfFinl qfqm fluxH2o turb qfFinl qfqm fluxMome turb qfFinl qfqm fluxTemp nsae qfFinl qfqm fluxTemp stor qfFinl qfqm fluxTemp turb qfFinl qfqm foot turb qfFinl For the terms that arent captured here, fluxCo2, fluxH2o, and fluxTemp are self-explanatory. The flux components are turb: Turbulent flux stor: Storage nsae: Net surface-atmosphere exchange The variables table contains the units for each field: knitr::kable(flux$variables) category system variable stat units data fluxCo2 nsae timeBgn NA data fluxCo2 nsae timeEnd NA data fluxCo2 nsae flux umolCo2 m-2 s-1 data fluxCo2 stor timeBgn NA data fluxCo2 stor timeEnd NA data fluxCo2 stor flux umolCo2 m-2 s-1 data fluxCo2 turb timeBgn NA data fluxCo2 turb timeEnd NA data fluxCo2 turb flux umolCo2 m-2 s-1 data fluxH2o nsae timeBgn NA data fluxH2o nsae timeEnd NA data fluxH2o nsae flux W m-2 data fluxH2o stor timeBgn NA data fluxH2o stor timeEnd NA data fluxH2o stor flux W m-2 data fluxH2o turb timeBgn NA data fluxH2o turb timeEnd NA data fluxH2o turb flux W m-2 data fluxMome turb timeBgn NA data fluxMome turb timeEnd NA data fluxMome turb veloFric m s-1 data fluxTemp nsae timeBgn NA data fluxTemp nsae timeEnd NA data fluxTemp nsae flux W m-2 data fluxTemp stor timeBgn NA data fluxTemp stor timeEnd NA data fluxTemp stor flux W m-2 data fluxTemp turb timeBgn NA data fluxTemp turb timeEnd NA data fluxTemp turb flux W m-2 data foot stat timeBgn NA data foot stat timeEnd NA data foot stat angZaxsErth deg data foot stat distReso m data foot stat veloYaxsHorSd m s-1 data foot stat veloZaxsHorSd m s-1 data foot stat veloFric m s-1 data foot stat distZaxsMeasDisp m data foot stat distZaxsRgh m data foot stat distZaxsAbl m data foot stat distXaxs90 m data foot stat distXaxsMax m data foot stat distYaxs90 m qfqm fluxCo2 nsae timeBgn NA qfqm fluxCo2 nsae timeEnd NA qfqm fluxCo2 nsae qfFinl NA qfqm fluxCo2 stor qfFinl NA qfqm fluxCo2 stor timeBgn NA qfqm fluxCo2 stor timeEnd NA qfqm fluxCo2 turb timeBgn NA qfqm fluxCo2 turb timeEnd NA qfqm fluxCo2 turb qfFinl NA qfqm fluxH2o nsae timeBgn NA qfqm fluxH2o nsae timeEnd NA qfqm fluxH2o nsae qfFinl NA qfqm fluxH2o stor qfFinl NA qfqm fluxH2o stor timeBgn NA qfqm fluxH2o stor timeEnd NA qfqm fluxH2o turb timeBgn NA qfqm fluxH2o turb timeEnd NA qfqm fluxH2o turb qfFinl NA qfqm fluxMome turb timeBgn NA qfqm fluxMome turb timeEnd NA qfqm fluxMome turb qfFinl NA qfqm fluxTemp nsae timeBgn NA qfqm fluxTemp nsae timeEnd NA qfqm fluxTemp nsae qfFinl NA qfqm fluxTemp stor qfFinl NA qfqm fluxTemp stor timeBgn NA qfqm fluxTemp stor timeEnd NA qfqm fluxTemp turb timeBgn NA qfqm fluxTemp turb timeEnd NA qfqm fluxTemp turb qfFinl NA qfqm foot turb timeBgn NA qfqm foot turb timeEnd NA qfqm foot turb qfFinl NA Lets plot some data! First, well need to convert the time stamps to an R date-time format (right now theyre just character fields). 5.6.4 Time stamps NEON sensor data come with time stamps for both the start and end of the averaging period. Depending on the analysis youre doing, you may want to use one or the other; for general plotting, re-formatting, and transformations, I prefer to use the start time, because there are some small inconsistencies between data products in a few of the end time stamps. Note that all NEON data use UTC time, noted as tz=\"GMT\" in the code below. This is true across NEONs instrumented, observational, and airborne measurements. When working with NEON data, its best to keep everything in UTC as much as possible, otherwise its very easy to end up with data in mismatched times, which can cause insidious and hard-to-detect problems. Be sure to include the tz argument in all the lines of code below - if there is no time zone specified, R will default to the local time zone it detects on your operating system. timeB &lt;- as.POSIXct(flux$NIWO$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%S&quot;, tz=&quot;GMT&quot;) flux$NIWO &lt;- cbind(timeB, flux$NIWO) plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d&quot;) Like a lot of flux data, these data have some stray spikes, but there is a clear diurnal pattern going into the growing season. Lets trim down to just two days of data to see a few other details. plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=20, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xlim=c(as.POSIXct(&quot;2018-07-07&quot;, tz=&quot;GMT&quot;), as.POSIXct(&quot;2018-07-09&quot;, tz=&quot;GMT&quot;)), ylim=c(-20,20), xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d %H:%M:%S&quot;) Note the timing of C uptake; the UTC time zone is clear here, where uptake occurs at times that appear to be during the night. 5.6.5 Merge flux data with other sensor data Many of the data sets we would use to interpret and model flux data are measured as part of the NEON project, but are not present in the eddy flux data product bundle. In this section, well download PAR data and merge them with the flux data; the steps taken here can be applied to any of the NEON instrumented (IS) data products. 5.6.5.1 Download PAR data To get NEON PAR data, use the loadByProduct() function from the neonUtilities package. loadByProduct() takes the same inputs as zipsByProduct(), but it loads the downloaded data directly into the current R environment. Lets download PAR data matching the Niwot Ridge flux data. The inputs needed are: dpID: DP1.00024.001 site: NIWO startdate: 2018-06 enddate: 2018-07 package: basic avg: 30 The new input here is avg=30, which downloads only the 30-minute data. Since the flux data are at a 30-minute resolution, we can save on download time by disregarding the 1-minute data files (which are of course 30 times larger). The avg input can be left off if you want to download all available averaging intervals. pr &lt;- loadByProduct(&quot;DP1.00024.001&quot;, site=&quot;NIWO&quot;, avg=30, startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, package=&quot;basic&quot;, check.size=F) ## Input parameter avg is deprecated; use timeIndex to download by time interval. ## Finding available files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Downloading files totaling approximately 1.304794 MB ## Downloading 11 files ## | | | 0% | |======= | 10% | |============== | 20% | |===================== | 30% | |============================ | 40% | |=================================== | 50% | |========================================== | 60% | |================================================= | 70% | |======================================================== | 80% | |=============================================================== | 90% | |======================================================================| 100% ## ## Stacking operation across a single core. ## Stacking table PARPAR_30min ## Merged the most recent publication of sensor position files for each site and saved to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 1 data tables and 3 metadata tables! ## Stacking took 0.3359709 secs pr is another named list, and again, metadata and units can be found in the variables table. The PARPAR_30min table contains a verticalPosition field. This field indicates the position on the tower, with 10 being the first tower level, and 20, 30, etc going up the tower. 5.6.5.2 Join PAR to flux data Well connect PAR data from the tower top to the flux data. pr.top &lt;- pr$PARPAR_30min[which(pr$PARPAR_30min$verticalPosition== max(pr$PARPAR_30min$verticalPosition)),] loadByProduct() automatically converts time stamps when it reads the data, so here we just need to indicate which time field to use to merge the flux and PAR data. timeB &lt;- pr.top$startDateTime pr.top &lt;- cbind(timeB, pr.top) And merge the two datasets: fx.pr &lt;- merge(pr.top, flux$NIWO, by=&quot;timeB&quot;) plot(fx.pr$data.fluxCo2.nsae.flux~fx.pr$PARMean, pch=&quot;.&quot;, ylim=c(-20,20), xlab=&quot;PAR&quot;, ylab=&quot;CO2 flux&quot;) If youre interested in data in the eddy covariance bundle besides the net flux data, the rest of this tutorial will guide you through how to get those data out of the bundle. 5.6.6 Vertical profile data (Level 3) The Level 3 (dp03) data are the spatially interpolated profiles of the rates of change of CO2, H2O, and temperature. Extract the Level 3 data from the HDF5 file using stackEddy() with the same syntax as for the Level 4 data. prof &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp03&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% knitr::kable(head(prof$NIWO)) timeBgn timeEnd data.co2Stor.rateRtioMoleDryCo2.X0.1.m data.co2Stor.rateRtioMoleDryCo2.X0.2.m data.co2Stor.rateRtioMoleDryCo2.X0.3.m data.co2Stor.rateRtioMoleDryCo2.X0.4.m data.co2Stor.rateRtioMoleDryCo2.X0.5.m data.co2Stor.rateRtioMoleDryCo2.X0.6.m data.co2Stor.rateRtioMoleDryCo2.X0.7.m data.co2Stor.rateRtioMoleDryCo2.X0.8.m data.co2Stor.rateRtioMoleDryCo2.X0.9.m data.co2Stor.rateRtioMoleDryCo2.X1.m data.co2Stor.rateRtioMoleDryCo2.X1.1.m data.co2Stor.rateRtioMoleDryCo2.X1.2.m data.co2Stor.rateRtioMoleDryCo2.X1.3.m data.co2Stor.rateRtioMoleDryCo2.X1.4.m data.co2Stor.rateRtioMoleDryCo2.X1.5.m data.co2Stor.rateRtioMoleDryCo2.X1.6.m data.co2Stor.rateRtioMoleDryCo2.X1.7.m data.co2Stor.rateRtioMoleDryCo2.X1.8.m data.co2Stor.rateRtioMoleDryCo2.X1.9.m data.co2Stor.rateRtioMoleDryCo2.X2.m data.co2Stor.rateRtioMoleDryCo2.X2.1.m data.co2Stor.rateRtioMoleDryCo2.X2.2.m data.co2Stor.rateRtioMoleDryCo2.X2.3.m data.co2Stor.rateRtioMoleDryCo2.X2.4.m data.co2Stor.rateRtioMoleDryCo2.X2.5.m data.co2Stor.rateRtioMoleDryCo2.X2.6.m data.co2Stor.rateRtioMoleDryCo2.X2.7.m data.co2Stor.rateRtioMoleDryCo2.X2.8.m data.co2Stor.rateRtioMoleDryCo2.X2.9.m data.co2Stor.rateRtioMoleDryCo2.X3.m data.co2Stor.rateRtioMoleDryCo2.X3.1.m data.co2Stor.rateRtioMoleDryCo2.X3.2.m data.co2Stor.rateRtioMoleDryCo2.X3.3.m data.co2Stor.rateRtioMoleDryCo2.X3.4.m data.co2Stor.rateRtioMoleDryCo2.X3.5.m data.co2Stor.rateRtioMoleDryCo2.X3.6.m data.co2Stor.rateRtioMoleDryCo2.X3.7.m data.co2Stor.rateRtioMoleDryCo2.X3.8.m data.co2Stor.rateRtioMoleDryCo2.X3.9.m data.co2Stor.rateRtioMoleDryCo2.X4.m data.co2Stor.rateRtioMoleDryCo2.X4.1.m data.co2Stor.rateRtioMoleDryCo2.X4.2.m data.co2Stor.rateRtioMoleDryCo2.X4.3.m data.co2Stor.rateRtioMoleDryCo2.X4.4.m data.co2Stor.rateRtioMoleDryCo2.X4.5.m data.co2Stor.rateRtioMoleDryCo2.X4.6.m data.co2Stor.rateRtioMoleDryCo2.X4.7.m data.co2Stor.rateRtioMoleDryCo2.X4.8.m data.co2Stor.rateRtioMoleDryCo2.X4.9.m data.co2Stor.rateRtioMoleDryCo2.X5.m data.co2Stor.rateRtioMoleDryCo2.X5.1.m data.co2Stor.rateRtioMoleDryCo2.X5.2.m data.co2Stor.rateRtioMoleDryCo2.X5.3.m data.co2Stor.rateRtioMoleDryCo2.X5.4.m data.co2Stor.rateRtioMoleDryCo2.X5.5.m data.co2Stor.rateRtioMoleDryCo2.X5.6.m data.co2Stor.rateRtioMoleDryCo2.X5.7.m data.co2Stor.rateRtioMoleDryCo2.X5.8.m data.co2Stor.rateRtioMoleDryCo2.X5.9.m data.co2Stor.rateRtioMoleDryCo2.X6.m data.co2Stor.rateRtioMoleDryCo2.X6.1.m data.co2Stor.rateRtioMoleDryCo2.X6.2.m data.co2Stor.rateRtioMoleDryCo2.X6.3.m data.co2Stor.rateRtioMoleDryCo2.X6.4.m data.co2Stor.rateRtioMoleDryCo2.X6.5.m data.co2Stor.rateRtioMoleDryCo2.X6.6.m data.co2Stor.rateRtioMoleDryCo2.X6.7.m data.co2Stor.rateRtioMoleDryCo2.X6.8.m data.co2Stor.rateRtioMoleDryCo2.X6.9.m data.co2Stor.rateRtioMoleDryCo2.X7.m data.co2Stor.rateRtioMoleDryCo2.X7.1.m data.co2Stor.rateRtioMoleDryCo2.X7.2.m data.co2Stor.rateRtioMoleDryCo2.X7.3.m data.co2Stor.rateRtioMoleDryCo2.X7.4.m data.co2Stor.rateRtioMoleDryCo2.X7.5.m data.co2Stor.rateRtioMoleDryCo2.X7.6.m data.co2Stor.rateRtioMoleDryCo2.X7.7.m data.co2Stor.rateRtioMoleDryCo2.X7.8.m data.co2Stor.rateRtioMoleDryCo2.X7.9.m data.co2Stor.rateRtioMoleDryCo2.X8.m data.co2Stor.rateRtioMoleDryCo2.X8.1.m data.co2Stor.rateRtioMoleDryCo2.X8.2.m data.co2Stor.rateRtioMoleDryCo2.X8.3.m data.co2Stor.rateRtioMoleDryCo2.X8.4.m data.h2oStor.rateRtioMoleDryH2o.X0.1.m data.h2oStor.rateRtioMoleDryH2o.X0.2.m data.h2oStor.rateRtioMoleDryH2o.X0.3.m data.h2oStor.rateRtioMoleDryH2o.X0.4.m data.h2oStor.rateRtioMoleDryH2o.X0.5.m data.h2oStor.rateRtioMoleDryH2o.X0.6.m data.h2oStor.rateRtioMoleDryH2o.X0.7.m data.h2oStor.rateRtioMoleDryH2o.X0.8.m data.h2oStor.rateRtioMoleDryH2o.X0.9.m data.h2oStor.rateRtioMoleDryH2o.X1.m data.h2oStor.rateRtioMoleDryH2o.X1.1.m data.h2oStor.rateRtioMoleDryH2o.X1.2.m data.h2oStor.rateRtioMoleDryH2o.X1.3.m data.h2oStor.rateRtioMoleDryH2o.X1.4.m data.h2oStor.rateRtioMoleDryH2o.X1.5.m data.h2oStor.rateRtioMoleDryH2o.X1.6.m data.h2oStor.rateRtioMoleDryH2o.X1.7.m data.h2oStor.rateRtioMoleDryH2o.X1.8.m data.h2oStor.rateRtioMoleDryH2o.X1.9.m data.h2oStor.rateRtioMoleDryH2o.X2.m data.h2oStor.rateRtioMoleDryH2o.X2.1.m data.h2oStor.rateRtioMoleDryH2o.X2.2.m data.h2oStor.rateRtioMoleDryH2o.X2.3.m data.h2oStor.rateRtioMoleDryH2o.X2.4.m data.h2oStor.rateRtioMoleDryH2o.X2.5.m data.h2oStor.rateRtioMoleDryH2o.X2.6.m data.h2oStor.rateRtioMoleDryH2o.X2.7.m data.h2oStor.rateRtioMoleDryH2o.X2.8.m data.h2oStor.rateRtioMoleDryH2o.X2.9.m data.h2oStor.rateRtioMoleDryH2o.X3.m data.h2oStor.rateRtioMoleDryH2o.X3.1.m data.h2oStor.rateRtioMoleDryH2o.X3.2.m data.h2oStor.rateRtioMoleDryH2o.X3.3.m data.h2oStor.rateRtioMoleDryH2o.X3.4.m data.h2oStor.rateRtioMoleDryH2o.X3.5.m data.h2oStor.rateRtioMoleDryH2o.X3.6.m data.h2oStor.rateRtioMoleDryH2o.X3.7.m data.h2oStor.rateRtioMoleDryH2o.X3.8.m data.h2oStor.rateRtioMoleDryH2o.X3.9.m data.h2oStor.rateRtioMoleDryH2o.X4.m data.h2oStor.rateRtioMoleDryH2o.X4.1.m data.h2oStor.rateRtioMoleDryH2o.X4.2.m data.h2oStor.rateRtioMoleDryH2o.X4.3.m data.h2oStor.rateRtioMoleDryH2o.X4.4.m data.h2oStor.rateRtioMoleDryH2o.X4.5.m data.h2oStor.rateRtioMoleDryH2o.X4.6.m data.h2oStor.rateRtioMoleDryH2o.X4.7.m data.h2oStor.rateRtioMoleDryH2o.X4.8.m data.h2oStor.rateRtioMoleDryH2o.X4.9.m data.h2oStor.rateRtioMoleDryH2o.X5.m data.h2oStor.rateRtioMoleDryH2o.X5.1.m data.h2oStor.rateRtioMoleDryH2o.X5.2.m data.h2oStor.rateRtioMoleDryH2o.X5.3.m data.h2oStor.rateRtioMoleDryH2o.X5.4.m data.h2oStor.rateRtioMoleDryH2o.X5.5.m data.h2oStor.rateRtioMoleDryH2o.X5.6.m data.h2oStor.rateRtioMoleDryH2o.X5.7.m data.h2oStor.rateRtioMoleDryH2o.X5.8.m data.h2oStor.rateRtioMoleDryH2o.X5.9.m data.h2oStor.rateRtioMoleDryH2o.X6.m data.h2oStor.rateRtioMoleDryH2o.X6.1.m data.h2oStor.rateRtioMoleDryH2o.X6.2.m data.h2oStor.rateRtioMoleDryH2o.X6.3.m data.h2oStor.rateRtioMoleDryH2o.X6.4.m data.h2oStor.rateRtioMoleDryH2o.X6.5.m data.h2oStor.rateRtioMoleDryH2o.X6.6.m data.h2oStor.rateRtioMoleDryH2o.X6.7.m data.h2oStor.rateRtioMoleDryH2o.X6.8.m data.h2oStor.rateRtioMoleDryH2o.X6.9.m data.h2oStor.rateRtioMoleDryH2o.X7.m data.h2oStor.rateRtioMoleDryH2o.X7.1.m data.h2oStor.rateRtioMoleDryH2o.X7.2.m data.h2oStor.rateRtioMoleDryH2o.X7.3.m data.h2oStor.rateRtioMoleDryH2o.X7.4.m data.h2oStor.rateRtioMoleDryH2o.X7.5.m data.h2oStor.rateRtioMoleDryH2o.X7.6.m data.h2oStor.rateRtioMoleDryH2o.X7.7.m data.h2oStor.rateRtioMoleDryH2o.X7.8.m data.h2oStor.rateRtioMoleDryH2o.X7.9.m data.h2oStor.rateRtioMoleDryH2o.X8.m data.h2oStor.rateRtioMoleDryH2o.X8.1.m data.h2oStor.rateRtioMoleDryH2o.X8.2.m data.h2oStor.rateRtioMoleDryH2o.X8.3.m data.h2oStor.rateRtioMoleDryH2o.X8.4.m data.tempStor.rateTemp.X0.1.m data.tempStor.rateTemp.X0.2.m data.tempStor.rateTemp.X0.3.m data.tempStor.rateTemp.X0.4.m data.tempStor.rateTemp.X0.5.m data.tempStor.rateTemp.X0.6.m data.tempStor.rateTemp.X0.7.m data.tempStor.rateTemp.X0.8.m data.tempStor.rateTemp.X0.9.m data.tempStor.rateTemp.X1.m data.tempStor.rateTemp.X1.1.m data.tempStor.rateTemp.X1.2.m data.tempStor.rateTemp.X1.3.m data.tempStor.rateTemp.X1.4.m data.tempStor.rateTemp.X1.5.m data.tempStor.rateTemp.X1.6.m data.tempStor.rateTemp.X1.7.m data.tempStor.rateTemp.X1.8.m data.tempStor.rateTemp.X1.9.m data.tempStor.rateTemp.X2.m data.tempStor.rateTemp.X2.1.m data.tempStor.rateTemp.X2.2.m data.tempStor.rateTemp.X2.3.m data.tempStor.rateTemp.X2.4.m data.tempStor.rateTemp.X2.5.m data.tempStor.rateTemp.X2.6.m data.tempStor.rateTemp.X2.7.m data.tempStor.rateTemp.X2.8.m data.tempStor.rateTemp.X2.9.m data.tempStor.rateTemp.X3.m data.tempStor.rateTemp.X3.1.m data.tempStor.rateTemp.X3.2.m data.tempStor.rateTemp.X3.3.m data.tempStor.rateTemp.X3.4.m data.tempStor.rateTemp.X3.5.m data.tempStor.rateTemp.X3.6.m data.tempStor.rateTemp.X3.7.m data.tempStor.rateTemp.X3.8.m data.tempStor.rateTemp.X3.9.m data.tempStor.rateTemp.X4.m data.tempStor.rateTemp.X4.1.m data.tempStor.rateTemp.X4.2.m data.tempStor.rateTemp.X4.3.m data.tempStor.rateTemp.X4.4.m data.tempStor.rateTemp.X4.5.m data.tempStor.rateTemp.X4.6.m data.tempStor.rateTemp.X4.7.m data.tempStor.rateTemp.X4.8.m data.tempStor.rateTemp.X4.9.m data.tempStor.rateTemp.X5.m data.tempStor.rateTemp.X5.1.m data.tempStor.rateTemp.X5.2.m data.tempStor.rateTemp.X5.3.m data.tempStor.rateTemp.X5.4.m data.tempStor.rateTemp.X5.5.m data.tempStor.rateTemp.X5.6.m data.tempStor.rateTemp.X5.7.m data.tempStor.rateTemp.X5.8.m data.tempStor.rateTemp.X5.9.m data.tempStor.rateTemp.X6.m data.tempStor.rateTemp.X6.1.m data.tempStor.rateTemp.X6.2.m data.tempStor.rateTemp.X6.3.m data.tempStor.rateTemp.X6.4.m data.tempStor.rateTemp.X6.5.m data.tempStor.rateTemp.X6.6.m data.tempStor.rateTemp.X6.7.m data.tempStor.rateTemp.X6.8.m data.tempStor.rateTemp.X6.9.m data.tempStor.rateTemp.X7.m data.tempStor.rateTemp.X7.1.m data.tempStor.rateTemp.X7.2.m data.tempStor.rateTemp.X7.3.m data.tempStor.rateTemp.X7.4.m data.tempStor.rateTemp.X7.5.m data.tempStor.rateTemp.X7.6.m data.tempStor.rateTemp.X7.7.m data.tempStor.rateTemp.X7.8.m data.tempStor.rateTemp.X7.9.m data.tempStor.rateTemp.X8.m data.tempStor.rateTemp.X8.1.m data.tempStor.rateTemp.X8.2.m data.tempStor.rateTemp.X8.3.m data.tempStor.rateTemp.X8.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.4.m qfqm.tempStor.rateTemp.X0.1.m qfqm.tempStor.rateTemp.X0.2.m qfqm.tempStor.rateTemp.X0.3.m qfqm.tempStor.rateTemp.X0.4.m qfqm.tempStor.rateTemp.X0.5.m qfqm.tempStor.rateTemp.X0.6.m qfqm.tempStor.rateTemp.X0.7.m qfqm.tempStor.rateTemp.X0.8.m qfqm.tempStor.rateTemp.X0.9.m qfqm.tempStor.rateTemp.X1.m qfqm.tempStor.rateTemp.X1.1.m qfqm.tempStor.rateTemp.X1.2.m qfqm.tempStor.rateTemp.X1.3.m qfqm.tempStor.rateTemp.X1.4.m qfqm.tempStor.rateTemp.X1.5.m qfqm.tempStor.rateTemp.X1.6.m qfqm.tempStor.rateTemp.X1.7.m qfqm.tempStor.rateTemp.X1.8.m qfqm.tempStor.rateTemp.X1.9.m qfqm.tempStor.rateTemp.X2.m qfqm.tempStor.rateTemp.X2.1.m qfqm.tempStor.rateTemp.X2.2.m qfqm.tempStor.rateTemp.X2.3.m qfqm.tempStor.rateTemp.X2.4.m qfqm.tempStor.rateTemp.X2.5.m qfqm.tempStor.rateTemp.X2.6.m qfqm.tempStor.rateTemp.X2.7.m qfqm.tempStor.rateTemp.X2.8.m qfqm.tempStor.rateTemp.X2.9.m qfqm.tempStor.rateTemp.X3.m qfqm.tempStor.rateTemp.X3.1.m qfqm.tempStor.rateTemp.X3.2.m qfqm.tempStor.rateTemp.X3.3.m qfqm.tempStor.rateTemp.X3.4.m qfqm.tempStor.rateTemp.X3.5.m qfqm.tempStor.rateTemp.X3.6.m qfqm.tempStor.rateTemp.X3.7.m qfqm.tempStor.rateTemp.X3.8.m qfqm.tempStor.rateTemp.X3.9.m qfqm.tempStor.rateTemp.X4.m qfqm.tempStor.rateTemp.X4.1.m qfqm.tempStor.rateTemp.X4.2.m qfqm.tempStor.rateTemp.X4.3.m qfqm.tempStor.rateTemp.X4.4.m qfqm.tempStor.rateTemp.X4.5.m qfqm.tempStor.rateTemp.X4.6.m qfqm.tempStor.rateTemp.X4.7.m qfqm.tempStor.rateTemp.X4.8.m qfqm.tempStor.rateTemp.X4.9.m qfqm.tempStor.rateTemp.X5.m qfqm.tempStor.rateTemp.X5.1.m qfqm.tempStor.rateTemp.X5.2.m qfqm.tempStor.rateTemp.X5.3.m qfqm.tempStor.rateTemp.X5.4.m qfqm.tempStor.rateTemp.X5.5.m qfqm.tempStor.rateTemp.X5.6.m qfqm.tempStor.rateTemp.X5.7.m qfqm.tempStor.rateTemp.X5.8.m qfqm.tempStor.rateTemp.X5.9.m qfqm.tempStor.rateTemp.X6.m qfqm.tempStor.rateTemp.X6.1.m qfqm.tempStor.rateTemp.X6.2.m qfqm.tempStor.rateTemp.X6.3.m qfqm.tempStor.rateTemp.X6.4.m qfqm.tempStor.rateTemp.X6.5.m qfqm.tempStor.rateTemp.X6.6.m qfqm.tempStor.rateTemp.X6.7.m qfqm.tempStor.rateTemp.X6.8.m qfqm.tempStor.rateTemp.X6.9.m qfqm.tempStor.rateTemp.X7.m qfqm.tempStor.rateTemp.X7.1.m qfqm.tempStor.rateTemp.X7.2.m qfqm.tempStor.rateTemp.X7.3.m qfqm.tempStor.rateTemp.X7.4.m qfqm.tempStor.rateTemp.X7.5.m qfqm.tempStor.rateTemp.X7.6.m qfqm.tempStor.rateTemp.X7.7.m qfqm.tempStor.rateTemp.X7.8.m qfqm.tempStor.rateTemp.X7.9.m qfqm.tempStor.rateTemp.X8.m qfqm.tempStor.rateTemp.X8.1.m qfqm.tempStor.rateTemp.X8.2.m qfqm.tempStor.rateTemp.X8.3.m qfqm.tempStor.rateTemp.X8.4.m 2018-06-01 00:00:00 2018-06-01 00:29:59 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 -0.0001014 -0.0001014 -0.0001014 -0.0001014 -0.0001014 -0.0001051 -0.0001112 -0.0001172 -0.0001233 -0.0001294 -0.0001354 -0.0001415 -0.0001476 -0.0001537 -0.0001597 -0.0001658 -0.0001719 -0.0001779 -0.0001840 -0.0001857 -0.0001870 -0.0001882 -0.0001895 -0.0001907 -0.0001919 -0.0001932 -0.0001944 -0.0001956 -0.0001969 -0.0001981 -0.0001994 -0.0002006 -0.0002018 -0.0002031 -0.0002043 -0.0002055 -0.0002068 -0.0002080 -0.0002093 -0.0002105 -0.0002117 -0.0002130 -0.0002142 -0.0002154 -0.0002172 -0.0002190 -0.0002208 -0.0002225 -0.0002243 -0.0002261 -0.0002278 -0.0002296 -0.0002314 -0.0002332 -0.0002349 -0.0002367 -0.0002385 -0.0002402 -0.0002420 -0.0002438 -0.0002456 -0.0002473 -0.0002491 -0.0002509 -0.0002526 -0.0002544 -0.0002562 -0.0002580 -0.0002597 -0.0002615 -0.0002633 -0.0002651 -0.0002668 -0.0002686 -0.0002704 -0.0002721 -0.0002739 -0.0002757 -0.0002775 -0.0002792 -0.0002810 -0.0002828 -0.0002845 -0.0002863 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01 00:30:00 2018-06-01 00:59:59 0.0004879 0.0004879 0.0004879 0.0004879 0.0004879 0.0004674 0.0004331 0.0003989 0.0003647 0.0003305 0.0002963 0.0002621 0.0002278 0.0001936 0.0001594 0.0001252 0.0000910 0.0000568 0.0000225 0.0000381 0.0000592 0.0000803 0.0001013 0.0001224 0.0001435 0.0001646 0.0001857 0.0002068 0.0002278 0.0002489 0.0002700 0.0002911 0.0003122 0.0003333 0.0003543 0.0003754 0.0003965 0.0004176 0.0004387 0.0004598 0.0004808 0.0005019 0.0005230 0.0005441 0.0005393 0.0005346 0.0005298 0.0005251 0.0005203 0.0005156 0.0005108 0.0005061 0.0005013 0.0004966 0.0004918 0.0004871 0.0004823 0.0004776 0.0004728 0.0004681 0.0004633 0.0004586 0.0004538 0.0004491 0.0004443 0.0004396 0.0004348 0.0004301 0.0004253 0.0004206 0.0004158 0.0004111 0.0004063 0.0004016 0.0003968 0.0003921 0.0003873 0.0003826 0.0003778 0.0003731 0.0003683 0.0003636 0.0003588 0.0003541 -0.0003565 -0.0003565 -0.0003565 -0.0003565 -0.0003565 -0.0003426 -0.0003192 -0.0002959 -0.0002726 -0.0002493 -0.0002260 -0.0002027 -0.0001793 -0.0001560 -0.0001327 -0.0001094 -0.0000861 -0.0000628 -0.0000394 -0.0000366 -0.0000361 -0.0000356 -0.0000350 -0.0000345 -0.0000339 -0.0000334 -0.0000329 -0.0000323 -0.0000318 -0.0000313 -0.0000307 -0.0000302 -0.0000296 -0.0000291 -0.0000286 -0.0000280 -0.0000275 -0.0000270 -0.0000264 -0.0000259 -0.0000253 -0.0000248 -0.0000243 -0.0000237 -0.0000279 -0.0000321 -0.0000363 -0.0000405 -0.0000446 -0.0000488 -0.0000530 -0.0000572 -0.0000614 -0.0000656 -0.0000697 -0.0000739 -0.0000781 -0.0000823 -0.0000865 -0.0000906 -0.0000948 -0.0000990 -0.0001032 -0.0001074 -0.0001116 -0.0001157 -0.0001199 -0.0001241 -0.0001283 -0.0001325 -0.0001366 -0.0001408 -0.0001450 -0.0001492 -0.0001534 -0.0001576 -0.0001617 -0.0001659 -0.0001701 -0.0001743 -0.0001785 -0.0001827 -0.0001868 -0.0001910 -0.0001814 -0.0001814 -0.0001814 -0.0001814 -0.0001814 -0.0001725 -0.0001576 -0.0001427 -0.0001278 -0.0001129 -0.0000980 -0.0000831 -0.0000683 -0.0000534 -0.0000385 -0.0000236 -0.0000087 0.0000062 0.0000211 0.0000247 0.0000271 0.0000295 0.0000318 0.0000342 0.0000366 0.0000390 0.0000414 0.0000437 0.0000461 0.0000485 0.0000509 0.0000533 0.0000556 0.0000580 0.0000604 0.0000628 0.0000652 0.0000675 0.0000699 0.0000723 0.0000747 0.0000771 0.0000794 0.0000818 0.0000836 0.0000854 0.0000872 0.0000890 0.0000908 0.0000926 0.0000944 0.0000962 0.0000980 0.0000998 0.0001016 0.0001033 0.0001051 0.0001069 0.0001087 0.0001105 0.0001123 0.0001141 0.0001159 0.0001177 0.0001195 0.0001213 0.0001231 0.0001249 0.0001267 0.0001285 0.0001303 0.0001320 0.0001338 0.0001356 0.0001374 0.0001392 0.0001410 0.0001428 0.0001446 0.0001464 0.0001482 0.0001500 0.0001518 0.0001536 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01 01:00:00 2018-06-01 01:29:59 0.0005086 0.0005086 0.0005086 0.0005086 0.0005086 0.0005025 0.0004925 0.0004825 0.0004724 0.0004624 0.0004523 0.0004423 0.0004323 0.0004222 0.0004122 0.0004021 0.0003921 0.0003820 0.0003720 0.0003480 0.0003224 0.0002968 0.0002712 0.0002457 0.0002201 0.0001945 0.0001689 0.0001434 0.0001178 0.0000922 0.0000666 0.0000410 0.0000155 -0.0000101 -0.0000357 -0.0000613 -0.0000869 -0.0001124 -0.0001380 -0.0001636 -0.0001892 -0.0002147 -0.0002403 -0.0002659 -0.0002551 -0.0002443 -0.0002335 -0.0002227 -0.0002119 -0.0002011 -0.0001904 -0.0001796 -0.0001688 -0.0001580 -0.0001472 -0.0001364 -0.0001256 -0.0001148 -0.0001040 -0.0000932 -0.0000824 -0.0000716 -0.0000609 -0.0000501 -0.0000393 -0.0000285 -0.0000177 -0.0000069 0.0000039 0.0000147 0.0000255 0.0000363 0.0000471 0.0000579 0.0000686 0.0000794 0.0000902 0.0001010 0.0001118 0.0001226 0.0001334 0.0001442 0.0001550 0.0001658 -0.0002073 -0.0002073 -0.0002073 -0.0002073 -0.0002073 -0.0002152 -0.0002283 -0.0002414 -0.0002545 -0.0002676 -0.0002807 -0.0002938 -0.0003069 -0.0003200 -0.0003331 -0.0003462 -0.0003593 -0.0003724 -0.0003855 -0.0003947 -0.0004034 -0.0004122 -0.0004209 -0.0004297 -0.0004384 -0.0004472 -0.0004559 -0.0004647 -0.0004735 -0.0004822 -0.0004910 -0.0004997 -0.0005085 -0.0005172 -0.0005260 -0.0005347 -0.0005435 -0.0005523 -0.0005610 -0.0005698 -0.0005785 -0.0005873 -0.0005960 -0.0006048 -0.0005965 -0.0005881 -0.0005798 -0.0005714 -0.0005631 -0.0005548 -0.0005464 -0.0005381 -0.0005297 -0.0005214 -0.0005131 -0.0005047 -0.0004964 -0.0004880 -0.0004797 -0.0004713 -0.0004630 -0.0004547 -0.0004463 -0.0004380 -0.0004296 -0.0004213 -0.0004130 -0.0004046 -0.0003963 -0.0003879 -0.0003796 -0.0003713 -0.0003629 -0.0003546 -0.0003462 -0.0003379 -0.0003296 -0.0003212 -0.0003129 -0.0003045 -0.0002962 -0.0002879 -0.0002795 -0.0002712 -0.0002556 -0.0002556 -0.0002556 -0.0002556 -0.0002556 -0.0002458 -0.0002293 -0.0002129 -0.0001965 -0.0001801 -0.0001637 -0.0001473 -0.0001308 -0.0001144 -0.0000980 -0.0000816 -0.0000652 -0.0000487 -0.0000323 -0.0000270 -0.0000229 -0.0000188 -0.0000147 -0.0000106 -0.0000065 -0.0000024 0.0000017 0.0000058 0.0000099 0.0000140 0.0000181 0.0000222 0.0000263 0.0000304 0.0000345 0.0000386 0.0000427 0.0000468 0.0000509 0.0000550 0.0000591 0.0000632 0.0000673 0.0000714 0.0000739 0.0000765 0.0000790 0.0000815 0.0000840 0.0000866 0.0000891 0.0000916 0.0000941 0.0000967 0.0000992 0.0001017 0.0001042 0.0001068 0.0001093 0.0001118 0.0001143 0.0001169 0.0001194 0.0001219 0.0001244 0.0001270 0.0001295 0.0001320 0.0001345 0.0001371 0.0001396 0.0001421 0.0001446 0.0001472 0.0001497 0.0001522 0.0001547 0.0001573 0.0001598 0.0001623 0.0001648 0.0001674 0.0001699 0.0001724 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01 01:30:00 2018-06-01 01:59:59 0.0013277 0.0013277 0.0013277 0.0013277 0.0013277 0.0013735 0.0014499 0.0015263 0.0016027 0.0016790 0.0017554 0.0018318 0.0019082 0.0019845 0.0020609 0.0021373 0.0022137 0.0022900 0.0023664 0.0023013 0.0022204 0.0021396 0.0020587 0.0019778 0.0018970 0.0018161 0.0017353 0.0016544 0.0015735 0.0014927 0.0014118 0.0013310 0.0012501 0.0011692 0.0010884 0.0010075 0.0009267 0.0008458 0.0007649 0.0006841 0.0006032 0.0005224 0.0004415 0.0003606 0.0003730 0.0003853 0.0003976 0.0004099 0.0004222 0.0004345 0.0004468 0.0004591 0.0004714 0.0004837 0.0004960 0.0005083 0.0005206 0.0005329 0.0005452 0.0005575 0.0005698 0.0005821 0.0005944 0.0006067 0.0006190 0.0006313 0.0006436 0.0006559 0.0006682 0.0006805 0.0006928 0.0007051 0.0007174 0.0007297 0.0007420 0.0007543 0.0007666 0.0007789 0.0007912 0.0008035 0.0008158 0.0008281 0.0008404 0.0008527 0.0000313 0.0000313 0.0000313 0.0000313 0.0000313 0.0000337 0.0000378 0.0000418 0.0000459 0.0000499 0.0000540 0.0000580 0.0000620 0.0000661 0.0000701 0.0000742 0.0000782 0.0000823 0.0000863 0.0000849 0.0000830 0.0000810 0.0000791 0.0000771 0.0000751 0.0000732 0.0000712 0.0000692 0.0000673 0.0000653 0.0000634 0.0000614 0.0000594 0.0000575 0.0000555 0.0000536 0.0000516 0.0000496 0.0000477 0.0000457 0.0000437 0.0000418 0.0000398 0.0000379 0.0000349 0.0000319 0.0000290 0.0000260 0.0000230 0.0000201 0.0000171 0.0000141 0.0000112 0.0000082 0.0000052 0.0000023 -0.0000007 -0.0000037 -0.0000066 -0.0000096 -0.0000126 -0.0000155 -0.0000185 -0.0000215 -0.0000244 -0.0000274 -0.0000304 -0.0000333 -0.0000363 -0.0000393 -0.0000422 -0.0000452 -0.0000482 -0.0000511 -0.0000541 -0.0000571 -0.0000600 -0.0000630 -0.0000660 -0.0000689 -0.0000719 -0.0000749 -0.0000778 -0.0000808 -0.0010983 -0.0010983 -0.0010983 -0.0010983 -0.0010983 -0.0010630 -0.0010043 -0.0009456 -0.0008868 -0.0008281 -0.0007693 -0.0007106 -0.0006519 -0.0005931 -0.0005344 -0.0004756 -0.0004169 -0.0003582 -0.0002994 -0.0002907 -0.0002876 -0.0002845 -0.0002814 -0.0002783 -0.0002751 -0.0002720 -0.0002689 -0.0002658 -0.0002627 -0.0002595 -0.0002564 -0.0002533 -0.0002502 -0.0002471 -0.0002439 -0.0002408 -0.0002377 -0.0002346 -0.0002315 -0.0002283 -0.0002252 -0.0002221 -0.0002190 -0.0002159 -0.0002160 -0.0002161 -0.0002162 -0.0002163 -0.0002164 -0.0002165 -0.0002166 -0.0002167 -0.0002168 -0.0002169 -0.0002170 -0.0002171 -0.0002173 -0.0002174 -0.0002175 -0.0002176 -0.0002177 -0.0002178 -0.0002179 -0.0002180 -0.0002181 -0.0002182 -0.0002183 -0.0002184 -0.0002185 -0.0002186 -0.0002188 -0.0002189 -0.0002190 -0.0002191 -0.0002192 -0.0002193 -0.0002194 -0.0002195 -0.0002196 -0.0002197 -0.0002198 -0.0002199 -0.0002200 -0.0002201 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01 02:00:00 2018-06-01 02:29:59 0.0007344 0.0007344 0.0007344 0.0007344 0.0007344 0.0008510 0.0010454 0.0012397 0.0014341 0.0016284 0.0018228 0.0020171 0.0022115 0.0024058 0.0026002 0.0027946 0.0029889 0.0031833 0.0033776 0.0032919 0.0031751 0.0030583 0.0029415 0.0028247 0.0027079 0.0025911 0.0024743 0.0023575 0.0022407 0.0021239 0.0020071 0.0018903 0.0017735 0.0016567 0.0015399 0.0014231 0.0013063 0.0011895 0.0010727 0.0009558 0.0008390 0.0007222 0.0006054 0.0004886 0.0004664 0.0004442 0.0004219 0.0003997 0.0003775 0.0003553 0.0003330 0.0003108 0.0002886 0.0002664 0.0002441 0.0002219 0.0001997 0.0001774 0.0001552 0.0001330 0.0001108 0.0000885 0.0000663 0.0000441 0.0000218 -0.0000004 -0.0000226 -0.0000448 -0.0000671 -0.0000893 -0.0001115 -0.0001338 -0.0001560 -0.0001782 -0.0002004 -0.0002227 -0.0002449 -0.0002671 -0.0002894 -0.0003116 -0.0003338 -0.0003560 -0.0003783 -0.0004005 -0.0000316 -0.0000316 -0.0000316 -0.0000316 -0.0000316 -0.0000246 -0.0000130 -0.0000013 0.0000103 0.0000219 0.0000336 0.0000452 0.0000568 0.0000685 0.0000801 0.0000917 0.0001034 0.0001150 0.0001266 0.0001239 0.0001195 0.0001152 0.0001109 0.0001065 0.0001022 0.0000978 0.0000935 0.0000891 0.0000848 0.0000804 0.0000761 0.0000717 0.0000674 0.0000630 0.0000587 0.0000543 0.0000500 0.0000456 0.0000413 0.0000369 0.0000326 0.0000283 0.0000239 0.0000196 0.0000170 0.0000145 0.0000120 0.0000095 0.0000070 0.0000045 0.0000020 -0.0000005 -0.0000030 -0.0000056 -0.0000081 -0.0000106 -0.0000131 -0.0000156 -0.0000181 -0.0000206 -0.0000231 -0.0000256 -0.0000282 -0.0000307 -0.0000332 -0.0000357 -0.0000382 -0.0000407 -0.0000432 -0.0000457 -0.0000482 -0.0000508 -0.0000533 -0.0000558 -0.0000583 -0.0000608 -0.0000633 -0.0000658 -0.0000683 -0.0000708 -0.0000733 -0.0000759 -0.0000784 -0.0000809 -0.0006432 -0.0006432 -0.0006432 -0.0006432 -0.0006432 -0.0006189 -0.0005784 -0.0005379 -0.0004973 -0.0004568 -0.0004163 -0.0003758 -0.0003353 -0.0002947 -0.0002542 -0.0002137 -0.0001732 -0.0001326 -0.0000921 -0.0000846 -0.0000808 -0.0000770 -0.0000732 -0.0000694 -0.0000656 -0.0000618 -0.0000580 -0.0000541 -0.0000503 -0.0000465 -0.0000427 -0.0000389 -0.0000351 -0.0000313 -0.0000275 -0.0000236 -0.0000198 -0.0000160 -0.0000122 -0.0000084 -0.0000046 -0.0000008 0.0000030 0.0000069 0.0000066 0.0000064 0.0000062 0.0000059 0.0000057 0.0000055 0.0000052 0.0000050 0.0000048 0.0000045 0.0000043 0.0000041 0.0000039 0.0000036 0.0000034 0.0000032 0.0000029 0.0000027 0.0000025 0.0000022 0.0000020 0.0000018 0.0000015 0.0000013 0.0000011 0.0000008 0.0000006 0.0000004 0.0000002 -0.0000001 -0.0000003 -0.0000005 -0.0000008 -0.0000010 -0.0000012 -0.0000015 -0.0000017 -0.0000019 -0.0000022 -0.0000024 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01 02:30:00 2018-06-01 02:59:59 -0.0009450 -0.0009450 -0.0009450 -0.0009450 -0.0009450 -0.0007653 -0.0004659 -0.0001665 0.0001329 0.0004323 0.0007317 0.0010311 0.0013305 0.0016300 0.0019294 0.0022288 0.0025282 0.0028276 0.0031270 0.0030239 0.0028760 0.0027282 0.0025803 0.0024325 0.0022846 0.0021368 0.0019889 0.0018411 0.0016932 0.0015454 0.0013975 0.0012497 0.0011018 0.0009540 0.0008061 0.0006583 0.0005104 0.0003626 0.0002147 0.0000669 -0.0000810 -0.0002288 -0.0003767 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0005245 -0.0001513 -0.0001513 -0.0001513 -0.0001513 -0.0001513 -0.0001414 -0.0001250 -0.0001085 -0.0000921 -0.0000756 -0.0000591 -0.0000427 -0.0000262 -0.0000098 0.0000067 0.0000232 0.0000396 0.0000561 0.0000725 0.0000696 0.0000645 0.0000594 0.0000544 0.0000493 0.0000442 0.0000391 0.0000340 0.0000289 0.0000238 0.0000188 0.0000137 0.0000086 0.0000035 -0.0000016 -0.0000067 -0.0000118 -0.0000168 -0.0000219 -0.0000270 -0.0000321 -0.0000372 -0.0000423 -0.0000474 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0000524 -0.0010168 -0.0010168 -0.0010168 -0.0010168 -0.0010168 -0.0009788 -0.0009156 -0.0008524 -0.0007891 -0.0007259 -0.0006627 -0.0005994 -0.0005362 -0.0004729 -0.0004097 -0.0003465 -0.0002832 -0.0002200 -0.0001568 -0.0001510 -0.0001516 -0.0001523 -0.0001529 -0.0001535 -0.0001542 -0.0001548 -0.0001554 -0.0001561 -0.0001567 -0.0001574 -0.0001580 -0.0001586 -0.0001593 -0.0001599 -0.0001605 -0.0001612 -0.0001618 -0.0001624 -0.0001631 -0.0001637 -0.0001643 -0.0001650 -0.0001656 -0.0001662 -0.0001655 -0.0001647 -0.0001639 -0.0001631 -0.0001623 -0.0001615 -0.0001607 -0.0001600 -0.0001592 -0.0001584 -0.0001576 -0.0001568 -0.0001560 -0.0001552 -0.0001545 -0.0001537 -0.0001529 -0.0001521 -0.0001513 -0.0001505 -0.0001497 -0.0001490 -0.0001482 -0.0001474 -0.0001466 -0.0001458 -0.0001450 -0.0001442 -0.0001435 -0.0001427 -0.0001419 -0.0001411 -0.0001403 -0.0001395 -0.0001387 -0.0001380 -0.0001372 -0.0001364 -0.0001356 -0.0001348 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.6.7 Un-interpolated vertical profile data (Level 2) The Level 2 data are interpolated in time but not in space. They contain the rates of change at the measurement heights. Again, they can be extracted from the HDF5 files using stackEddy() with the same syntax: prof.l2 &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp02&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |= | 2% | |== | 3% | |==== | 5% | |===== | 7% | |====== | 8% | |======= | 10% | |======== | 12% | |========= | 13% | |========== | 15% | |============ | 17% | |============= | 18% | |============== | 20% | |=============== | 22% | |================ | 23% | |================== | 25% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 42% | |============================== | 43% | |================================ | 45% | |================================= | 47% | |================================== | 48% | |=================================== | 50% | |==================================== | 52% | |===================================== | 53% | |====================================== | 55% | |======================================== | 57% | |========================================= | 58% | |========================================== | 60% | |=========================================== | 62% | |============================================ | 63% | |============================================== | 65% | |=============================================== | 67% | |================================================ | 68% | |================================================= | 70% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 75% | |====================================================== | 77% | |======================================================= | 78% | |======================================================== | 80% | |========================================================= | 82% | |========================================================== | 83% | |============================================================ | 85% | |============================================================= | 87% | |============================================================== | 88% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 95% | |==================================================================== | 97% | |===================================================================== | 98% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% knitr::kable(head(prof.l2$HARV)) Note that here, as in the PAR data, there is a verticalPosition field. It has the same meaning as in the PAR data, indicating the tower level of the measurement. 5.6.8 Calibrated raw data (Level 1) Level 1 (dp01) data are calibrated, and aggregated in time, but otherwise untransformed. Use Level 1 data for raw gas concentrations and atmospheric stable isotopes. Using stackEddy() to extract Level 1 data requires additional inputs. The Level 1 files are too large to simply pull out all the variables by default, and they include mutiple averaging intervals, which cant be merged. So two additional inputs are needed: avg: The averaging interval to extract var: One or more variables to extract What variables are available, at what averaging intervals? Another function in the neonUtilities package, getVarsEddy(), returns a list of HDF5 file contents. It requires only one input, a filepath to a single NEON HDF5 file: vars &lt;- getVarsEddy(&quot;./data/filesToStack00200/NEON.D01.HARV.DP4.00200.001.nsae.2018-06.basic.20211220T155630Z.h5&quot;) knitr::kable(head(vars)) site level category system hor ver tmi subsys name otype dclass dim oth 5 HARV dp01 data amrs 000 060 01m NA angNedXaxs H5I_DATASET COMPOUND 43200 NA 6 HARV dp01 data amrs 000 060 01m NA angNedYaxs H5I_DATASET COMPOUND 43200 NA 7 HARV dp01 data amrs 000 060 01m NA angNedZaxs H5I_DATASET COMPOUND 43200 NA 9 HARV dp01 data amrs 000 060 30m NA angNedXaxs H5I_DATASET COMPOUND 1440 NA 10 HARV dp01 data amrs 000 060 30m NA angNedYaxs H5I_DATASET COMPOUND 1440 NA 11 HARV dp01 data amrs 000 060 30m NA angNedZaxs H5I_DATASET COMPOUND 1440 NA Inputs to var can be any values from the name field in the table returned by getVarsEddy(). Lets take a look at CO2 and H2O, 13C in CO2 and 18O in H2O, at 30-minute aggregation. Lets look at Harvard Forest for these data, since deeper canopies generally have more interesting profiles: iso &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp01&quot;, var=c(&quot;rtioMoleDryCo2&quot;,&quot;rtioMoleDryH2o&quot;, &quot;dlta13CCo2&quot;,&quot;dlta18OH2o&quot;), avg=30) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======= | 11% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========== | 14% | |========== | 15% | |=========== | 15% | |=========== | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================= | 24% | |================= | 25% | |================== | 25% | |================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |===================== | 31% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |======================== | 35% | |========================= | 35% | |========================= | 36% | |========================== | 37% | |========================== | 38% | |=========================== | 38% | |=========================== | 39% | |============================ | 39% | |============================ | 40% | |============================ | 41% | |============================= | 41% | |============================= | 42% | |============================== | 42% | |============================== | 43% | |=============================== | 44% | |=============================== | 45% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |====================================== | 54% | |====================================== | 55% | |======================================= | 55% | |======================================= | 56% | |======================================== | 57% | |======================================== | 58% | |========================================= | 58% | |========================================= | 59% | |========================================== | 59% | |========================================== | 60% | |========================================== | 61% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 62% | |============================================ | 63% | |============================================= | 64% | |============================================= | 65% | |============================================== | 65% | |============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |======================================================== | 81% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |=========================================================== | 84% | |=========================================================== | 85% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================= | 88% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 89% | |=============================================================== | 90% | |=============================================================== | 91% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 92% | |================================================================= | 93% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |= | 2% | |=== | 4% | |==== | 6% | |===== | 7% | |====== | 9% | |======== | 11% | |========= | 13% | |========== | 15% | |============ | 17% | |============= | 19% | |============== | 20% | |================ | 22% | |================= | 24% | |================== | 26% | |=================== | 28% | |===================== | 30% | |====================== | 31% | |======================= | 33% | |========================= | 35% | |========================== | 37% | |=========================== | 39% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 46% | |================================== | 48% | |=================================== | 50% | |==================================== | 52% | |====================================== | 54% | |======================================= | 56% | |======================================== | 57% | |========================================= | 59% | |=========================================== | 61% | |============================================ | 63% | |============================================= | 65% | |=============================================== | 67% | |================================================ | 69% | |================================================= | 70% | |=================================================== | 72% | |==================================================== | 74% | |===================================================== | 76% | |====================================================== | 78% | |======================================================== | 80% | |========================================================= | 81% | |========================================================== | 83% | |============================================================ | 85% | |============================================================= | 87% | |============================================================== | 89% | |================================================================ | 91% | |================================================================= | 93% | |================================================================== | 94% | |=================================================================== | 96% | |===================================================================== | 98% | |======================================================================| 100% knitr::kable(head(iso$HARV)) Lets plot vertical profiles of CO2 and 13C in CO2 on a single day. Here, for convenience, instead of converting the time stamps to a time format, its easy to use the character format to extract the ones we want using grep(). And discard the verticalPosition values that are string values - those are the calibration gases. iso.d &lt;- iso$HARV[grep(&quot;2018-06-25&quot;, iso$HARV$timeBgn, fixed=T),] iso.d &lt;- iso.d[-which(is.na(as.numeric(iso.d$verticalPosition))),] ## Warning in which(is.na(as.numeric(iso.d$verticalPosition))): NAs introduced by ## coercion ggplot is well suited to these types of data, lets use it to plot the profiles. library(ggplot2) g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.co2Stor.rtioMoleDryCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;CO2&quot;) + ylab(&quot;Tower level&quot;) g ## Warning: Removed 2 row(s) containing missing values (geom_path). g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.isoCo2.dlta13CCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;d13C&quot;) + ylab(&quot;Tower level&quot;) g Warning message: Removed 55 rows containing missing values (geom_path). The legends are omitted for space, see if you can work out the times of day the different colors represent. 5.6.9 Convert NEON flux data variables to AmeriFlux FP standard Install and load packages #Install NEONprocIS.base from GitHub, this package is a dependency of eddy4R.base devtools::install_github(repo=&quot;NEONScience/NEON-IS-data-processing&quot;, ref=&quot;master&quot;, subdir=&quot;pack/NEONprocIS.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) #Install eddy4R.base from GitHub devtools::install_github(repo=&quot;NEONScience/eddy4R&quot;, ref=&quot;master&quot;, subdir=&quot;pack/eddy4R.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) packReq &lt;- c(&quot;rhdf5&quot;, &quot;eddy4R.base&quot;, &quot;jsonlite&quot;, &quot;lubridate&quot;) lapply(packReq, function(x) { print(x) if(require(x, character.only = TRUE) == FALSE) { install.packages(x) library(x, character.only = TRUE) }}) Select your site of interest from the list of NEON sites below. site &lt;- &quot;KONZ&quot; #&quot;BARR&quot;,&quot;CLBJ&quot;,&quot;MLBS&quot;,&quot;DSNY&quot;,&quot;NIWO&quot;,&quot;ORNL&quot;,&quot;OSBS&quot;, #&quot;SCBI&quot;,&quot;LENO&quot;,&quot;TALL&quot;,&quot;CPER&quot;,&quot;BART&quot;,&quot;HARV&quot;,&quot;BLAN&quot;, #&quot;SERC&quot;,&quot;JERC&quot;,&quot;GUAN&quot;,&quot;LAJA&quot;,&quot;STEI&quot;,&quot;TREE&quot;,&quot;UNDE&quot;, #&quot;KONA&quot;,&quot;KONZ&quot;,&quot;UKFS&quot;,&quot;GRSM&quot;,&quot;DELA&quot;,&quot;DCFS&quot;,&quot;NOGP&quot;, #&quot;WOOD&quot;,&quot;RMNP&quot;,&quot;OAES&quot;,&quot;YELL&quot;,&quot;MOAB&quot;,&quot;STER&quot;,&quot;JORN&quot;, #&quot;SRER&quot;,&quot;ONAQ&quot;,&quot;ABBY&quot;,&quot;WREF&quot;,&quot;SJER&quot;,&quot;SOAP&quot;,&quot;TEAK&quot;, #&quot;TOOL&quot;,&quot;BONA&quot;,&quot;DEJU&quot;,&quot;HEAL&quot;,&quot;PUUM&quot; } If you would like to download a set range of dates, define the following paramemters. If these are not defined, it will default to the entire record at the site #define start and end dates, optional, defaults to entire period of site operation. Use %Y-%m-%d format. dateBgn &lt;- &quot;2020-03-01&quot; dateEnd &lt;- &quot;2020-05-31&quot; # Data package from the portal Pack &lt;- c(&#39;basic&#39;,&#39;expanded&#39;)[1] #The version data for the FP standard conversion processing ver = paste0(&quot;v&quot;,format(Sys.time(), &quot;%Y%m%dT%H%m&quot;)) Specify Download directory for HDF5 files from the NEON data portal and output directory to save the resulting csv files. Change save paths to where you want the files on your computer. #download directory DirDnld=tempdir() #Output directory, change this to where you want to save the output csv DirOutBase &lt;-paste0(&quot;~/eddy/data/Ameriflux/&quot;,ver) Specify Data Product number, for the Bundled Eddy-Covariance files, this is DP4.00200.001 #DP number dpID &lt;- &#39;DP4.00200.001&#39; Get metadata from Ameriflux Site Info BADM sheets for the site of interest #Grab a list of all Ameriflux sites, containing site ID and site description sites_web &lt;- jsonlite::fromJSON(&quot;http://ameriflux-data.lbl.gov/AmeriFlux/SiteSearch.svc/SiteList/AmeriFlux&quot;) #Grab only NEON sites sitesNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] #For all NEON sites siteNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] metaSite &lt;- lapply(siteNeon$SITE_ID, function(x) { pathSite &lt;- paste0(&quot;http://ameriflux-data.lbl.gov/BADM/Anc/SiteInfo/&quot;,x) tmp &lt;- fromJSON(pathSite) return(tmp) }) Use Ameriflux site IDs to name metadata lists #use NEON ID as list name names(metaSite) &lt;- site #Use Ameriflux site ID as list name #names(metaSite) &lt;- sitesNeon$SITE_ID Check if dateBgn is defined, if not make it the initial operations date IOCR of the site if(!exists(&quot;dateBgn&quot;) || is.na(dateBgn) || is.null(dateBgn)){ dateBgn &lt;- as.Date(metaSite[[site]]$values$GRP_FLUX_MEASUREMENTS[[1]]$FLUX_MEASUREMENTS_DATE_START, &quot;%Y%m%d&quot;) } else { dateBgn &lt;- dateBgn }#End of checks for missing dateBgn #Check if dateEnd is defined, if not make it the system date if(!exists(&quot;dateEnd&quot;) || is.na(dateEnd) || is.null(dateEnd)){ dateEnd &lt;- as.Date(Sys.Date()) } else { dateEnd &lt;- dateEnd }#End of checks for missing dateEnd Grab the UTC time offset from the Ameriflux API timeOfstUtc &lt;- as.integer(metaSite[[site]]$values$GRP_UTC_OFFSET[[1]]$UTC_OFFSET) Create the date sequence setDate &lt;- seq(from = as.Date(dateBgn), to = as.Date(dateEnd), by = &quot;month&quot;) Start processing the site time range specified, verify that the site and date range are specified as intended msg &lt;- paste0(&quot;Starting Ameriflux FP standard conversion processing workflow for &quot;, site, &quot; for &quot;, dateBgn, &quot; to &quot;, dateEnd) print(msg) Create output directory by checking if the download directory exists and create it if not if(dir.exists(DirDnld) == FALSE) dir.create(DirDnld, recursive = TRUE) #Append the site to the base output directory DirOut &lt;- paste0(DirOutBase, &quot;/&quot;, siteNeon$SITE_ID) #Check if directory exists and create if not if(!dir.exists(DirOut)) dir.create(DirOut, recursive = TRUE) Download and extract data #Initialize data List dataList &lt;- list() #Read data from the API dataList &lt;- lapply(setDate, function(x) { # year &lt;- lubridate::year(x) # mnth &lt;- lubridate::month(x) date &lt;- stringr::str_extract(x, pattern = paste0(&quot;[0-9]{4}&quot;, &quot;-&quot;, &quot;[0-9]{2}&quot;)) tryCatch(neonUtilities::zipsByProduct(dpID = dpID, site = site, startdate = date, enddate = date, package = &quot;basic&quot;, savepath = DirDnld, check.size = FALSE), error=function(e) NULL) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) utils::unzip(paste0(DirDnld, &quot;/filesToStack00200/&quot;, files[grep(pattern = paste0(site,&quot;.*.&quot;, date, &quot;.*.zip&quot;), x = files)]), exdir = paste0(DirDnld, &quot;/filesToStack00200&quot;)) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) dataIdx &lt;- rhdf5::h5read(file = paste0(DirDnld, &quot;/filesToStack00200/&quot;, max(files[grep(pattern = paste0(site,&quot;.*.&quot;, date,&quot;.*.h5&quot;), x = files)])), name = paste0(site, &quot;/&quot;)) if(!is.null(dataIdx)){ dataIdx$dp0p &lt;- NULL dataIdx$dp02 &lt;- NULL dataIdx$dp03 &lt;- NULL dataIdx$dp01$ucrt &lt;- NULL dataIdx$dp04$ucrt &lt;- NULL dataIdx$dp01$data &lt;- lapply(dataIdx$dp01$data,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) dataIdx$dp01$qfqm &lt;- lapply(dataIdx$dp01$qfqm,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) } return(dataIdx) }) Add names to list for year/month combinations names(dataList) &lt;- paste0(lubridate::year(setDate),sprintf(&quot;%02d&quot;,lubridate::month(setDate))) Remove NULL elements from list dataList &lt;- dataList[vapply(dataList, Negate(is.null), NA)] Determine tower horizontal &amp; vertical indices #Find the tower top level by looking at the vertical index of the turbulent CO2 concentration measurements LvlTowr &lt;- grep(pattern = &quot;_30m&quot;, names(dataList[[1]]$dp01$data$co2Turb), value = TRUE) LvlTowr &lt;- gsub(x = LvlTowr, pattern = &quot;_30m&quot;, replacement = &quot;&quot;) #get tower top level LvlTop &lt;- strsplit(LvlTowr,&quot;&quot;) LvlTop &lt;- base::as.numeric(LvlTop[[1]][6]) #Ameriflux vertical levels based off of https://ameriflux.lbl.gov/data/aboutdata/data-variables/ section 3.3.1 &quot;Indices must be in order, starting with the highest.&quot; idxVerAmfx &lt;- base::seq(from = 1, to = LvlTop, by = 1) #get the sequence from top to first level LvlMeas &lt;- base::seq(from = LvlTop, to = 1, by = -1) #Recreate NEON naming conventions LvlMeas &lt;- paste0(&quot;000_0&quot;,LvlMeas,&quot;0&quot;,sep=&quot;&quot;) #Give NEON naming conventions to Ameriflux vertical levels names(idxVerAmfx) &lt;- LvlMeas #Ameriflux horizontal index idxHorAmfx &lt;- 1 Subset to the Ameriflux variables to convert dataListFlux &lt;- lapply(names(dataList), function(x) { data.frame( &quot;TIMESTAMP_START&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), &quot;TIMESTAMP_END&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), # &quot;TIMESTAMP_START&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;), format = &quot;%Y%m%d%H%M&quot;), # &quot;TIMESTAMP_END&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;) + 60, format = &quot;%Y%m%d%H%M&quot;), &quot;FC&quot;= dataList[[x]]$dp04$data$fluxCo2$turb$flux, &quot;SC&quot;= dataList[[x]]$dp04$data$fluxCo2$stor$flux, &quot;NEE&quot;= dataList[[x]]$dp04$data$fluxCo2$nsae$flux, &quot;LE&quot; = dataList[[x]]$dp04$data$fluxH2o$turb$flux, &quot;SLE&quot; = dataList[[x]]$dp04$data$fluxH2o$stor$flux, &quot;USTAR&quot; = dataList[[x]]$dp04$data$fluxMome$turb$veloFric, &quot;H&quot; = dataList[[x]]$dp04$data$fluxTemp$turb$flux, &quot;SH&quot; = dataList[[x]]$dp04$data$fluxTemp$stor$flux, &quot;FETCH_90&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxs90, &quot;FETCH_MAX&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxsMax, &quot;V_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloYaxsHorSd, #&quot;W_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloZaxsHorSd, &quot;CO2_1_1_1&quot; = dataList[[x]]$dp01$data$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean, &quot;H2O_1_1_1&quot; = dataList[[x]]$dp01$data$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean, &quot;qfFinlH2oTurbFrt00Samp&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$frt00Samp$qfFinl, &quot;qfH2O_1_1_1&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl, &quot;qfCO2_1_1_1&quot; = dataList[[x]]$dp01$qfqm$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl, &quot;qfSC&quot; = dataList[[x]]$dp04$qfqm$fluxCo2$stor$qfFinl, &quot;qfSLE&quot; = dataList[[x]]$dp04$qfqm$fluxH2o$stor$qfFinl, &quot;qfSH&quot; = dataList[[x]]$dp04$qfqm$fluxTemp$stor$qfFinl, &quot;qfT_SONIC&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$qfFinl, &quot;qfWS_1_1_1&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$qfFinl, rbind.data.frame(lapply(names(idxVerAmfx), function(y) { tryCatch({rlog$debug(y)}, error=function(cond){print(y)}) rpt &lt;- list() rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$co2Stor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$h2oStor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$co2Stor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$h2oStor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoCo2[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoH2o[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt &lt;- rbind.data.frame(rpt) return(rpt) } )), &quot;WS_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$mean, &quot;WS_MAX_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$max, &quot;WD_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$angZaxsErth$mean, &quot;T_SONIC&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean, &quot;T_SONIC_SIGMA&quot; = base::sqrt(dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean) , stringsAsFactors = FALSE) }) names(dataListFlux) &lt;- names(dataList) Combine the monthly data into a single dataframe, remove lists and clean memory dataDfFlux &lt;- do.call(rbind.data.frame,dataListFlux) rm(list=c(&quot;dataListFlux&quot;,&quot;dataList&quot;)) gc() Regularize timeseries to 30 minutes in case timestamps are missing from NEON files due to processing errors timeRglr &lt;- eddy4R.base::def.rglr(timeMeas = as.POSIXlt(dataDfFlux$TIMESTAMP_START), dataMeas = dataDfFlux, BgnRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_START[1]), EndRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_END[length(dataDfFlux$TIMESTAMP_END)]), TzRglr = &quot;UTC&quot;, FreqRglr = 1/(60*30)) #Reassign data to data.frame dataDfFlux &lt;- timeRglr$dataRglr #Format timestamps dataDfFlux$TIMESTAMP_START &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc), format = &quot;%Y%m%d%H%M&quot;) dataDfFlux$TIMESTAMP_END &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc) + lubridate::minutes(30), format = &quot;%Y%m%d%H%M&quot;) Define validation times, and remove this data from the dataset. At NEON sites, validations with a series of gasses of known concentration are run every 23.5 hours. These values are used to correct for measurment drift and are run every 23.5 hours to achive daily resolution while also spreading the impact of lost measurements throughout the day. #Remove co2Turb and h2oTurb data based off of qfFlow (qfFinl frt00) dataDfFlux$FC[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN dataDfFlux$LE[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$USTAR[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$SC[(which(dataDfFlux$qfSC == 1))] &lt;- NaN dataDfFlux$SLE[(which(dataDfFlux$qfSLE == 1))] &lt;- NaN dataDfFlux$SH[(which(dataDfFlux$qfSH == 1))] &lt;- NaN dataDfFlux$T_SONIC[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$T_SONIC_SIGMA[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_MAX_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WD_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H2O_1_1_1[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$CO2_1_1_1[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN lapply(idxVerAmfx, function(x){ #x &lt;- 1 dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN }) Remove quality flagging variables from output setIdxQf &lt;- grep(&quot;qf&quot;, names(dataDfFlux)) dataDfFlux[,setIdxQf] &lt;- NULL Set range thresholds #assign list Rng &lt;- list() Rng$Min &lt;- data.frame( &quot;FC&quot; = -100, #[umol m-2 s-1] &quot;SC&quot; = -100, #[umol m-2 s-1] &quot;NEE&quot; = -100, #[umol m-2 s-1] &quot;LE&quot; = -500, #[W m-2] &quot;H&quot; = -500, #[W m-2] &quot;USTAR&quot; = 0, #[m s-1] &quot;CO2&quot; = 200, #[umol mol-1] &quot;H2O&quot; = 0, #[mmol mol-1] &quot;WS_1_1_1&quot; = 0, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 0, #[m s-1] &quot;WD_1_1_1&quot; = -0.1, #[deg] &quot;T_SONIC&quot; = -55.0 #[C] ) Set Max thresholds Rng$Max &lt;- data.frame( &quot;FC&quot; = 100, #[umol m-2 s-1] &quot;SC&quot; = 100, #[umol m-2 s-1] &quot;NEE&quot; = 100, #[umol m-2 s-1] &quot;LE&quot; = 1000, #[W m-2] &quot;H&quot; = 1000, #[W m-2] &quot;USTAR&quot; = 5, #[m s-1] &quot;CO2&quot; = 800, #[umol mol-1] &quot;H2O&quot; = 100, #[mmol mol-1] &quot;WS_1_1_1&quot; = 50, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 50, #[m s-1] &quot;WD_1_1_1&quot; = 360, #[deg] &quot;T_SONIC&quot; = 45.0 #[C] ) Grab all CO2/H2O columns to apply same thresholds, replace missing values with -9999 nameCO2 &lt;- grep(&quot;CO2&quot;,names(dataDfFlux),value = TRUE) nameH2O &lt;- grep(&quot;H2O&quot;,names(dataDfFlux),value = TRUE) #Apply the CO2/H2O threshold to all variables in HOR_VER_REP Rng$Min[nameCO2] &lt;- Rng$Min$CO2 Rng$Min[nameH2O] &lt;- Rng$Min$H2O Rng$Max[nameCO2] &lt;- Rng$Max$CO2 Rng$Max[nameH2O] &lt;- Rng$Max$H2O #Apply the range test to the output, and replace values with NaN lapply(names(dataDfFlux), function(x) { dataDfFlux[which(dataDfFlux[,x]&lt;Rng$Min[[x]] | dataDfFlux[,x]&gt;Rng$Max[[x]]),x] &lt;&lt;- NaN}) # Delete any NEE that have either FC or SC removed dataDfFlux[is.na(dataDfFlux$FC) | is.na(dataDfFlux$SC),&quot;NEE&quot;] &lt;- NaN #Change NA to -9999 dataDfFlux[is.na(dataDfFlux)] &lt;- -9999 Write output data to csv #Create output filename based off of Ameriflux file naming convention nameFileOut &lt;- base::paste0(DirOut,&quot;/&quot;,siteNeon$SITE_ID,&#39;_HH_&#39;,dataDfFlux$TIMESTAMP_START[1],&#39;_&#39;,utils::tail(dataDfFlux$TIMESTAMP_END,n=1),&#39;_flux.csv&#39;) #Write output to .csv write.csv(x = dataDfFlux, file = nameFileOut, row.names = FALSE) Clean up environment rm(list=&quot;dataDfFlux&quot;) gc() 5.7 Exercises 5.7.1 Computational NEON data are submitted to AmeriFlux quarterly after one year of non-quality flagged or otherwise missing data are available. Use the workflow above to extend the data coverage of an already submitted NEON site by downloading existing data from the AmeriFlux website and recently published HDF5 files from the NEON data portal. Process the NEON data such that it is in AmeriFlux format and plot the entire timerseries. Hint: NEON sites start with US-x Using metScanR package, find co-located NEON and AmeriFlux sites. Download data for an overlapping time period, and compare FC and H values by making a scatter plot and seeing how far off the data are from a 1:1 line. "],["neon-aop.html", "Chapter 6 NEON AOP 6.1 Hyperspectral Remote Sensing 6.2 Key Metadata for Hyperspectral Data 6.3 Intro to Working with Hyperspectral Remote Sensing Data in HDF5 Format 6.4 About Hyperspectral Remote Sensing Data 6.5 Read HDF5 data into R 6.6 Create a Georeferenced Raster 6.7 Light Detection And Ranging (LiDAR) Data 6.8 Calculating Forest Structural Diversity Metrics from NEON LiDAR Data 6.9 Introduction to Structural Diversity Metrics 6.10 NEON AOP Discrete Return LIDAR 6.11 Calculating Structural Diversity Metrics 6.12 Matching GEDI waveforms with NEON AOP LiDAR pointclouds 6.13 NEON AOP Written Questions: 6.14 NEON AOP Coding Lab 6.15 NEON AOP Culmination Write Up", " Chapter 6 NEON AOP 6.1 Hyperspectral Remote Sensing 6.1.1 Learning Objectives After completing this tutorial, you will be able to: Define hyperspectral remote sensing. Explain the fundamental principles of hyperspectral remote sensing data. Describe the key attributes that are required to effectively work with hyperspectral remote sensing data in tools like R or Python. Describe what a band is. 6.1.1.1 Mapping the Invisible 6.1.2 About Hyperspectral Remote Sensing Data The electromagnetic spectrum is composed of thousands of bands representing different types of light energy. Imaging spectrometers (instruments that collect hyperspectral data) break the electromagnetic spectrum into groups of bands that support classification of objects by their spectral properties on the earths surface. Hyperspectral data consists of many bands  up to hundreds of bands  that cover the electromagnetic spectrum. The NEON imaging spectrometer collects data within the 380nm to 2510nm portions of the electromagnetic spectrum within bands that are approximately 5nm in width. This results in a hyperspectral data cube that contains approximately 426 bands - which means big, big data. 6.2 Key Metadata for Hyperspectral Data 6.2.1 Bands and Wavelengths A band represents a group of wavelengths. For example, the wavelength values between 695nm and 700nm might be one band as captured by an imaging spectrometer. The imaging spectrometer collects reflected light energy in a pixel for light in that band. Often when you work with a multi or hyperspectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 695-700 nm, the center would be 697.5). Imaging spectrometers collect reflected light information within defined bands or regions of the electromagnetic spectrum. Source: National Ecological Observatory Network (NEON) 6.2.2 Spectral Resolution The spectral resolution of a dataset that has more than one band, refers to the width of each band in the dataset. In the example above, a band was defined as spanning 695-700nm. The width or spatial resolution of the band is thus 5 nanometers. To see an example of this, check out the band widths for the Landsat sensors. 6.2.3 Full Width Half Max (FWHM) The full width half max (FWHM) will also often be reported in a multi or hyperspectral dataset. This value represents the spread of the band around that center point. The Full Width Half Max (FWHM) of a band relates to the distance in nanometers between the band center and the edge of the band. In this case, the FWHM for Band C is 5 nm. In the illustration above, the band that covers 695-700nm has a FWHM of 5 nm. While a general spectral resolution of the sensor is often provided, not all sensors create bands of uniform widths. For instance bands 1-9 of Landsat 8 are listed below (Courtesy of USGS) Band Wavelength range (microns) Spatial Resolution (m) Spectral Width (microns) Band 1 - Coastal aerosol 0.43 - 0.45 30 0.02 Band 2 - Blue 0.45 - 0.51 30 0.06 Band 3 - Green 0.53 - 0.59 30 0.06 Band 4 - Red 0.64 - 0.67 30 0.03 Band 5 - Near Infrared (NIR) 0.85 - 0.88 30 0.03 Band 6 - SWIR 1 1.57 - 1.65 30 0.08 Band 7 - SWIR 2 2.11 - 2.29 30 0.18 Band 8 - Panchromatic 0.50 - 0.68 15 0.18 Band 9 - Cirrus 1.36 - 1.38 30 0.02 6.3 Intro to Working with Hyperspectral Remote Sensing Data in HDF5 Format Contributors: Felipe Sanchez, Leah A. Wasser, Edmund Hart, Donal OLeary Based on NEON Science Tutorial Series: https://www.neonscience.org/intro-hsi-r-series In this tutorial, we will explore reading and extracting spatial raster data stored within a HDF5 file using R. 6.3.1 Learning Objectives After completing this section, you will be able to: Explain how HDF5 data can be used to store spatial data and the associated benefits of this format when working with large spatial data cubes. Extract metadata from HDF5 files. Slice or subset HDF5 data. You will extract one band of pixels. Plot a matrix as an image and a raster. Export a final GeoTIFF (spatially projected) that can be used both in further analysis and in common GIS tools like QGIS. 6.3.2 R Libraries to Install: rhdf5: install.packages(\"BiocManager\"), BiocManager::install(\"rhdf5\") raster: install.packages('raster') rgdal: install.packages('rgdal') 6.3.3 Data to Download Download NEON Teaching Data Subset: Imaging Spectrometer Data - HDF5 These hyperspectral remote sensing data provide information on the National Ecological Observatory Networks San Joaquin Exerimental Range field site in March of 2019. The data were collected over the San Joaquin field site located in California (Domain 17) and processed at NEON headquarters. This data subset is derived from the mosaic tile named NEON_D17_SJER_DP3_257000_4112000_reflectance.h5. The entire dataset can be accessed by request from the NEON Data Portal. Download Dataset Remember that the example dataset linked here only has 1 out of every 4 bands included in a full NEON hyperspectral dataset (this substantially reduces the file size!). When we refer to bands in this section, we will note the band numbers for this example dataset, which are different from NEON production data. To convert a band number (b) from this example data subset to the equivalent band in a full NEON hyperspectral file (b), use the following equation: b = 1+4*(b-1). 6.4 About Hyperspectral Remote Sensing Data The electromagnetic spectrum is composed of thousands of bands representing different types of light energy. Imaging spectrometers (instruments that collect hyperspectral data) break the electromagnetic spectrum into groups of bands that support classification of objects by their spectral properties on the Earths surface. Hyperspectral data consists of many bands - up to hundreds of bands - that cover the electromagnetic spectrum. The NEON imaging spectrometer (NIS) collects data within the 380 nm to 2510 nm portions of the electromagnetic spectrum within bands that are approximately 5 nm in width. This results in a hyperspectral data cube that contains approximately 428 bands - which means BIG DATA. Remember that the example dataset used here only has 1 out of every 4 bands included in a full NEON hyperspectral dataset (this substantially reduces size!). When we refer to bands in this tutorial, we will note the band numbers for this example dataset, which may be different from NEON production data. A data cube of NEON hyperspectral data. Each layer in the cube represents a band. The HDF5 data model natively compresses data stored within it (makes it smaller) and supports data slicing (extracting only the portions of the data that you need to work with rather than reading the entire dataset into memory). These features in addition to the ability to support spatial data and associated metadata make it ideal for working with large data cubes such as those generated by imaging spectrometers. In this section we will explore reading and extracting spatial raster data stored within a HDF5 file using R. 6.5 Read HDF5 data into R We will use the raster and rhdf5 packages to read in the HDF5 file that contains hyperspectral data for the NEON San Joaquin (SJER) field site. Lets start by calling the needed packages and reading in our NEON HDF5 file. Please be sure that you have at least version 2.10 of rhdf5 installed. Use: packageVersion(\"rhdf5\") to check the package version. # Load `raster` and `rhdf5` packages and read NIS data into R library(raster) ## Loading required package: sp library(rhdf5) library(rgdal) ## Please note that rgdal will be retired by the end of 2023, ## plan transition to sf/stars/terra functions using GDAL and PROJ ## at your earliest convenience. ## ## rgdal: version: 1.5-32, (SVN revision 1176) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.4.3, released 2022/04/22 ## Path to GDAL shared files: C:/Users/rohan/AppData/Local/R/win-library/4.2/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 7.2.1, January 1st, 2021, [PJ_VERSION: 721] ## Path to PROJ shared files: C:/Users/rohan/AppData/Local/R/win-library/4.2/rgdal/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.5-0 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading sp or rgdal. # Define the file name to be opened f &lt;- (&#39;./data/NEON_hyperspectral_tutorial_example_subset.h5&#39;) Data Tip: To update all packages installed in R, use update.packages(). # look at the HDF5 file structure head(h5ls(f,all=T)) When you look at the structure of the data, take note of the map info dataset, the Coordinate_System group, and the wavelength and Reflectance datasets. The Coordinate_System folder contains the spatial attributes of the data including its EPSG Code, which is easily converted to a Coordinate Reference System (CRS). The CRS documents how the data are physically located on the Earth. The wavelength dataset contains the middle wavelength values for each band in the data. The Reflectance dataset contains the image data that we will use for both data processing and visualization. More Information on raster metadata: Raster Data in R The Basics - this tutorial explains more about how rasters work in R and their associated metadata. About Hyperspectral Remote Sensing Data -this tutorial explains more about metadata and important concepts associated with multi-band (multi and hyperspectral) rasters. Data Tip - HDF5 Structure: Note that the structure of individual HDF5 files may vary depending on who produced the data. In this case, the Wavelength and reflectance data within the file are both datasets. However, the spatial information is contained within a group. Data downloaded from another organization like NASA, may look different. This is why its important to explore the data before diving into using it! We can use the h5readAttributes() function to read and extract metadata from the HDF5 file. Lets start by learning about the wavelengths described within this file. # get information about the wavelengths of this dataset wavelengthInfo &lt;- h5readAttributes(f,&quot;/SJER/Reflectance/Metadata/Spectral_Data/Wavelength&quot;) wavelengthInfo ## $Description ## [1] &quot;Central wavelength of the reflectance bands.&quot; ## ## $Units ## [1] &quot;nanometers&quot; Next, we can use the h5read function to read the data contained within the HDF5 file. Lets read in the wavelengths of the band centers: # read in the wavelength information from the HDF5 file wavelengths &lt;- h5read(f,&quot;/SJER/Reflectance/Metadata/Spectral_Data/Wavelength&quot;) head(wavelengths) ## [1] 381.5437 401.5756 421.6075 441.6394 461.6713 481.7032 tail(wavelengths) ## [1] 2404.764 2424.796 2444.828 2464.860 2484.892 2504.924 Which wavelength is band 6 associated with? (Hint: look at the wavelengths vector that we just imported and check out the data located at index 6 - wavelengths[6]). 482 nanometers falls within the blue portion of the electromagnetic spectrum. Source: National Ecological Observatory Network Band 6 has a associate wavelength center of 481.7032 nanometers (nm) which is in the blue portion of the visible electromagnetic spectrum (~ 400-700 nm). 6.5.1 Bands and Wavelengths A band represents a group of wavelengths. For example, the wavelength values between 695 nm and 700 nm might be one band as captured by an imaging spectrometer. The imaging spectrometer collects reflected light energy in a pixel for light in that band. Often when you work with a multi or hyperspectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 695-700 nm, the center would be 697.5 nm). The full width half max (FWHM) will also be reported. This value represents the spread of the band around that center point. So, a band that covers 800 nm-805 nm might have a FWHM of 5 nm and a wavelength value of 802.5 nm. Bands represent a range of values (types of light) within the electromagnetic spectrum. Values for each band are often represented as the center point value of each band. Source: National Ecological Observatory Network (NEON) The HDF5 dataset that we are working with in this activity may contain more information than we need to work with. For example, we dont necessarily need to process all 107 bands available in this example dataset (or all 426 bands available in a full NEON hyperspectral reflectance file, for that matter) - if we are interested in creating a product like NDVI which only uses bands in the near infra-red and red portions of the spectrum. Or we might only be interested in a spatial subset of the data - perhaps a region where we have plots in the field. The HDF5 format allows us to slice (or subset) the data - quickly extracting the subset that we need to process. Lets extract one of the green bands in our dataset - band 9. By the way - what is the center wavelength value associated with band 9? Hint: wavelengths[9]. How do we know this band is a green band in the visible portion of the spectrum? In order to effectively subset our data, lets first read the important reflectance metadata stored as attributes in the Reflectance_Data dataset. # First, we need to extract the reflectance metadata: reflInfo &lt;- h5readAttributes(f, &quot;/SJER/Reflectance/Reflectance_Data&quot;) reflInfo ## $Cloud_conditions ## [1] &quot;For cloud conditions information see Weather Quality Index dataset.&quot; ## ## $Cloud_type ## [1] &quot;Cloud type may have been selected from multiple flight trajectories.&quot; ## ## $Data_Ignore_Value ## [1] -9999 ## ## $Description ## [1] &quot;Atmospherically corrected reflectance.&quot; ## ## $Dimension_Labels ## [1] &quot;Line, Sample, Wavelength&quot; ## ## $Dimensions ## [1] 500 500 107 ## ## $Interleave ## [1] &quot;BSQ&quot; ## ## $Scale_Factor ## [1] 10000 ## ## $Spatial_Extent_meters ## [1] 257500 258000 4112500 4113000 ## ## $Spatial_Resolution_X_Y ## [1] 1 1 ## ## $Units ## [1] &quot;Unitless.&quot; ## ## $Units_Valid_range ## [1] 0 10000 ## ## $dim ## [1] 107 500 500 # Next, we read the different dimensions nRows &lt;- reflInfo$Dimensions[1] nCols &lt;- reflInfo$Dimensions[2] nBands &lt;- reflInfo$Dimensions[3] nRows ## [1] 500 nCols ## [1] 500 nBands ## [1] 107 The HDF5 read function reads data in the order: Bands, Cols, Rows. This is different from how R reads data. Well adjust for this later. # Extract or &quot;slice&quot; data for band 9 from the HDF5 file b9 &lt;- h5read(f,&quot;/SJER/Reflectance/Reflectance_Data&quot;,index=list(9,1:nCols,1:nRows)) # what type of object is b9? class(b9) ## [1] &quot;array&quot; 6.5.2 A Note About Data Slicing in HDF5 Data slicing allows us to extract and work with subsets of the data rather than reading in the entire dataset into memory. Thus, in this case, we can extract and plot the green band without reading in all 107 bands of information. The ability to slice large datasets makes HDF5 ideal for working with big data. Next, lets convert our data from an array (more than 2 dimensions) to a matrix (just 2 dimensions). We need to have our data in a matrix format to plot it. # convert from array to matrix by selecting only the first band b9 &lt;- b9[1,,] # check it class(b9) ## [1] &quot;matrix&quot; &quot;array&quot; 6.5.3 Arrays vs. Matrices Arrays are matrices with more than 2 dimensions. When we say dimension, we are talking about the z associated with the data (imagine a series of tabs in a spreadsheet). Put the other way: matrices are arrays with only 2 dimensions. Arrays can have any number of dimensions one, two, ten or more. Here is a matrix that is 4 x 3 in size (4 rows and 3 columns): Metric species 1 species 2 total number 23 45 average weight 14 5 average length 2.4 3.5 average height 32 12 6.5.4 Dimensions in Arrays An array contains 1 or more dimensions in the z direction. For example, lets say that we collected the same set of species data for every day in a 30 day month. We might then have a matrix like the one above for each day for a total of 30 days making a 4 x 3 x 30 array (this dataset has more than 2 dimensions). More on R object types here (links to external site, DataCamp). A matrix has only 2 dimensions An array has more than 2 dimensions. Next, lets look at the metadata for the reflectance data. When we do this, take note of 1) the scale factor and 2) the data ignore value. Then we can plot the band 9 data. Plotting spatial data as a visual data check is a good idea to make sure processing is being performed correctly and all is well with the image. # look at the metadata for the reflectance dataset h5readAttributes(f,&quot;/SJER/Reflectance/Reflectance_Data&quot;) ## $Cloud_conditions ## [1] &quot;For cloud conditions information see Weather Quality Index dataset.&quot; ## ## $Cloud_type ## [1] &quot;Cloud type may have been selected from multiple flight trajectories.&quot; ## ## $Data_Ignore_Value ## [1] -9999 ## ## $Description ## [1] &quot;Atmospherically corrected reflectance.&quot; ## ## $Dimension_Labels ## [1] &quot;Line, Sample, Wavelength&quot; ## ## $Dimensions ## [1] 500 500 107 ## ## $Interleave ## [1] &quot;BSQ&quot; ## ## $Scale_Factor ## [1] 10000 ## ## $Spatial_Extent_meters ## [1] 257500 258000 4112500 4113000 ## ## $Spatial_Resolution_X_Y ## [1] 1 1 ## ## $Units ## [1] &quot;Unitless.&quot; ## ## $Units_Valid_range ## [1] 0 10000 ## ## $dim ## [1] 107 500 500 # plot the image image(b9) Figure 6.1: Plot of reflectance values for band 9 data. This plot shows a very washed out image lacking any detail. That is hard to visually interpret. What happens if we plot a log of the data? image(log(b9)) What do you notice about the first image? Its washed out and lacking any detail. What could be causing this? It got better when plotting the log of the values, but still not great. Lets look at the distribution of reflectance values in our data to figure out what is going on. # Plot range of reflectance values as a histogram to view range # and distribution of values. hist(b9,breaks=40,col=&quot;darkmagenta&quot;) Figure 6.2: Histogram of reflectance values for band 9. The x-axis represents the reflectance values and ranges from 0 to 8000. The frequency of these values is on the y-axis. The histogram shows reflectance values are skewed to the right, where the majority of the values lie between 0 and 1000. We can conclude that reflectance values are not equally distributed across the range of reflectance values, resulting in a washed out image. # View values between 0 and 5000 hist(b9,breaks=40,col=&quot;darkmagenta&quot;,xlim = c(0, 5000)) Figure 6.3: Histogram of reflectance values between 0 and 5000 for band 9. Reflectance values are on the x-axis, and the frequency is on the y-axis. The x-axis limit has been set 5000 in order to better visualize the distribution of reflectance values. We can confirm that the majority of the values are indeed within the 0 to 4000 range. # View higher values hist(b9, breaks=40,col=&quot;darkmagenta&quot;,xlim = c(5000, 15000),ylim=c(0,100)) Figure 6.4: Histogram of reflectance values between 5000 and 15000 for band 9. Reflectance values are on the x-axis, and the frequency is on the y-axis. Plot shows that a very few number of pixels have reflectance values larger than 5,000. These values are skewing how the image is being rendered and heavily impacting the way the image is drawn on our monitor. As youre examining the histograms above, keep in mind that reflectance values range between 0-1. The data scale factor in the metadata tells us to divide all reflectance values by 10,000. Thus, a value of 5,000 equates to a reflectance value of 0.50. Storing data as integers (without decimal places) compared to floating points (with decimal places) creates a smaller file. You will see this done often when working with remote sensing data. Notice in the data that there are some larger reflectance values (&gt;5,000) that represent a smaller number of pixels. These pixels are skewing how the image renders. 6.5.5 Data Ignore Value Image data in raster format will often contain a data ignore value and a scale factor. The data ignore value represents pixels where there are no data. Among other causes, no data values may be attributed to the sensor not collecting data in that area of the image or to processing results which yield null values. Remember that the metadata for the Reflectance dataset designated -9999 as data ignore value. Thus, lets set all pixels with a value == -9999 to NA (no value). If we do this, R wont try to render these pixels. # there is a no data value in our raster - let&#39;s define it myNoDataValue &lt;- as.numeric(reflInfo$Data_Ignore_Value) myNoDataValue ## [1] -9999 # set all values equal to -9999 to NA b9[b9 == myNoDataValue] &lt;- NA # plot the image now image(b9) Figure 6.5: Plot of reflectance values for band 9 data with values equal to -9999 set to NA. Image data in raster format will often contain no data values, which may be attributed to the sensor not collecting data in that area of the image or to processing results which yield null values. Reflectance datasets designate -9999 as data ignore values. As such, we will reassign -9999 values to NA so R wont try to render these pixels. 6.5.6 Reflectance Values and Image Stretch Our image still looks dark because R is trying to render all reflectance values between 0 and 14999 as if they were distributed equally in the histogram. However we know they are not distributed equally. There are many more values between 0-5000 then there are values &gt;5000. Images have a distribution of reflectance values. A typical image viewing program will render the values by distributing the entire range of reflectance values across a range of shades that the monitor can render - between 0 and 255. However, often the distribution of reflectance values is not linear. For example, in the case of our data, most of the reflectance values fall between 0 and 0.5. Yet there are a few values &gt;0.8 that are heavily impacting the way the image is drawn on our monitor. Imaging processing programs like ENVI, QGIS and ArcGIS (and even Adobe Photoshop) allow you to adjust the stretch of the image. This is similar to adjusting the contrast and brightness in Photoshop. The proper way to adjust our data would be whats called an image stretch. We will learn how to stretch our image data, later. For now, lets plot the values as the log function on the pixel reflectance values to factor out those larger values. image(log(b9)) Figure 6.6: Plot of log transformed reflectance values for the previous b9 image. Applying the log to the image increases the contrast making it look more like an image by factoring out those larger values. While an improvement, the image is still far from great. The proper way to adjust an image is by doing whats called an image stretch. The log applied to our image increases the contrast making it look more like an image. However, look at the images below. The top one is what our log adjusted image looks like when plotted. The bottom on is an RGB version of the same image. Notice a difference? TOP: The image as it should look. BOTTOM: the image that we outputted from the code above. Notice a difference? 6.5.7 Transpose Image Notice that there are three data dimensions for this file: Bands x Rows x Columns. However, when R reads in the dataset, it reads them as: Columns x Bands x Rows. The data are flipped. We can quickly transpose the data to correct for this using the t or transpose command in R. The orientation is rotated in our log adjusted image. This is because R reads in matrices starting from the upper left hand corner. Whereas, most rasters read pixels starting from the lower left hand corner. In the next section, we will deal with this issue by creating a proper georeferenced (spatially located) raster in R. The raster format will read in pixels following the same methods as other GIS and imaging processing software like QGIS and ENVI do. # We need to transpose x and y values in order for our # final image to plot properly b9 &lt;- t(b9) image(log(b9), main=&quot;Transposed Image&quot;) Figure 6.7: Plot showing the transposed image of the log transformed reflectance values of b9. The orientation of the image is rotated in our log transformed image, because R reads in the matrices starting from the upper left hand corner. 6.6 Create a Georeferenced Raster Next, we will create a proper raster using the b9 matrix. The raster format will allow us to define and manage: Image stretch Coordinate reference system &amp; spatial reference Resolution and other raster attributes It will also account for the orientation issue discussed above. To create a raster in R, we need a few pieces of information, including: The coordinate reference system (CRS) The spatial extent of the image 6.6.1 Define Raster CRS First, we need to define the Coordinate reference system (CRS) of the raster. To do that, we can first grab the EPSG code from the HDF5 attributes, and covert the EPSG to a CRS string. Then we can assign that CRS to the raster object. # Extract the EPSG from the h5 dataset myEPSG &lt;- h5read(f, &quot;/SJER/Reflectance/Metadata/Coordinate_System/EPSG Code&quot;) # convert the EPSG code to a CRS string myCRS &lt;- crs(paste0(&quot;+init=epsg:&quot;,myEPSG)) # define final raster with projection info # note that capitalization will throw errors on a MAC. # if UTM is all caps it might cause an error! b9r &lt;- raster(b9, crs=myCRS) # view the raster attributes b9r ## class : RasterLayer ## dimensions : 500, 500, 250000 (nrow, ncol, ncell) ## resolution : 0.002, 0.002 (x, y) ## extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs ## source : memory ## names : layer ## values : 0, 9210 (min, max) # let&#39;s have a look at our properly oriented raster. Take note of the # coordinates on the x and y axis. image(log(b9r), xlab = &quot;UTM Easting&quot;, ylab = &quot;UTM Northing&quot;, main = &quot;Properly Oriented Raster&quot;) Figure 6.8: Plot of the properly oriented raster image of the band 9 data. In order to orient the image correctly, the coordinate reference system was defined and assigned to the raster object. X-axis represents the UTM Easting values, and the Y-axis represents the Northing values. Next we define the extents of our raster. The extents will be used to calculate the rasters resolution. Fortunately, the spatial extent is provided in the HDF5 file Reflectance_Data group attributes that we saved before as reflInfo. # Grab the UTM coordinates of the spatial extent xMin &lt;- reflInfo$Spatial_Extent_meters[1] xMax &lt;- reflInfo$Spatial_Extent_meters[2] yMin &lt;- reflInfo$Spatial_Extent_meters[3] yMax &lt;- reflInfo$Spatial_Extent_meters[4] # define the extent (left, right, top, bottom) rasExt &lt;- extent(xMin,xMax,yMin,yMax) rasExt ## class : Extent ## xmin : 257500 ## xmax : 258000 ## ymin : 4112500 ## ymax : 4113000 # assign the spatial extent to the raster extent(b9r) &lt;- rasExt # look at raster attributes b9r ## class : RasterLayer ## dimensions : 500, 500, 250000 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : 257500, 258000, 4112500, 4113000 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs ## source : memory ## names : layer ## values : 0, 9210 (min, max) The extent of a raster represents the spatial location of each corner. The coordinate units will be determined by the spatial projection/ coordinate reference system that the data are in. Source: National Ecological Observatory Network (NEON) Learn more about raster attributes including extent, and coordinate reference systems here. We can adjust the colors of our raster too if we want. # let&#39;s change the colors of our raster and adjust the zlims col &lt;- terrain.colors(25) image(b9r, xlab = &quot;UTM Easting&quot;, ylab = &quot;UTM Northing&quot;, main= &quot;Raster w Custom Colors&quot;, col=col, zlim=c(0,3000)) Figure 6.9: Plot of the properly oriented raster image of B9 with custom colors. We can adjust the colors of the image by adjusting the z limits, which in this case makes the highly reflective surfaces more vibrant. This color adjustment is more apparent in the bottom left of the image, where the parking lot, buildings and bare surfaces are located. X-axis represents the UTM Easting values, and the Y-axis represents the Northing values. Weve now created a raster from band 9 reflectance data. We can export the data as a raster, using the writeRaster command. # write out the raster as a geotiff writeRaster(b9r, file=paste0(wd,&quot;band9.tif&quot;), format=&quot;GTiff&quot;, overwrite=TRUE) # It&#39;s always good practice to close the H5 connection before moving on! # close the H5 file H5close() 6.6.2 Challenge: Work with Rasters Try these three extensions on your own: Create rasters using other bands in the dataset. Vary the distribution of values in the image to mimic an image stretch. e.g. b9[b9 &gt; 6000 ] &lt;- 6000 Use what you know to extract ALL of the reflectance values for ONE pixel rather than for an entire band. HINT: this will require you to pick an x and y value and then all values in the z dimension: aPixel&lt;- h5read(f,\"Reflectance\",index=list(NULL,100,35)). Plot the spectra output. 6.7 Light Detection And Ranging (LiDAR) Data A 6:02 introduction video on LiDAR: A ~17 minute video lecture by Dr. Tristan Goulden on discrete LiDAR and LiDAT data: A ~35 minute video on LiDAR Uncertainty: 6.8 Calculating Forest Structural Diversity Metrics from NEON LiDAR Data Contributors: Jeff Atkins, Keith Krause, Atticus Stovall Authors: Elizabeth LaRue, Donal OLeary 6.8.1 Learning Objectives After completing this tutorial, you will be able to: Read a NEON LiDAR file (laz) into R Visualize a spatial subset of the LiDAR tile Correct a spatial subset of the LiDAR tile for topographic varation Calculate 13 structural diversity metrics described in LaRue, Wagner, et al. (2020) 6.8.2 R Libraries to Install: lidR: install.packages('lidR') gstat: install.packages('gstat') Important Note: If you have R version 3.6 or above youll need to update data.table: data.table::update.dev.pkg() 6.8.3 Data to Download For this tutorial, we will be using two .laz files containing NEON AOP point clouds for 1km tiles from the Harvard Forest (HARV) and Lower Teakettle (TEAK) sites. Link to download .laz files on Google Drive Here. 6.8.4 Recommended Skills For this tutorial, you should have an understanding of Light Detection And Ranging (LiDAR) technology, specifically how discrete return lidar data are collected and represented in las/laz files. For more information on how lidar works, please see NEONs Introduction to Lidar Tutorial Series. 6.8.5 Additional Resources Jean-Romain Roussel, Tristan R.H. Goodbody, Piotr Tompalski have written a fantastic gitbook on their lidR package which can be found here. If loading some of the packages needed for this tutorial results in non-stop errors, I strongly suggest you run this section using a Docker container,(e.g. Rocker Projects Geospatial image), or one from CyVerse: https://hub.docker.com/r/cyversevice/rstudio-geospatial 6.9 Introduction to Structural Diversity Metrics Forest structure influences many important ecological processes, including: biogeochemical cycling, wildfires, species richness and diversity, and many others. Quantifying forest structure, hereafter referred to as structural diversity, presents a challenge for many reasons, including difficulty in measuring individual trees, limbs, and leaves across large areas. In order to overcome this challenge, todays researchers use Light Detection And Ranging (LiDAR) technology to measure large areas of forest. It is also challenging to calculate meaningful summary metrics of a forests structure that 1) are ecologically relevant and 2) can be used to compare different forested locations. In this tutorial, you will be introduced to a few tools that will help you to explore and quantify forest structure using LiDAR data collected at two field sites of the National Ecological Observatory Network. 6.10 NEON AOP Discrete Return LIDAR The NEON Airborne Observation Platform (AOP) . has several sensors including discrete-return LiDAR, which is useful for measuring forest structural diversity that can be summarized into four categories of metrics: (1) canopy height, (2) canopy cover and openness, and (3) canopy heterogeneity (internal and external), and (4) vegetation area. We will be comparing the structural diversity of two NEON sites that vary in their structural characteristics. First, we will look at Harvard Forest (HARV), which is located in Massachusetts. It is a lower elevation, mixed deciduous and evergreen forest dominated by Quercus rubra, Acer rubrum, and Aralia nudicaulis. Second, we will look at Lower Teakettle (TEAK), which is a high elevation forested NEON site in California. TEAK is an evergreen forest dominated by Abies magnifica, Abies concolor, Pinus jeffreyi, and Pinus contorta. As you can imagine, these two forest types will have both similarities and differences in their structural attributes. We can quantify these attributes by calculating several different structural diversity metrics, and comparing the results. 6.10.1 Loading the LIDAR Products To begin, we first need to load our required R packages, and set our working directory to the location where we saved the input LiDAR .laz files that can be downloaded from the NEON Data Portal. library(lidR) ## ## Attaching package: &#39;lidR&#39; ## The following objects are masked from &#39;package:raster&#39;: ## ## projection, projection&lt;- library(gstat) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## shift library(kableExtra) Next, we will read in the LiDAR data using the lidR::readLAS() function. Note that this function can read in both .las and .laz file formats. # Read in LiDAR data #2017 1 km2 tile .laz file type for HARV and TEAK #Watch out for outlier Z points - this function also allows for the #ability to filter outlier points well above or below the landscape #(-drop_z_blow and -drop_z_above). See how we have done this here #for you. HARV &lt;- lidR::readLAS(&#39;/Users/rohan/katharynduffy.github.io/data/NEON_D01_HARV_DP1_727000_4702000_classified_point_cloud_colorized.laz&#39;,filter = &quot;-drop_z_below 150 -drop_z_above 325&quot;) TEAK &lt;- lidR::readLAS(&#39;/Users/rohan/katharynduffy.github.io/data/NEON_D17_TEAK_DP1_316000_4091000_classified_point_cloud_colorized.laz&#39;,filter = &quot;-drop_z_below 1694 -drop_z_above 2500&quot;) Lets check out: the extent, coordinate system, and a 3D plot of each .laz file. Note that on Mac computers you may need to install XQuartz for 3D plots - see xquartz.org summary(HARV) lidR::plot(HARV) class : LAS (v1.3 format 3) memory : 521.6 Mb extent : 727000, 728000, 4702000, 4703000 (xmin, xmax, ymin, ymax) coord. ref. : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 area : 1 km² points : 5.94 million points density : 5.94 points/m² File signature: LASF File source ID: 211 Global encoding: - GPS Time Type: GPS Week Time - Synthetic Return Numbers: no - Well Know Text: CRS is GeoTIFF - Aggregate Model: false Project ID - GUID: 00000000-0000-0000-0000-000000000000 Version: 1.3 System identifier: LAStools (c) by rapidlasso GmbH Generating software: lascolor (190812) commercial File creation d/y: 278/2019 header size: 235 Offset to point data: 329 Num. var. length record: 1 Point data format: 3 Point data record length: 34 Num. of point records: 5944282 Num. of points by return: 4581566 1255117 104991 2608 0 Scale factor X Y Z: 0.01 0.01 0.01 Offset X Y Z: 7e+05 4700000 0 min X Y Z: 727000 4702000 223.2 max X Y Z: 728000 4703000 324.99 Variable length records: Variable length record 1 of 1 Description: Tags: Key 1024 value 1 Key 1025 value 2 Key 3072 value 32618 Key 4099 value 9001 1 km-squared point cloud from Harvard Forest showing a gentle slope covered in a continuous canopy of mixed forest. summary(TEAK) lidR::plot(TEAK) class : LAS (v1.3 format 3) memory : 439.2 Mb extent : 316000, 317000, 4091231, 4092000 (xmin, xmax, ymin, ymax) coord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 area : 0.75 km² points : 5.01 million points density : 6.7 points/m² File signature: LASF File source ID: 211 Global encoding: - GPS Time Type: GPS Week Time - Synthetic Return Numbers: no - Well Know Text: CRS is GeoTIFF - Aggregate Model: false Project ID - GUID: 00000000-0000-0000-0000-000000000000 Version: 1.3 System identifier: LAStools (c) by rapidlasso GmbH Generating software: lascolor (190812) commercial File creation d/y: 206/2019 header size: 235 Offset to point data: 329 Num. var. length record: 1 Point data format: 3 Point data record length: 34 Num. of point records: 5006002 Num. of points by return: 3315380 1182992 416154 91476 0 Scale factor X Y Z: 0.01 0.01 0.01 Offset X Y Z: 3e+05 4e+06 0 min X Y Z: 316000 4091231 2117.03 max X Y Z: 317000 4092000 2376.98 Variable length records: Variable length record 1 of 1 Description: Tags: Key 1024 value 1 Key 1025 value 2 Key 3072 value 32611 Key 4099 value 9001 1 km-squared point cloud from Lower Teakettle showing mountainous terrain covered in a patchy conifer forest, with tall, skinny conifers clearly visible emerging from the discontinuous canopy. 6.10.2 Normalizing Tree Height to Ground To begin, we will take a look at the structural diversity of the dense mixed deciduous/evergreen forest of HARV. Were going to choose a 40 x 40 m spatial extent for our analysis, but first we need to normalize the height values of this LiDAR point cloud from an absolute elevation above mean sea level to height above the ground using the lidR::normalize_height() function. This function relies on spatial interpolation, and therefore we want to perform this step on an area that is quite a bit larger than our area of interest to avoid edge effects. To be safe, we will clip out an area of 200 x 200 m, normalize it, and then clip out our smaller area of interest. # Correct for elevation #We&#39;re going to choose a 40 x 40 m spatial extent, which is the #extent for NEON base plots. #First set the center of where you want the plot to be (note easting #and northing works in units of m because these data are in a UTM #proejction as shown in the summary above). x &lt;- 727500 #easting y &lt;- 4702500 #northing #Cut out a 200 x 200 m buffer by adding 100 m to easting and #northing coordinates (x,y). data.200m &lt;- clip_rectangle(HARV, xleft = (x - 100), ybottom = (y - 100), xright = (x + 100), ytop = (y + 100)) #Correct for ground height using a kriging function to interpolate #elevation from ground points in the .laz file. #If the function will not run, then you may need to checkfor outliers #by adjusting the &#39;drop_z_&#39; arguments when reading in the .laz files. dtm &lt;- grid_terrain(data.200m, 1, kriging(k = 10L)) data.200m &lt;- normalize_height(data.200m, dtm) #Will often give a warning if not all points could be corrected, #but visually check to see if it corrected for ground height. lidR::plot(data.200m) #There&#39;s only a few uncorrected points and we&#39;ll fix these in #the next step. #Clip 20 m out from each side of the easting and northing #coordinates (x,y). data.40m &lt;- clip_rectangle(data.200m, xleft = (x - 20), ybottom = (y - 20), xright = (x + 20), ytop = (y + 20)) data.40m@data$Z[data.40m@data$Z &lt;= .5] &lt;- NA #This line filters out all z_vals below .5 m as we are less #interested in shrubs/trees. #You could change it to zero or another height depending on interests. #visualize the clipped plot point cloud lidR::plot(data.40m) 40 meter by 40 meter point cloud from Harvard Forest showing a cross-section of the forest structure with a complex canopy- and sub-canopy structure with many rounded crowns, characteristic of a deciduous-dominated section of forest. 6.11 Calculating Structural Diversity Metrics Now that we have our area of interest normalized and clipped, we can proceed with calculating our structural diversity metrics. 6.11.1 GENERATE CANOPY HEIGHT MODEL (CHM) (i.e. a 1 m2 raster grid of vegetations heights) # Structural diversity metrics #res argument specifies pixel size in meters and dsmtin is #for raster interpolation chm &lt;- grid_canopy(data.40m, res = 1, dsmtin()) #visualize CHM lidR::plot(chm) 6.11.1.1 MEAN OUTER CANOPY HEIGHT (MOCH) #calculate MOCH, the mean CHM height value mean.max.canopy.ht &lt;- mean(chm@data@values, na.rm = TRUE) 6.11.1.2 MAX CANOPY HEIGHT #calculate HMAX, the maximum CHM height value max.canopy.ht &lt;- max(chm@data@values, na.rm=TRUE) 6.11.1.3 RUMPLE #calculate rumple, a ratio of outer canopy surface area to #ground surface area (1600 m^2) rumple &lt;- rumple_index(chm) 6.11.1.4 TOP RUGOSITY #top rugosity, the standard deviation of pixel values in chm and #is a measure of outer canopy roughness top.rugosity &lt;- sd(chm@data@values, na.rm = TRUE) 6.11.1.5 DEEP GAPS &amp; DEEP GAP FRACTION #number of cells in raster (also area in m2) cells &lt;- length(chm@data@values) chm.0 &lt;- chm chm.0[is.na(chm.0)] &lt;- 0 #replace NAs with zeros in CHM #create variable for the number of deep gaps, 1 m^2 canopy gaps zeros &lt;- which(chm.0@data@values == 0) deepgaps &lt;- length(zeros) #number of deep gaps #deep gap fraction, the number of deep gaps in the chm relative #to total number of chm pixels deepgap.fraction &lt;- deepgaps/cells 6.11.1.6 COVER FRACTION #cover fraction, the inverse of deep gap fraction cover.fraction &lt;- 1 - deepgap.fraction 6.11.1.7 HEIGHT SD The standard deviation of height values for all points in the plot point cloud #height SD, the standard deviation of height values for all points #in the plot point cloud vert.sd &lt;- cloud_metrics(data.40m, sd(Z, na.rm = TRUE)) #SD of VERTICAL SD of HEIGHT #rasterize plot point cloud and calculate the standard deviation #of height values at a resolution of 1 m^2 sd.1m2 &lt;- grid_metrics(data.40m, sd(Z), 1) #standard deviation of the calculated standard deviations #from previous line #This is a measure of internal and external canopy complexity sd.sd &lt;- sd(sd.1m2[,3], na.rm = TRUE) #some of the next few functions won&#39;t handle NAs, so we need #to filter these out of a vector of Z points Zs &lt;- data.40m@data$Z Zs &lt;- Zs[!is.na(Zs)] 6.11.1.8 ENTROPY Quantifies diversity &amp; evenness of point cloud heights #by = 1 partitions point cloud in 1 m tall horizontal slices #ranges from 0-1, with 1 being more evenly distributed points #across the 1 m tall slices entro &lt;- entropy(Zs, by = 1) 6.11.1.9 GAP FRACTION PROFILE Gap fraction profile, assesses the distribution of gaps in the canopy volume #dz = 1 partitions point cloud in 1 m horizontal slices #z0 is set to a reasonable height based on the age and height of #the study sites gap_frac &lt;- gap_fraction_profile(Zs, dz = 1, z0=3) #defines gap fraction profile as the average gap fraction in each #1 m horizontal slice assessed in the previous line GFP.AOP &lt;- mean(gap_frac$gf) 6.11.1.10 VAI Leaf area density, assesses leaf area in the canopy volume #k = 0.5 is a standard extinction coefficient for foliage #dz = 1 partitions point cloud in 1 m horizontal slices #z0 is set to the same height as gap fraction profile above LADen&lt;-LAD(Zs, dz = 1, k=0.5, z0=3) #vegetation area index, sum of leaf area density values for #all horizontal slices assessed in previous line VAI.AOP &lt;- sum(LADen$lad, na.rm=TRUE) 6.11.1.11 VCI A vertical complexity index, fixed normalization of entropy metric calculated above #set zmax comofortably above maximum canopy height #by = 1 assesses the metric based on 1 m horizontal slices in #the canopy VCI.AOP &lt;- VCI(Zs, by = 1, zmax=100) We now have 13 different structural diversity metrics. Lets organize them into a new dataframe: #OUTPUT CALCULATED METRICS INTO A TABLE #creates a dataframe row, out.plot, containing plot descriptors #and calculated metrics HARV_structural_diversity &lt;- data.frame(matrix(c(x, y, mean.max.canopy.ht, max.canopy.ht, rumple, deepgaps,deepgap.fraction, cover.fraction,top.rugosity, vert.sd, sd.sd, entro,GFP.AOP, VAI.AOP, VCI.AOP), ncol = 15)) #provide descriptive names for the calculated metrics colnames(HARV_structural_diversity) &lt;- c(&quot;easting&quot;, &quot;northing&quot;, &quot;mean.max.canopy.ht.aop&quot;, &quot;max.canopy.ht.aop&quot;, &quot;rumple.aop&quot;, &quot;deepgaps.aop&quot;, &quot;deepgap.fraction.aop&quot;,&quot;cover.fraction.aop&quot;, &quot;top.rugosity.aop&quot;, &quot;vert.sd.aop&quot;, &quot;sd.sd.aop&quot;, &quot;entropy.aop&quot;, &quot;GFP.AOP.aop&quot;, &quot;VAI.AOP.aop&quot;, &quot;VCI.AOP.aop&quot;) #View the results HARV_structural_diversity 6.11.2 Combining Everything Into One Function Now that we have run through how to measure each structural diversity metric, lets create a convenient function to run these a little faster on the TEAK site for a comparison of structural diversity with HARV. #Let&#39;s correct for elevation and measure structural diversity for TEAK x &lt;- 316400 y &lt;- 4091700 data.200m &lt;- clip_rectangle(TEAK, xleft = (x - 100), ybottom = (y - 100), xright = (x + 100), ytop = (y + 100)) dtm &lt;- grid_terrain(data.200m, 1, kriging(k = 10L)) data.200m &lt;- normalize_height(data.200m, dtm) data.40m &lt;- clip_rectangle(data.200m, xleft = (x - 20), ybottom = (y - 20), xright = (x + 20), ytop = (y + 20)) data.40m@data$Z[data.40m@data$Z &lt;= .5] &lt;- 0 plot(data.40m) 40 meter by 40 meter point cloud from Lower Teakettle showing a cross-section of the forest structure with several tall, pointed conifers separated by deep gaps in the canopy. #Zip up all the code we previously used and write function to #run all 13 metrics in a single function. structural_diversity_metrics &lt;- function(data.40m) { chm &lt;- grid_canopy(data.40m, res = 1, dsmtin()) mean.max.canopy.ht &lt;- mean(chm@data@values, na.rm = TRUE) max.canopy.ht &lt;- max(chm@data@values, na.rm=TRUE) rumple &lt;- rumple_index(chm) top.rugosity &lt;- sd(chm@data@values, na.rm = TRUE) cells &lt;- length(chm@data@values) chm.0 &lt;- chm chm.0[is.na(chm.0)] &lt;- 0 zeros &lt;- which(chm.0@data@values == 0) deepgaps &lt;- length(zeros) deepgap.fraction &lt;- deepgaps/cells cover.fraction &lt;- 1 - deepgap.fraction vert.sd &lt;- cloud_metrics(data.40m, sd(Z, na.rm = TRUE)) sd.1m2 &lt;- grid_metrics(data.40m, sd(Z), 1) sd.sd &lt;- sd(sd.1m2[,3], na.rm = TRUE) Zs &lt;- data.40m@data$Z Zs &lt;- Zs[!is.na(Zs)] entro &lt;- entropy(Zs, by = 1) gap_frac &lt;- gap_fraction_profile(Zs, dz = 1, z0=3) GFP.AOP &lt;- mean(gap_frac$gf) LADen&lt;-LAD(Zs, dz = 1, k=0.5, z0=3) VAI.AOP &lt;- sum(LADen$lad, na.rm=TRUE) VCI.AOP &lt;- VCI(Zs, by = 1, zmax=100) out.plot &lt;- data.frame( matrix(c(x, y, mean.max.canopy.ht,max.canopy.ht, rumple,deepgaps, deepgap.fraction, cover.fraction, top.rugosity, vert.sd, sd.sd, entro, GFP.AOP, VAI.AOP,VCI.AOP), ncol = 15)) colnames(out.plot) &lt;- c(&quot;easting&quot;, &quot;northing&quot;, &quot;mean.max.canopy.ht.aop&quot;, &quot;max.canopy.ht.aop&quot;, &quot;rumple.aop&quot;, &quot;deepgaps.aop&quot;, &quot;deepgap.fraction.aop&quot;, &quot;cover.fraction.aop&quot;, &quot;top.rugosity.aop&quot;,&quot;vert.sd.aop&quot;,&quot;sd.sd.aop&quot;, &quot;entropy.aop&quot;, &quot;GFP.AOP.aop&quot;, &quot;VAI.AOP.aop&quot;, &quot;VCI.AOP.aop&quot;) print(out.plot) } TEAK_structural_diversity &lt;- structural_diversity_metrics(data.40m) 6.11.3 Comparing Metrics Between Forests How does the structural diversity of the evergreen TEAK forest compare to the mixed deciduous/evergreen forest from HARV? Lets combine the result data.frames for a direct comparison: combined_results=rbind(HARV_structural_diversity, TEAK_structural_diversity) # Add row names for clarity row.names(combined_results)=c(&quot;HARV&quot;,&quot;TEAK&quot;) # Take a look to compare combined_results 6.12 Matching GEDI waveforms with NEON AOP LiDAR pointclouds author: Donal OLeary GEDI has amazing coverage around the globe, but is limited in its spatial resolution. Here, we extract NEON pointcloud data corresponding to individual GEDI waveforms to better understand how the GEDI return waveform gets its shape. 6.12.1 Learning Objectives After completing this section you will be able to: Search for GEDI data based on a NEON site bounding box Extract NEON LiDAR pointcloud data corresponding to a specific GEDI footprint Visualize NEON and GEDI LiDAR data together in 3D 6.12.2 Things Youll Need To Complete This GEDI Section 6.12.3 R Packages to Install Prior to starting the tutorial ensure that the following packages are installed. raster: install.packages(\"raster\") rGEDI: install.packages(\"rGEDI\") sp: install.packages(\"sp\") sf: install.packages(\"sf\") lidR: install.packages(\"lidR\") neonUtilities: install.packages(\"neonUtilities\") viridis: install.packages(\"viridis\") maptools: install.packages(\"maptools\") 6.12.4 Example Data Set 6.12.4.1 GEDI Example Data Subset This dataset has been subset from a full GEDI orbit retaining only the shots that correspond to a single 1km AOP tile, and only the datasets (attributes) that are needed to visualize the GEDI waveform as shown below. Download GEDI Example Dataset 6.12.4.2 Datum difference between WGS84 and NAD83 This dataset describes the differences between the common WGS84 and NAD83 standards for vertical data in North America. Download Dataset 6.12.4.3 Datum difference between GEOID12A and NAD83 This dataset describes the differences between the GEOID12A and NAD83 standards for vertical data in North America. Download Dataset 6.12.5 Getting Started In this section we will compare NEON and GEDI LiDAR data by comparing the information that they both capture in the same location. NEON data are actually one of the datasets used by the GEDI mission to calibrate and validate GEDI waveforms, so this makes for a valuable comparison! In order to compare these data, we will need to download GEDI data that overlap a NEON site. Fortunately, Carlos Silva et al. have made a convenient R package clled rGEDI and this excellent tutorial hosted on CRAN desribing how to work with GEDI data in R. However, GEDI data are currently only available to download per complete orbit, which means that the vast majority of the orbits data does not fall within a NEON site. The GEDI orbit datasets come in HDF5 data format, and contain about 7Gb of data, so you may want to run the first few sections of this tutorial to get the GEDI download started, and refresh your memory of HDF5 files with NEONs Intro to HDF5 series. First, we will load the required libraries and set our working directory: # devtools::install_git(&quot;https://github.com/carlos-alberto-silva/rGEDI&quot;, dependencies = TRUE) # library(rGEDI) library(raster) library(sp) library(sf) ## Linking to GEOS 3.9.1, GDAL 3.4.3, PROJ 7.2.1; sf_use_s2() is TRUE library(rgl) library(lidR) library(neonUtilities) library(viridis) ## Loading required package: viridisLite library(maptools) ## Checking rgeos availability: TRUE ## Please note that &#39;maptools&#39; will be retired by the end of 2023, ## plan transition at your earliest convenience; ## some functionality will be moved to &#39;sp&#39;. #wd &lt;- &quot;./data&quot; # This will depend on your local environment #setwd(wd) Next, we will download a Canopy Height Model (CHM) tile from the Wind River Experiemental Forest (WREF) using the byTileAOP() function to use as a preliminary map on which to overlay the GEDI data. There is one particularly interesting mosaic tile which we will select using the easting and northing arguments. We then load the raster into R and make a simple plot. For more information about CHMs, please see our tutorial What is a CHM? # Define the SITECODE as a variable because # we will use it several times in this tutorial SITECODE &lt;- &quot;WREF&quot; byTileAOP(dpID = &quot;DP3.30015.001&quot;, site = SITECODE, year = 2017, easting = 580000, northing = 5075000, check.size = F, savepath = &#39;./data&#39;) ## Downloading files totaling approximately 4.035953 MB ## Downloading 6 files ## | | | 0% | |============== | 20% | |============================ | 40% | |========================================== | 60% | |======================================================== | 80% | |======================================================================| 100% ## Successfully downloaded 6 files. ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shp downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.kml downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/kmls ## NEON_D16_WREF_DP3_580000_5075000_CHM.tif downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.prj downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.dbf downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shx downloaded to ./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps chm &lt;- raster(&#39;./data/DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif/NEON_D16_WREF_DP3_580000_5075000_CHM.tif&#39;) plot(chm) As you can see, this particular CHM is showing a conspicuous, triangle-shaped clearcut in this section of the experimental forest, where the tree canopy is much shorter than the towering 60m+ trees in the undisturbed areas. This variation will give us a variety of forest structures to investigate. 6.12.6 Downloading GEDI data This next section on downloading and working with GEDI data is loosely based on the excellent rGEDI package tutorial posted on CRAN here. If you would prefer to avoid a large download (&gt;7Gb) of a full GEDI orbit, you can forego these steps and use the example GEDI dataset here, and skip to the readLevel1B() function below. The Global Ecosystem Dynamics Investigation (GEDI) is a NASA mission with the primary data collection being performed by a novel waveform lidar instrument mounted on the International Space Station (ISS). Please see this open-access paper published in Science of Remote Sensing that describes this mission in detail. The ISS orbits around the world every 90 minutes, and can be tracked on this cool NASA website. As described here on the Land Processes Distributed Active Archive Center (LP DAAC), the sole GEDI observable is the waveform from which all other data products are derived. Each waveform is captured with a nominal ~30 m diameter. As of the date of publication, GEDI data are only offered in HDF5 format, with each file containing the data for a full orbit. The LP DAAC has developed a tool that allows researchers to input a bounding box, which will return a list of every orbit that has a shot (waveform return) that falls within that box. Unfortunately, at this time, the tool will not subset out the specific shots that fall within that bounding box; you must download the entire orbit (~7Gb). This functionality may be improved in the future. Our next few steps involve defining our bounding box, requesting the list of GEDI orbits that contain data relevant to that bounding box, and downloading those data. Lets focus on the extent of our CHM that we downloaded above - but we will first need to re-project the CHM from its UTM projection into WGS84. To do so, we will refer to the EPSG code for WGS84. To look up any of these codes, please see the amazing resource spatialrerference.org. # Project CHM to WGS84 chm_WGS = projectRaster(chm, crs=CRS(&quot;+init=epsg:4326&quot;)) # Study area boundary box coordinates ul_lat&lt;- extent(chm_WGS)[4] lr_lat&lt;- extent(chm_WGS)[3] ul_lon&lt;- extent(chm_WGS)[1] lr_lon&lt;- extent(chm_WGS)[2] Next, we use that bounding box information as an input to the gedifinder() funciton. # Specifying the optional date range, if desired daterange=c(&quot;2019-03-25&quot;, #first date of GEDI availability &quot;2020-07-15&quot;) # Get path to GEDI data # These lines use an API request to determine which orbits are available # that intersect the bounding box of interest. # Note that you still must download the entire orbit and then subset to # your area of interest! gLevel1B &lt;- gedifinder(product=&quot;GEDI01_B&quot;, ul_lat, ul_lon, lr_lat,lr_lon, version=&quot;001&quot;,daterange=NULL) # # # View list of available data gLevel1B Great! There are several GEDI orbits available that have at least 1 shot within our bounding box of interest. For more information about GEDI filename conventions, and other valuable information about GEDI data, see this page on the LP DAAC. However, as mentioned before, each of these files are quite large (~7Gb), so lets focus on just the first one for now. # Downloading GEDI data, if you haven&#39;t already # Note that this will download a large file (&gt;7Gb) which # can be avoided by using the example dataset provided above. # wd=&#39;./data/&#39; # if(!file.exists(paste0(&quot;./data/&quot;,basename(gLevel1B[2])))){ # gediDownload(filepath=gLevel1B[2],outdir=&quot;./data/&quot;) # } Next, we use the rGEDI package to read in the GEDI data. First, we need to make a gedi.level1b object using the readLevel1B() function. Next, we extract the geographic position of the center of each shot from the gedi.level1b object using the getLevel1BGeo() function, and display the head of the resulting table. From this point on, we will be using the (much smaller) example subset of GEDI data linked at the top of this section. All functions and processes should also work with a full GEDI dataset downloaded above, if you choose to pursue this research on your own! gedilevel1b&lt;-readLevel1B(level1Bpath = file.path(&#39;./data/NEON_WREF_GEDI_subset.h5&#39;)) #gedilevel1b&lt;-readLevel1B(level1Bpath = file.path(wd, &quot;GEDI01_B_2019206022612_O03482_T00370_02_003_01.h5&quot;)) level1bGeo&lt;-getLevel1BGeo(level1b=gedilevel1b,select=c(&quot;elevation_bin0&quot;)) head(level1bGeo) 6.12.7 Plot GEDI footprints on CHM Lets visualize where the GEDI footprints are located on the CHM tile. To do so, we will need to first convert the GEDI data into a spatial object. For this example, we will use a spatial features object type from the sp package: # drop any shots with missing latitude/longitude values level1bGeo = level1bGeo[!is.na(level1bGeo$latitude_bin0)&amp; !is.na(level1bGeo$longitude_bin0),] # Convert the GEDI data.frame into an &#39;sf&#39; object level1bGeo_spdf&lt;-st_as_sf(level1bGeo, coords = c(&quot;longitude_bin0&quot;, &quot;latitude_bin0&quot;), crs=CRS(&quot;+init=epsg:4326&quot;)) # crop to the CHM that is in WGS84 level1bgeo_WREF=st_crop(level1bGeo_spdf, chm_WGS) Next, project the GEDI geospatial data into the UTM zone that the CHM is within (Zone 10 North). These data come as the point location at the center of the GEDI footprint, so we next convert the GEDI footprint center (lat/long) into a circle using the buffer() function. Finally, we can plot the CHM and overlay the GEDI footprint circles, and label with the last three digits of the shot number. # project to UTM level1bgeo_WREF_UTM=st_transform( level1bgeo_WREF, crs=chm$NEON_D16_WREF_DP3_580000_5075000_CHM@crs) # buffer the GEDI shot center by a radius of 12.5m # to represent the full 25m diameter GEDI footprint level1bgeo_WREF_UTM_buffer=st_buffer(level1bgeo_WREF_UTM, dist=12.5) # plot CHM and overlay GEDI data plot(chm) plot(level1bgeo_WREF_UTM_buffer, add=T, col=&quot;transparent&quot;) # add labes with the last three digits of the GEDI shot_number pointLabel(st_coordinates(level1bgeo_WREF_UTM), labels=level1bgeo_WREF_UTM$shot_number, cex=1) 6.12.8 Extract Waveform for a single Shot Lets take a look at a waveform for a single GEDI shot. We can select a shot by using its shot_number as shown below. Note, however, that the example data subset shots have been re-numbered, and those numbers will not correspond with full orbit GEDI data. # Extracting GEDI full-waveform for a given shot_number wf &lt;- getLevel1BWF(gedilevel1b,shot_number = 20) # or, if using a full GEDI dataset, # wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF_UTM_buffer$shot_number[which(level1bgeo_WREF_UTM_buffer$shot_number==1786)]) # Save current plotting parameters to revert to oldpar &lt;- par() # Set up plotting paramters par(mfrow = c(1,2), mar=c(4,4,1,1), cex.axis = 1.5) # Plot filled-in waveform plot(wf, relative=FALSE, polygon=TRUE, type=&quot;l&quot;, lwd=2, col=&quot;forestgreen&quot;, xlab=&quot;Waveform Amplitude&quot;, ylab=&quot;Elevation (m)&quot;) grid() #add a grid to the plot # Plot a simple line with no fill plot(wf, relative=TRUE, polygon=FALSE, type=&quot;l&quot;, lwd=2, col=&quot;forestgreen&quot;, xlab=&quot;Waveform Amplitude (%)&quot;, ylab=&quot;Elevation (m)&quot;) grid()#add a grid to the plot # Revert plotting parameters to previous values. par(oldpar) This waveform shows some noise above and below the main ecosystem return, with a fairly dense canopy around 370m elevation, and a characteristic ground return spike at about 340m. While the GEDI data are extremely valuable and offer a near-global coverage, it is hard to get a good sense of what the ecosystem really looks like from this GEDI waveform. Lets download some NEON AOP pointcloud data to pair up with this waveform to get a better sense of what GEDI is reporting. 6.12.9 Download and Plot NEON AOP LiDAR pointcloud data Here, we will use the byTileAOP() function from the neonUtilities package to download the classified pointcloud mosaic data (DP1.30003.001). Since this is a mosaic tile like the CHM, we can just pass this function the lower left corner of the CHM tile to get the corresponding lidar pointcloud mosaic tile for our analysis. # Download the pointcloud data if you don&#39;t have it already if (!file.exists(&#39;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&#39;)){ byTileAOP(dpID = &quot;DP1.30003.001&quot;, site = SITECODE, year = 2017, easting = extent(chm)[1], northing=extent(chm)[3], check.size = F, savepath = &#39;./data/&#39;) # Edit savepath as needed } After downloading the point cloud data, lets read them into our R session using the readLAS() function from the lidaR package, and plot them in 3D. Note, you may need to update your XQuartz if you are using a Mac. WREF_LAS=readLAS(&quot;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&quot;) lidR::plot(WREF_LAS) Oh, yikes! There are a lot of outliers above the actual forest, and a few below. Lets use some simple statistics to throw out those outliers. We will first calculate the mean and standard deviation for the verical axis, and then use the filter options of the readLAS() function to eliminate the vertical outliers. # ### remove outlier lidar point outliers using mean and sd statistics Z_sd=sd(WREF_LAS@data$Z) Z_mean=mean(WREF_LAS@data$Z) # make filter string in form filter = &quot;-drop_z_below 50 -drop_z_above 1000&quot; # You can increase or decrease (from 4) the number of sd&#39;s to filter outliers f = paste(&quot;-drop_z_below&quot;,(Z_mean-4*Z_sd),&quot;-drop_z_above&quot;,(Z_mean+4*Z_sd)) # Read in LAS file, trimming off vertical outlier points WREF_LAS=readLAS(&quot;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&quot;, filter = f) #Plot the full LiDAR point cloud mosaic tile (1km^2) plot(WREF_LAS) Ahhh, thats better. 6.12.10 Clip AOP LiDAR Pointcloud to GEDI footprints We can now use the GEDI footprint circles (that we made using the buffer() function) to clip out the NEON AOP LiDAR points that correspond with the GEDI footprints: # Clip the pointcloud by the GEDI footprints created by buffering above. # This makes a &#39;list&#39; of LAS objects WREF_GEDI_footprints=lasclip(WREF_LAS, geometry = level1bgeo_WREF_UTM_buffer) # we can now plot individual footprint clips plot(WREF_GEDI_footprints[[8]]) 6.12.11 Plot GEDI Waveform with AOP Pointcloud in 3D space Now that we can extract individual waveforms, and the AOP LiDAR pointcloud that corresponds with each GEDI waveform, lets see if we can plot them both in 3D space. We already know how to plot the AOP LiDAR points, so lets write a function to draw the GEDI waveform, too, using the points3d() function: for(shot_n in c(20)){ # First, plot the NEON AOP LiDAR clipped to the GEDI footprint # We save the plot as an object &#39;p&#39; which gives the (x,y) offset for the lower # left corner of the plot. The &#39;rgl&#39; package offsets all (x,y) locations # to this point, so we will need to subtract these values from any other # (x,y) points that we want to add to the plot p=plot(WREF_GEDI_footprints[[shot_n]]) # Extract the specific waveform from the GEDI data wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF$shot_number[shot_n]) # Make a new data.frame &#39;d&#39; to convert the waveform data coordinates into 3D space d=wf@dt # normalize rxwaveform to 0-1 d$rxwaveform=d$rxwaveform-min(d$rxwaveform) d$rxwaveform=d$rxwaveform/max(d$rxwaveform) # Add xy data in UTMs, and offset lower left corner of &#39;p&#39; d$x=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[1]-p[1] d$y=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[2]-p[2] # Make a new column &#39;x_wf&#39; where we place the GEDI waveform in space with the # NEON AOP LiDAR data, we scale the waveform to 30m in the x-dimension, and # offset by 12.5m (the radius of the GEDI footprint) in the x-dimension. d$x_wf=d$x+d$rxwaveform*30+12.5 # Add GEDI points to 3D space in &#39;green&#39; color points3d(x=d$x_wf, y=d$y, z=d$elevation, col=&quot;green&quot;, add=T) } Whoa, it looks like there is a bad vertical mismatch between those two data sources. This is because the two data sources have a different vertical datum. The NEON AOP data are delivered in the GEOID12A datum, while the GEDI data are delivered in the WGS84 native datum. We will need to convert one to the other to get them to line up correctly. 6.12.12 Datum, Geoid, and how to best measure the Earth We are seeing a vertical mismatch between the NEON and GEDI LiDAR data sources because they are using different standards for measuring the Earth. Here, we will briefly describe the main differences between these two datum models, and then show how to correct for this discrepancy. 6.12.12.1 WGS84 As described in this great Wikipedia article the World Geodedic System (WGS) is a global standard for cartography, geodesy, and navigation including GPS. Most people refer to WGS84, which is the latest revision to this system in 1984. To describe elevation, WGS84 uses an idealized mathematical model to describe the oblate spheroid of the Earth (basically, it looks kind of like a sphere that is wider around the equator and shorter from pole to pole). This model, called a datum, defines the relative elevational surface of the Earth. However, the Earth isnt exactly this idealized shape - it has a lot of undulations caused by topogrophy and local differences in its gravitational field. In the locations where the natural variations of the Earth do not coencide with the mathematical model, it can be harder to accurately describe the elevation of certain landforms and structures. Still, for a global datum, WGS84 does a great job overall. 6.12.12.2 GEOID12A While WGS84 is a convenient global standard, precisely describing location information is a common problem in geography, and is often remedied by using a local datum or projection. For latitude and longitude spatial information in North America, this is often done by using the North American Datum (NAD83) or various Universal Transverse Mercator (UTM) or State Plane projections based on the NAD83 datum. The NAD83 Datum was developed in 1983 as the horizontal and geometric control datum for the United States, Canada, Mexico, and Central America. This is the official datum for the government of the United States of America, and therefore many public datasets that are specific to this geographic region use this datum. In order to further refine the vertical precision over a wide geographic area, we can model the gravitational field of the Earth - this surface is called a Geoid. According to NOAA, a geoid can be defined as the equipotential surface of the Earths gravity field which best fits, in a least squares sense, global mean sea level. For a nice description of the North American geoid, please see this page on the NOAA website written by Dr. Allen Joel Anderson. As described by Dr. Anderson, this gravitational field is changing all the time, as can global mean sea level (due to melting glaciers, etc.). Therefore local and regional geoids must be updated regularly to reflect these changes, as shown in this list of NOAA geoids. One recent standard from 2012 is &lt;GEOID12A - this is the geoid selected by NEON as the vertical reference for the Z-dimension of our LiDAR data. Note that GEOID12B superscedes GEOID12A, however they are identical everywhere except for Puerto Rico and the US Virgin Islands according to NOAA. If you are working with any NEON data from Puerto Rico, please be aware that these data are delivered in GEOID12A format and may need to be converted to GEOID12B for some purposes. For more information about these standards, please see this ESRI help document. 6.12.13 Aligning the Vertical Datum AOP data have a relatively small area of coverage, and are therefore delivered with the coordinate reference system (CRS) in the UTM zone in which they were collected, set to the GEOID12A vertical datum (roughly equaling mean sea level). Meanwhile, GEDI data are global, so they are delivered with the common WGS84 ellipsoidal refrence datum. We need to align these vertical datums to ensure that the two data products line up correctly. However, this is not a trivial step, nor as simple as a reprojection. In this example, we will keep the NEON LiDAR data in its current datum, and convert the vertical position of the GEDI data from WGS84 into GEOID12A vertical position. NOAA has a useful tool called Vdatum that will convert from one datum to another. For this example, we will use Vdatum to convert from WGS84 to NAD83. Seeing as GEOID12A is based on the NAD83 datum, we can first convert the GEDI datas vertical position from WGS84 to NAD83 datum, then apply the offset between NAD83 and GEOID12A. Rather than use the Vdatum tool for each point, we will use a raster created by NEON scientists that reports the vertical difference between WGS84 and NAD83 for all points in the conterminous USA. You can access this dataset by clicking here to download the WGS84-NAD83 datum offset raster. 6.12.14 GEOID12A Height Model GEOID12A is a surface model that is described in terms of its relative height compared to NAD83. You can use this interactive webpage to find the geoid height for any location within North America. However, that would be combersome to have to use this webpage for every location. Instead, you can download a &lt;a href=https://www.ngs.noaa.gov/GEOID/GEOID12A/GEOID12A_CONUS.shtml&gt;binary file from the NOAA website that describes this geoids height, and convert that into a raster similar to the one that we just downloaded above. To simplify things here, we have already gone through the process of downloading the binary file and converting it to a raster for you to download, which can now easily be read into your R environment. You can access this dataset by clicking here to download the GEOID12A-NAD83 datum offset raster. GEOID12A_diff_rast=raster(&quot;./data/GEOID12A_NAD83_offset.tif&quot;) WGS84_diff_rast=raster(&quot;./data/WGS84_NAD83_offset.tif&quot;) Now that we have the two offset rasters, lets plot them together to compare their spatial patterns. par(mfrow=c(2,1), mar=c(2.5,2.5, 3.5,1)) plot(GEOID12A_diff_rast, col=viridis(100),main=&quot;GEOID12A minus NAD83 (m)&quot;) plot(WGS84_diff_rast, main=&quot;WGS84 minus NAD83 (m)&quot;) As you can see, the differences between the GEOID12A geoid, and the NAD83 sphereoid vary quite a lot across space, especially in mountainous areas. The magnitude of these differences is also large, upwards of 35m in some areas. Meanwhile, the differences between the NAD83 and WGS84 sphereoids shows a smooth gradient that is relatively small, with total difference less than 2m across the Conterminous USA. 6.12.15 Extract vertical offset for GEDI shots Now that we know the relative vertical offsets between WGS84, NAD83, and GEOID12A for all of the conterminous USA, we can use the extract() function to retrieve those relative offsets for any location. By combining those offsets together, we can finally rectify the vertical position of the NEON and GEDI LiDAR data. ***** DO I need to convert GEDI x/y locations from WGS84 to NAD83?? ***** # Make a new DF to store the GEDI footprint (x,y) locations, and the relative datum/geoid offsets footprint_df=as.data.frame( cbind(level1bgeo_WREF$longitude_lastbin, level1bgeo_WREF$latitude_lastbin)) WGS_NAD_diff &lt;- extract(WGS84_diff_rast, footprint_df) GEOID12A_NAD_diff &lt;- extract(GEOID12A_diff_rast,footprint_df) # Add together the offsets to calculate a vector of net differences in elevation net_diff=WGS_NAD_diff+GEOID12A_NAD_diff 6.12.16 Plot vertically corrected GEDI waveform in 3D Now that we have a vertical offset for each of the GEDI footprints, lets try again to plot the NEON AOP pointcloud data with the GEDI waveform. # You can enter whichever shots that you want to plot in the for loop here #for(shot_n in 1:length(WREF_GEDI_footprints)){ for(shot_n in c(20)){ # First, plot the NEON AOP LiDAR clipped to the GEDI footprint # We save the plot as an object &#39;p&#39; which gives the (x,y) offset for the lower # left corner of the plot. The &#39;rgl&#39; package offsets all (x,y) locations # to this point, so we will need to subtract these values from any other # (x,y) points that we want to add to the plot p=plot(WREF_GEDI_footprints[[shot_n]]) # Extract the specific waveform from the GEDI data wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF$shot_number[shot_n]) # Make a new data.frame &#39;d&#39; to convert the waveform data coordinates into 3D space d=wf@dt # normalize rxwaveform to 0-1 d$rxwaveform=d$rxwaveform-min(d$rxwaveform) d$rxwaveform=d$rxwaveform/max(d$rxwaveform) # Add xy data in UTMs, and offset lower left corner of &#39;p&#39; d$x=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[1]-p[1] d$y=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[2]-p[2] # Make a new column &#39;x_wf&#39; where we place the GEDI waveform in space with the # NEON AOP LiDAR data, we scale the waveform to 30m in the x-dimension, and # offset by 12.5m (the radius of the GEDI footprint) in the x-dimension. d$x_wf=d$x+d$rxwaveform*30+12.5 # Add GEDI points to 3D space in &#39;green&#39; color # This time, subtracting the elevation difference for that shot points3d(x=d$x_wf, y=d$y, z=d$elevation-net_diff[shot_n], col=&quot;green&quot;, add=T) } 6.12.17 Optional - NEON base plots You may also be interested to see if any of the GEDI footprints intersect a NEON base plot, which would allow for a direct comparison of the GEDI waveform with many of the datasets which are collected within the base plots, such as the vegetation structure data product containing height, DBH, and species identification of all trees &gt;10cm DBH. While it is statistically pretty unlikely that a GEDI footprint will intersect with your base plot of interest, it is possible that some GEDI footprint will intersetc with some base plot in your study area, so we may as well take a look: setwd(&quot;./data/&quot;) # This will depend upon your local environment # Download the NEON TOS plots polygons directly from the NEON website download.file(url=&quot;https://data.neonscience.org/api/v0/documents/All_NEON_TOS_Plots_V8&quot;, destfile=&quot;All_NEON_TOS_Plots_V8.zip&quot;, mode = &#39;wb&#39;) unzip(&quot;All_NEON_TOS_Plots_V8.zip&quot;) NEON_all_plots &lt;- st_read(&#39;All_NEON_TOS_Plots_V8/All_NEON_TOS_Plot_Polygons_V8.shp&#39;) ## Reading layer `All_NEON_TOS_Plot_Polygons_V8&#39; from data source ## `C:\\Users\\rohan\\katharynduffy.github.io\\data\\All_NEON_TOS_Plots_V8\\All_NEON_TOS_Plot_Polygons_V8.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3841 features and 36 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -156.6516 ymin: 17.9514 xmax: -66.82358 ymax: 71.3169 ## Geodetic CRS: WGS 84 # Select just the WREF site SITECODE = &#39;WREF&#39; base_plots_SPDF &lt;- NEON_all_plots[ (NEON_all_plots$siteID == SITECODE)&amp; (NEON_all_plots$subtype == &#39;basePlot&#39;),] rm(NEON_all_plots) base_crop=st_crop(base_plots_SPDF, extent(chm_WGS)) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries plot(chm_WGS) plot(base_crop$geometry, border = &#39;blue&#39;, add=T) 6.13 NEON AOP Written Questions: Reminder: these questions are largely based on the assigned video lectures. I highly recommend you watching or re-watching them before tackling these questions. What is the overlap in the type of data generated by NEON AOP and PhenoCam. What metrics from PhenoCam could you apply to NEON AOP data, especially once there are &gt;10 years of aerial data? List 4 challenges to timing AOP flight campaigns. When using NEON AOP data, when should you use byTileAOP versus byFileAOP? What additional processing might you need to do if you worked with the byFileAOP data? Hint: try pulling the data in both formats (this will take a while) How do the additional bands in NEON AOP improve our contraint of Biodiversity and Ecosystem Structure relative to satelite-derived data? What role do campaigns such as NEON AOP play in filling the sampling gap of essential in situ data such as FLUXNET (Eddy Co-Variance) data? List 3 attributes of NEON AOP data that are unique to other publicly served data. Hints: Band width? Resolution? Something else? Which data product(s) derived from the NEON AOP campaign align with remotely-sensed products from NASA? Hint: start with vegetation indices 6.14 NEON AOP Coding Lab For the purpose of this coding lab we will evaluate 2 different forested sites in 2018: Guanica (GUAN) in Puerto Rico: Bartlett Experimental Forest (BART) in New Hampshire: 1: How are these two forests similar? How are they different? (3-5 sentences) A few places to start if you need them. You dont have to address these, just in case you need some hints: Climate? How that could affect vegetation structure? Heterogeniety of the ecossytems? How seasonal are they? (hint:PhenoCam) Something else? 2: Using this NEON tutorial and the tutorials weve covered in this textbook (hint: you did half of this workflow in your very first coding lab) pull the NEON AOP derived Canopy Height Model (CHM, DP3.30015.001) and High-resolution orthorectified camera imagery mosaic DP3.30010.001 for each forest and overlay the NEON TOS Woody Vegetation Structure DP1.10098.001 basal diameter data product to evaluate how well AOP captures trees in each forest. Generate a labeled 2x2 plot panel including: Each RGB image with basal diameter overlaid Each CHM with basal diameter overlaid Hints/reminders from section 2.9: &lt;- loadByProduct &lt;- getLocTOS &lt;- merge Write 2-3 sentences summarizing your findings and thoughts. 3: Use the byTileAOP function of the neonUtilities package to pull a subset of the descrete LiDAR pointcloud for each forest (Hint: You can feed byTileAOP Easting and Northing from your Vegetation Structure dataframe(s)). Use the structural_diversity_metrics function that you defined in section 6.5 of the textbook to process discrete return LiDAR for each site and generte structural diversity metrics. Using lidR generate a labeled 2-panel plot of your canopy height model for each forest Using lidR generate a labeled 2-panel plot of a cross-section for each forest Use section 6.5.3 Comparing Metrics Between Forests to compare each forest and generate a a clean summary table via kable Using Table 2 from LaRue, Wagner, et al. (2020) as a reference, write 1-2 paragraphs summarizing the differences in forest structural diversity between the two forests and how they may relate to your answers to Question 1. 6.15 NEON AOP Culmination Write Up Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using NEON AOP data. Include the types of NEON (and other data) that you would need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. Suggestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragraph summarizing how this data or analysis is useful to you and/or the infrastructure. "],["nasas-earth-observing-system-eos.html", "Chapter 7 NASAs Earth Observing System (EOS) 7.1 Learning Objectives 7.2 NASA EOS Project Mission &amp; Design 7.3 NASA EOS Earth Data Account: 7.4 NASA EOS Coding Assignment 1 7.5 Distributed Active Archive Centers 7.6 The LPDAAC Mission: Process, Archive, Distribute, Apply 7.7 AppEEARS 7.8 Hands on: Pulling AppEEARS Data via the API 7.9 Getting Started with the AppEEARS API (Point Request) 7.10 Topics Covered in this section: 7.11 Getting Started with the AppEEARS API 7.12 Query Available Products 7.13 Search and Explore Available Products 7.14 Search and Explore Available Layers 7.15 Submit a Point Request 7.16 Download a Request 7.17 Download Files in a Request (Automation) 7.18 Explore AppEEARS Quality Service 7.19 Decode Quality Values 7.20 Load Request Output and Visualize 7.21 Load a CSV 7.22 Plot Results (Line/Scatter Plots) 7.23 Submit an Area Request 7.24 Submit an area request using a NEON site boundary as the region of interest for extracting elevation, vegetation and land surface temperature data 7.25 Getting Started with an Area Request 7.26 Set Up the Output Directory 7.27 Load your Earth Data Token 7.28 Query Available Products 7.29 Search and Explore Available Products 7.30 Search and Explore Available Layers 7.31 Submit an Area Request 7.32 Search and Explore Available Projections 7.33 Compile a JSON Object 7.34 Submit a Task Request 7.35 Retrieve Task Status 7.36 Download a Request 7.37 Download Files in a Request (Automation) 7.38 Explore AppEEARS Quality Service 7.39 List Quality Layers 7.40 Show Quality Values 7.41 Decode Quality Values 7.42 BONUS: Load Request Output and Visualize 7.43 Load a GeoTIFF 7.44 Plot a GeoTIFF 7.45 NASA EOS Coding Lab #2 7.46 NASA EOS Written Questions 7.47 NASA EOS Culmination Write Up", " Chapter 7 NASAs Earth Observing System (EOS) Estimated Time: 3 hours 7.1 Learning Objectives Describe the mission of NASA EOS Describe the roles of DAACs in distributing data Interact with the AppEEARS API to query the list of available products Submit and download submitting a point and area sample requests, download the request Filter data based on quality 7.2 NASA EOS Project Mission &amp; Design NASAs Earth Observing System (EOS) is a coordinated series of polar-orbiting and low inclination satellites for long-term global observations of the land surface, biosphere, solid Earth, atmosphere, and oceans. As a major component of the Earth Science Division of NASAs Science Mission Directorate, EOS enables an improved understanding of the Earth as an integrated system. Review NASA EOSs Mission Profile Completed Missions Current Missions Future Missions Credit: NASAs Goddard Space Flight Center 7.3 NASA EOS Earth Data Account: The EOSDIS Earthdata Login provides a centralized and simplified mechanism for user registration and profile management for all EOSDIS system components. End users may register and edit their profile information in one location allowing them access to the wide array of EOSDIS data and services. The EOSDIS Earthdata Login also helps the EOSDIS program better understand the user demographics and access patterns in support of planning for new value-added features and customized services that can be directed to specific users or user groups resulting in better user experience. Earthdata Login provides user registration and authentication services and a common set of user information to all EOSDIS data centers in a manner that permits the data center to integrate their additional requirements with the Earthdata Login services. Below is a brief description of services provided by the Earthdata Login. 7.4 NASA EOS Coding Assignment 1 Suggested completion: following lecture 1 on NASA EOS To submit via BBLearn: Follow the steps in the NASA EOSDIS documentation to sign up for an Earth Data account. Write an R script that stores your user and password called EARTHDATA_Token.R and submit the following line of code via .Rmd and PDF: source(&#39;./Tokens/EARTHDATA_Token.R&#39;) #path will change based on where you stored it exists(&#39;user&#39;) 7.5 Distributed Active Archive Centers 7.5.1 LP DAAC The Land Processes Distributed Active Archive Center (LP DAAC) is one of several discipline-specific data centers within the NASA EOS Data and Information System (EOSDIS). The LP DAAC operates as a partnership between the U.S. Geological Survey (USGS) and the National Aeronautics and Space Administration (NASA). Data specialists, system engineers, user service representatives, and science communicators work in collaboration to support LP DAAC activities. Watch this 4:02 minute video on LP DAACs 2019-2021 Prosectus 7.6 The LPDAAC Mission: Process, Archive, Distribute, Apply The LP DAAC processes, archives, and distributes land data products to hundreds of thousands of users in the earth science community. Land data products are made universally accessible and support the ongoing monitoring of Earths land dynamics and environmental systems to facilitate interdisciplinary research, education, and decision-making. Process: Raw data collected from specific satellite sensors, such as ASTER onboard NASAs Terra satellite, are received and processed into a readable and interpretable format here at the LP DAAC, while other data undergo processing in other facilities around the country before arriving to the LP DAAC to be archived and distributed to the public. Archive: The LP DAAC continually archives a wide variety of land remote sensing data products collected by sensors onboard satellites, aircraft, and the International Space Station (ISS). The archive currently totals more than 3.5 petabytes of data, the equivalent of listening to 800 million songs, and distributes data to over 200,000 global users. Distribute: All data products in the archive are distributed free of charge through NASA Earthdata Search and USGS EarthExplorer search and download clients. The LP DAAC also supports tools and services, like the Application for Extracting and Exploring Analysis Ready Samples (AppEEARS), which allows users to transform and visualize data before download while offering enhanced subsetting and reprojecting capabilities. 7.6.1 How you can use LP DAACs data Watch this 2:13 minute long video on searching for data at the LP DAAC 7.7 AppEEARS The Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The AppEEARS API offers users programmatic access to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial. 7.8 Hands on: Pulling AppEEARS Data via the API The following section was adapted from LPDAACs E-Learning Tutorials on the AppEEARS API and modified to request data for NEON sites. Contributing Authors: Material written by Mahsa Jami1 and Cole Krehbiel1 Contact: LPDAAC@usgs.gov Voice: +1-866-573-3222 Organization: Land Processes Distributed Active Archive Center (LP DAAC) Website: https://lpdaac.usgs.gov/ Date last modified: 06-12-2020 1 KBR, Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Work performed under USGS contract G15PD00467 for LP DAAC2. 2 LP DAAC Work performed under NASA contract NNG14HH33I. Run the following chunk to install any packages you will need for this section: # Packages you will need for AppEEARS API Tutorials packages = c(&#39;getPass&#39;,&#39;httr&#39;,&#39;jsonlite&#39;,&#39;ggplot2&#39;,&#39;dplyr&#39;,&#39;tidyr&#39;,&#39;readr&#39;,&#39;geojsonio&#39;,&#39;geojsonR&#39;,&#39;rgdal&#39;,&#39;sp&#39;, &#39;raster&#39;, &#39;rasterVis&#39;, &#39;RColorBrewer&#39;, &#39;jsonlite&#39;) # Identify missing packages new.packages = packages[!(packages %in% installed.packages()[,&quot;Package&quot;])] # Loop through and download the required packages if (length(new.packages)[1]==0){ message(&#39;All packages already installed&#39;) }else{ for (i in 1:length(new.packages)){ message(paste0(&#39;Installing: &#39;, new.packages)) install.packages(new.packages[i]) } } 7.9 Getting Started with the AppEEARS API (Point Request) This section demonstrates how to use R to connect to the AppEEARS API The Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The AppEEARS API offers users programmatic access to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial. 7.9.1 Example: Submit a point request for multiple NEON sites to extract vegetation and land surface temperature data In this tutorial, Connecting to the AppEEARS API, querying the list of available products, submitting a point sample request, downloading the request, working with the AppEEARS Quality API, and loading the results into R for visualization are covered. AppEEARS point requests allow users to subset their desired data using latitude/longitude geographic coordinate pairs (points) for a time period of interest, and for specific data layers within data products. AppEEARS returns the valid data from the parameters defined within the sample request. 7.9.1.1 Data Used in the Example: Data layers: Combined MODIS Leaf Area Index (LAI) MCD15A3H.006, 500m, 4 day: Lai_500m Terra MODIS Land Surface Temperature MOD11A2.006, 1000m, 8 day: LST_Day_1km, LST_Night_1km 7.10 Topics Covered in this section: Getting Started 1a. Load Packages 1b. Set Up the Output Directory 1c. Login Query Available Products 2a. Search and Explore Available Products 2b. Search and Explore Available Layers Submit a Point Request 3a. Compile a JSON Object 3b. Submit a Task Request 3c. Retrieve Task Status Download a Request 4a. Explore Files in Request Output 4b. Download Files in a Request (Automation) Explore AppEEARS Quality API 5a. List Quality Layers 5b. Show Quality Values 5c. Decode Quality Values BONUS: Load Request Output and Visualize 6a. Load CSV 6b. Plot Results (Line/Scatter Plots) 7.10.1 Prerequisites: A NASA Earthdata Login account is required to login to the AppEEARS API and submit a request . You can create an account at the link provided. R and RStudio. These tutorials have been tested on Windows and MAC systems using R Version 4.0.0, RStudio version 1.1.463, and the specifications listed below. Required packages: getPass httr jsonlite warnings To read and visualize the tabular data: dplyr tidyr readr ggplot2 7.10.1.1 Getting Started: Clone/download AppEEARS API Getting Started in R Repository from the LP DAAC Data User Resources Repository or pull code from this textbook. If you opted to clone LP DAACs repo open the AppEEARS_API_R.Rproj file to directly open the project. Next, select the AppEEARS_API_Point_R.Rmd from the files list and open it. 7.10.2 AppEEARS Information: To access AppEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/. For comprehensive documentation of the full functionality of the AppEEARS API, please see the AppEEARS API Documentation. 7.11 Getting Started with the AppEEARS API 7.11.1 Load Packages First, load the R packages necessary to run the tutorial. # Load necessary packages into R library(getPass) # A micro-package for reading passwords library(httr) # To send a request to the server/receive a response from the server library(jsonlite) # Implements a bidirectional mapping between JSON data and the most important R data types library(ggplot2) # Functions for graphing and mapping library(tidyr) # Function for working with tabular data library(dplyr) # Function for working with tabular data library(readr) # Read rectangular data like CSV 7.11.2 Set Up the Output Directory Set your input directory, and create an output directory for the results. outDir &lt;- file.path(&#39;./data/&#39;) # Create an output directory if it doesn&#39;t exist suppressWarnings(dir.create(outDir)) 7.11.3 Login to Earth Data To submit a request, you must first login to the AppEEARS API. Use your private R Script to enter your NASA Earthdata login Username and Password. source(&#39;EARTHDATA_Token.R&#39;) #path will change based on where you stored it exists(&#39;user&#39;) ## [1] TRUE Decode the username and password to be used to post login request. secret &lt;- jsonlite::base64_enc(paste(user, password, sep = &quot;:&quot;)) # Encode the string of username and password Next, assign the AppEEARS API URL to a static variable. API_URL = &quot;https://appeears.earthdatacloud.nasa.gov/api/&quot; # Set the AppEEARS API to a variable Use the httr package to post your username and password. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the API Documentation. # Insert API URL, call login service, set the component of HTTP header, and post the request to the server response &lt;- httr::POST(paste0(API_URL,&quot;login&quot;), add_headers(&quot;Authorization&quot; = paste(&quot;Basic&quot;, gsub(&quot;\\n&quot;, &quot;&quot;, secret)), &quot;Content-Type&quot; =&quot;application/x-www-form-urlencoded;charset=UTF-8&quot;), body = &quot;grant_type=client_credentials&quot;) response_content &lt;- content(response) # Retrieve the content of the request token_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the response to the JSON object remove(user, password, secret, response) # Remove the variables that are not needed anymore prettify(token_response) # Print the prettified response ## { ## &quot;token_type&quot;: &quot;Bearer&quot;, ## &quot;token&quot;: &quot;3XvrYjK2H8Q4vCMa3OaW712gYBtzUIVXMpE-qN4DDk0dCDSRP82pfAQExZ91nQ-cvnG_UQQaI3_K5bcfTh5iTw&quot;, ## &quot;expiration&quot;: &quot;2022-09-30T22:34:33Z&quot; ## } ## Above, you should see a Bearer token. Notice that this token will expire approximately 48 hours after being acquired. 7.12 Query Available Products The product API provides details about all of the products and layers available in AppEEARS. For more information, please see the API Documentation. Below, call the product API to list all of the products available in AppEEARS. prods_req &lt;- GET(paste0(API_URL, &quot;product&quot;)) # Request the info of all products from product service prods_content &lt;- content(prods_req) # Retrieve the content of request all_Prods &lt;- toJSON(prods_content, auto_unbox = TRUE) # Convert the info to JSON object remove(prods_req, prods_content) # Remove the variables that are not needed anymore # prettify(all_Prods) # Print the prettified product response 7.13 Search and Explore Available Products Create a list indexed by product name to make it easier to query a specific product. # Divides information from each product. divided_products &lt;- split(fromJSON(all_Prods), seq(nrow(fromJSON(all_Prods)))) # Create a list indexed by the product name and version products &lt;- setNames(divided_products,fromJSON(all_Prods)$ProductAndVersion) # Print no. products available in AppEEARS sprintf(&quot;AppEEARS currently supports %i products.&quot; ,length(products)) ## [1] &quot;AppEEARS currently supports 162 products.&quot; Next, look at the products names and descriptions. Below, the ProductAndVersion and Description are printed for all products. # Loop through the products in the list and print the product name and description for (p in products){ print(paste0(p$ProductAndVersion,&quot; is &quot;,p$Description,&quot; from &quot;,p$Source)) } ## [1] &quot;GPW_DataQualityInd.411 is Quality of Input Data for Population Count and Density Grids from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopCount.411 is UN-adjusted Population Count from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopDensity.411 is UN-adjusted Population Density from SEDAC&quot; ## [1] &quot;MCD12Q1.006 is Land Cover Type from LP DAAC&quot; ## [1] &quot;MCD12Q2.006 is Land Cover Dynamics from LP DAAC&quot; ## [1] &quot;MCD12Q1.061 is Land Cover Type from LP DAAC&quot; ## [1] &quot;MCD12Q2.061 is Land Cover Dynamics from LP DAAC&quot; ## [1] &quot;MCD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A3H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A3H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD43A1.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A1.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A2.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A3.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A3.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A4.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A4.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD64A1.006 is Burned Area (fire) from LP DAAC&quot; ## [1] &quot;MCD64A1.061 is Burned Area (fire) from LP DAAC&quot; ## [1] &quot;MOD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09A1.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GA.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09GQ.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09Q1.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MOD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MOD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A1.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A2.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A3.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13Q1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MOD14A2.061 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MOD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MOD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MOD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2.061 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A3GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2H.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2HGF.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2HGF.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MOD17A3HGF.061 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MOD21A1D.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD21A1N.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD21A2.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD44B.006 is Vegetation Continuous Fields (VCF) from LP DAAC&quot; ## [1] &quot;MOD44W.006 is Land/Water Mask from LP DAAC&quot; ## [1] &quot;MODOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MODTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;MYD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09A1.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GA.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09GQ.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09Q1.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MYD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MYD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A1.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A2.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A3.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13Q1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MYD14A2.061 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MYD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MYD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MYD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2.061 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A3GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MYD17A2H.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MYD17A2HGF.006 is Gross Primary Productivity (GPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A2HGF.061 is Gross Primary Productivity (GPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A3HGF.061 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD21A1D.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1D.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1N.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1N.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYDOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MYDTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;NASADEM_NC.001 is Elevation from LP DAAC&quot; ## [1] &quot;NASADEM_NUMNC.001 is Source from LP DAAC&quot; ## [1] &quot;SPL3SMP_E.005 is Enhanced L3 Radiometer Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3SMP.008 is Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL4CMDL.006 is Carbon Net Ecosystem Exchange from NSIDC DAAC&quot; ## [1] &quot;SPL4SMGP.006 is Surface and Root Zone Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3FTP.003 is Freeze/Thaw State from NSIDC DAAC&quot; ## [1] &quot;SRTMGL1_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL1_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;ASTGTM_NC.003 is Elevation from LP DAAC&quot; ## [1] &quot;ASTGTM_NUMNC.003 is Source from LP DAAC&quot; ## [1] &quot;ASTWBD_ATTNC.001 is Water Bodies Database Attributes from LP DAAC&quot; ## [1] &quot;ASTWBD_NC.001 is Water Bodies Database Elevation from LP DAAC&quot; ## [1] &quot;VNP09H1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP09A1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP09GA.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP13A1.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A2.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A3.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP14A1.001 is Thermal Anomalies/Fire from LP DAAC&quot; ## [1] &quot;VNP15A2H.001 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;VNP21A1D.001 is Land Surface Temperature &amp; Emissivity Day (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A1N.001 is Land Surface Temperature &amp; Emissivity Night (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A2.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP22Q2.001 is Global Land Surface Phenology (GLSP) from LP DAAC&quot; ## [1] &quot;VNP43IA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43IA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43IA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43IA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;VNP43MA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43MA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43MA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43MA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;DAYMET.004 is Daily Surface Weather Data for North America from ORNL&quot; ## [1] &quot;ECO2LSTE.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;ECO2CLD.001 is Cloud Mask from LP DAAC&quot; ## [1] &quot;ECO3ETPTJPL.001 is Evapotranspiration PT-JPL from LP DAAC&quot; ## [1] &quot;ECO3ANCQA.001 is L3/L4 Ancillary Data Quality Assurance (QA) Flags from LP DAAC&quot; ## [1] &quot;ECO4ESIPTJPL.001 is Evaporative Stress Index PT-JPL from LP DAAC&quot; ## [1] &quot;ECO4WUE.001 is Water Use Efficiency from LP DAAC&quot; ## [1] &quot;ECO1BGEO.001 is Geolocation from LP DAAC&quot; ## [1] &quot;ECO1BMAPRAD.001 is Resampled Radiance from LP DAAC&quot; ## [1] &quot;ECO3ETALEXI.001 is Evapotranspiration dis-ALEXI from LP DAAC&quot; ## [1] &quot;ECO4ESIALEXI.001 is Evaporative Stress Index dis-ALEXI from LP DAAC&quot; ## [1] &quot;ECO_L1B_GEO.002 is Geolocation from LP DAAC&quot; ## [1] &quot;ECO_L2_CLOUD.002 is Cloud Mask Instantaneous from LP DAAC&quot; ## [1] &quot;ECO_L2_LSTE.002 is Swath Land Surface Temperature and Emissivity Instantaneous from LP DAAC&quot; ## [1] &quot;HLSS30.020 is Land Surface Reflectance from LP DAAC&quot; ## [1] &quot;HLSL30.020 is Land Surface Reflectance from LP DAAC&quot; The product service provides many useful details, including if a product is currently available in AppEEARS, a description, and information on the spatial and temporal resolution. Below, the product details are retrieved using ProductAndVersion. # Convert the MCD15A3H.006 info to JSON object and print the prettified info prettify(toJSON(products$&quot;MCD15A3H.006&quot;)) ## [ ## { ## &quot;Product&quot;: &quot;MCD15A3H&quot;, ## &quot;Platform&quot;: &quot;Combined MODIS&quot;, ## &quot;Description&quot;: &quot;Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR)&quot;, ## &quot;RasterType&quot;: &quot;Tile&quot;, ## &quot;Resolution&quot;: &quot;500m&quot;, ## &quot;TemporalGranularity&quot;: &quot;4 day&quot;, ## &quot;Version&quot;: &quot;006&quot;, ## &quot;Available&quot;: true, ## &quot;DocLink&quot;: &quot;https://doi.org/10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;Source&quot;: &quot;LP DAAC&quot;, ## &quot;TemporalExtentStart&quot;: &quot;2002-07-04&quot;, ## &quot;TemporalExtentEnd&quot;: &quot;Present&quot;, ## &quot;Deleted&quot;: false, ## &quot;DOI&quot;: &quot;10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## Also, the products can be searched using their description. Below, search for products containing Leaf Area Index in their description and make a list of their productAndVersion. LAI_Products &lt;- list() # Create an empty list for (p in products){ # Loop through the product list if (grepl(&#39;Leaf Area Index&#39;, p$Description )){ # Look through the product description for a keyword LAI_Products &lt;- append(LAI_Products, p$ProductAndVersion) # Append the LAI products to the list } } LAI_Products ## [[1]] ## [1] &quot;MCD15A2H.006&quot; ## ## [[2]] ## [1] &quot;MCD15A2H.061&quot; ## ## [[3]] ## [1] &quot;MCD15A3H.006&quot; ## ## [[4]] ## [1] &quot;MCD15A3H.061&quot; ## ## [[5]] ## [1] &quot;MOD15A2H.006&quot; ## ## [[6]] ## [1] &quot;MOD15A2H.061&quot; ## ## [[7]] ## [1] &quot;MYD15A2H.006&quot; ## ## [[8]] ## [1] &quot;MYD15A2H.061&quot; ## ## [[9]] ## [1] &quot;VNP15A2H.001&quot; Using the info above, Create a list of desired products. desired_products &lt;- c(&#39;MCD15A3H.006&#39;,&#39;MOD11A2.006&#39;) # Create a vector of desired products desired_products ## [1] &quot;MCD15A3H.006&quot; &quot;MOD11A2.006&quot; 7.14 Search and Explore Available Layers This API call will list all of the layers available for a given product. Each product is referenced by its ProductAndVersion property which is also referred to as the product_id. First, request the layers for the MCD15A3H.006 product. # Request layers for the 1st product in the list: MCD15A3H.006 MCD15A3H_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[1])) # Request the info of a product from product URL MCD15A3H_content &lt;- content(MCD15A3H_req) # Retrieve content of the request MCD15A3H_response &lt;- toJSON(MCD15A3H_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MCD15A3H_req, MCD15A3H_content) # Remove the variables that are not needed anymore #prettify(MCD15A3H_response) # Print the prettified response names(fromJSON(MCD15A3H_response)) # print the layer&#39;s names ## [1] &quot;FparExtra_QC&quot; &quot;FparLai_QC&quot; &quot;FparStdDev_500m&quot; &quot;Fpar_500m&quot; ## [5] &quot;LaiStdDev_500m&quot; &quot;Lai_500m&quot; Next, request the layers for the MOD11A2.006 product. MOD11_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[2])) # Request the info of a product from product URL MOD11_content &lt;- content(MOD11_req) # Retrieve content of the request MOD11_response &lt;- toJSON(MOD11_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MOD11_req, MOD11_content) # Remove the variables that are not needed anymore names(fromJSON(MOD11_response)) # print the layer names ## [1] &quot;Clear_sky_days&quot; &quot;Clear_sky_nights&quot; &quot;Day_view_angl&quot; &quot;Day_view_time&quot; ## [5] &quot;Emis_31&quot; &quot;Emis_32&quot; &quot;LST_Day_1km&quot; &quot;LST_Night_1km&quot; ## [9] &quot;Night_view_angl&quot; &quot;Night_view_time&quot; &quot;QC_Day&quot; &quot;QC_Night&quot; Lastly, select the desired layers and pertinent products and make a data frame using this information. This data frame will be inserted into the nested data frame that will be used to create a JSON object to submit a request in Section 3. desired_layers &lt;- c(&quot;LST_Day_1km&quot;,&quot;LST_Night_1km&quot;,&quot;Lai_500m&quot;) # Create a vector of desired layers desired_prods &lt;- c(&quot;MOD11A2.006&quot;,&quot;MOD11A2.006&quot;,&quot;MCD15A3H.006&quot;) # Create a vector of products including the desired layers # Create a data frame including the desired data products and layers layers &lt;- data.frame(product = desired_prods, layer = desired_layers) 7.15 Submit a Point Request The Submit task API call provides a way to submit a new request to be processed. It can accept data via JSON or query string. In the example below, create a JSON object and submit a request. Tasks in AppEEARS correspond to each request associated with your user account. Therefore, each of the calls to this service requires an authentication token. 7.15.1 Compile a JSON Object In this section, begin by setting up the information needed for a nested data frame that will be later converted to a JSON object for submitting an AppEEARS point request. For detailed information on required JSON parameters, see the API Documentation. For point requests, beside the date range and desired layers information, the coordinates property must also be inside the task object. Optionally, set id and category properties to further identify your selected coordinates. Well start by requesting point-based data for NEON.D17.SOAP and NEON.D17.SJER: startDate &lt;- &quot;01-01-2020&quot; # Start of the date range for which to extract data: MM-DD-YYYY endDate &lt;- &quot;10-01-2020&quot; # End of the date range for which to extract data: MM-DD-YYYY recurring &lt;- FALSE # Specify True for a recurring date range #yearRange &lt;- [2000,2016] # If recurring = True, set yearRange, change start/end date to MM-DD lat &lt;- c(37.0334, 37.1088) # Latitude of the point sites lon &lt;- c(-119.2622, -119.7323) # Longitude of the point sites id &lt;- c(&quot;0&quot;,&quot;1&quot;) # ID for the point sites category &lt;- c(&quot;SOAP&quot;, &quot;SJER&quot;) # Category for point sites taskName &lt;- &#39;NEON SOAP SJER Vegetation&#39; # Enter name of the task here taskType &lt;- &#39;point&#39; # Specify the task type, it can be either &quot;area&quot; or &quot;point&quot; To be able to successfully submit a task, the JSON object should be structured in a certain way. The code chunk below uses the information from the previous chunk to create a nested data frame. This nested data frame will be converted to JSON object that can be used to complete the request. # Create a data frame including the date range for the request date &lt;- data.frame(startDate = startDate, endDate = endDate) # Create a data frame including lat and long coordinates. ID and category name is optional. coordinates &lt;- data.frame(id = id, longitude = lon, latitude = lat, category = category) task_info &lt;- list(date,layers, coordinates) # Create a list of data frames names(task_info) &lt;- c(&quot;dates&quot;, &quot;layers&quot;, &quot;coordinates&quot;) # Assign names task &lt;- list(task_info, taskName, taskType) # Create a nested list names(task) &lt;- c(&quot;params&quot;, &quot;task_name&quot;, &quot;task_type&quot;) # Assign names remove(date, layers, coordinates, task_info) # Remove the variables that are not needed anymore toJSON function from jsonlite package converts the type of data frame to a string that can be recognized as a JSON object to be submitted as a point request. task_json &lt;- toJSON(task,auto_unbox = TRUE) # Convert to JSON object 7.15.2 Submit a Task Request Token information is needed to submit a request. Below the login token is assigned to a variable. token &lt;- paste(&quot;Bearer&quot;, fromJSON(token_response)$token) # Save login token to a variable Below, post a call to the API task service, using the task_json created above. # Post the point request to the API task service response &lt;- POST(paste0(API_URL, &quot;task&quot;), body = task_json , encode = &quot;json&quot;, add_headers(Authorization = token, &quot;Content-Type&quot; = &quot;application/json&quot;)) task_content &lt;- content(response) # Retrieve content of the request task_response &lt;- prettify(toJSON(task_content, auto_unbox = TRUE))# Convert the content to JSON object remove(response, task_content) # Remove the variables that are not needed anymore task_response # Print the prettified task response ## { ## &quot;task_id&quot;: &quot;f6cd3e97-1424-4401-9440-434a19b53c5c&quot;, ## &quot;status&quot;: &quot;pending&quot; ## } ## 7.15.3 Retrieve Task Status This API call will list all of the requests associated with your user account, automatically sorted by date descending with the most recent requests listed first. The AppEEARS API contains some helpful formatting resources. Below, limit the API response to 2 entries for the last 2 requests and set pretty to True to format the response as an organized JSON object to make it easier to read. Additional information on AppEEARS API retrieve task, pagination, and formatting can be found in the API documentation. params &lt;- list(limit = 2, pretty = TRUE) # Set up query parameters # Request the task status of last 2 requests from task URL response_req &lt;- GET(paste0(API_URL,&quot;task&quot;), query = params, add_headers(Authorization = token)) response_content &lt;- content(response_req) # Retrieve content of the request status_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the content to JSON object remove(response_req, response_content) # Remove the variables that are not needed anymore prettify(status_response) # Print the prettified response ## [ ## { ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;pending&quot;, ## &quot;created&quot;: &quot;2022-09-29T00:01:49.833402&quot;, ## &quot;task_id&quot;: &quot;f6cd3e97-1424-4401-9440-434a19b53c5c&quot;, ## &quot;updated&quot;: &quot;2022-09-29T00:01:49.994789&quot;, ## &quot;user_id&quot;: &quot;rdb273@nau.edu&quot;, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;has_swath&quot;: false, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;3.12&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;has_nsidc_daac&quot;: false, ## &quot;expires_on&quot;: &quot;2022-11-28T00:01:49.994789&quot; ## }, ## { ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;pending&quot;, ## &quot;created&quot;: &quot;2022-09-29T00:00:00.370981&quot;, ## &quot;task_id&quot;: &quot;29c96457-4900-4631-8313-c41337ef3fbc&quot;, ## &quot;updated&quot;: &quot;2022-09-29T00:00:00.648818&quot;, ## &quot;user_id&quot;: &quot;rdb273@nau.edu&quot;, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;has_swath&quot;: false, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;3.12&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;has_nsidc_daac&quot;: false, ## &quot;expires_on&quot;: &quot;2022-11-28T00:00:00.648818&quot; ## } ## ] ## The task_id that was generated when submitting your request can also be used to retrieve a task status. task_id &lt;- fromJSON(task_response)$task_id # Extract the task_id of submitted point request # Request the task status of a task with the provided task_id from task URL status_req &lt;- GET(paste0(API_URL,&quot;task/&quot;, task_id), add_headers(Authorization = token)) status_content &lt;- content(status_req) # Retrieve content of the request statusResponse &lt;-toJSON(status_content, auto_unbox = TRUE) # Convert the content to JSON object stat &lt;- fromJSON(statusResponse)$status # Assign the task status to a variable remove(status_req, status_content) # Remove the variables that are not needed anymore prettify(statusResponse) # Print the prettified response ## { ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ], ## &quot;coordinates&quot;: [ ## { ## &quot;id&quot;: &quot;0&quot;, ## &quot;category&quot;: &quot;SOAP&quot;, ## &quot;latitude&quot;: 37.0334, ## &quot;longitude&quot;: -119.2622 ## }, ## { ## &quot;id&quot;: &quot;1&quot;, ## &quot;category&quot;: &quot;SJER&quot;, ## &quot;latitude&quot;: 37.1088, ## &quot;longitude&quot;: -119.7323 ## } ## ] ## }, ## &quot;status&quot;: &quot;pending&quot;, ## &quot;created&quot;: &quot;2022-09-29T00:01:49.833402&quot;, ## &quot;task_id&quot;: &quot;f6cd3e97-1424-4401-9440-434a19b53c5c&quot;, ## &quot;updated&quot;: &quot;2022-09-29T00:01:49.994789&quot;, ## &quot;user_id&quot;: &quot;rdb273@nau.edu&quot;, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;has_swath&quot;: false, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;3.12&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;has_nsidc_daac&quot;: false, ## &quot;expires_on&quot;: &quot;2022-11-28T00:01:49.994789&quot; ## } ## Retrieve the task status every 5 seconds. The task status should be done to be able to download the output. while (stat != &#39;done&#39;) { Sys.sleep(5) # Request the task status and retrieve content of request from task URL stat_content &lt;- content(GET(paste0(API_URL,&quot;task/&quot;, task_id), add_headers(Authorization = token))) stat &lt;-fromJSON(toJSON(stat_content, auto_unbox = TRUE))$status # Get the status remove(stat_content) print(stat) } 7.16 Download a Request 7.16.1 Explore Files in Request Output Before downloading the request output, examine the files contained in the request output. # Request the task bundle info from API bundle URL response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id), add_headers(Authorization = token)) response_content &lt;- content(response) # Retrieve content of the request bundle_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the content to JSON object prettify(bundle_response) # Print the prettified response ## { ## &quot;message&quot;: &quot;Document not found: {&#39;task_id&#39;: &#39;f6cd3e97-1424-4401-9440-434a19b53c5c&#39;}&quot; ## } ## 7.17 Download Files in a Request (Automation) The bundle API provides information about completed tasks. For any completed task, a bundle can be queried to return the files contained as a part of the task request. Below, call the bundle API and return all of the output files. Next, read the contents of the bundle in JSON format and loop through file_id to automate downloading all of the output files into the output directory. For more information, please see AppEEARS API Documentation. bundle &lt;- fromJSON(bundle_response)$files for (id in bundle$file_id){ # retrieve the filename from the file_id filename &lt;- bundle[bundle$file_id == id,]$file_name # create a destination directory to store the file in filepath &lt;- paste(outDir,filename, sep = &quot;/&quot;) suppressWarnings(dir.create(dirname(filepath))) # write the file to disk using the destination directory and file name response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id, &quot;/&quot;, id), write_disk(filepath, overwrite = TRUE), progress(), add_headers(Authorization = token)) } 7.18 Explore AppEEARS Quality Service The quality API provides quality details about all of the data products available in AppEEARS. Below are examples of how to query the quality API for listing quality products, layers, and values. The final example (Section 5c.) demonstrates how AppEEARS quality services can be leveraged to decode pertinent quality values for your data. For more information visit AppEEARS API documentation. First, reset pagination to include offset which allows you to set the number of results to skip before starting to return entries. Next, make a call to list all of the data product layers and the associated quality product and layer information. params &lt;- list(limit = 6, offset = 20, pretty = TRUE) # Set up the query parameters q_req &lt;- GET(paste0(API_URL, &quot;quality&quot;), query = params) # Request the quality info from quality API_URL q_content &lt;- content(q_req) # Retrieve the content of request q_response &lt;- toJSON(q_content, auto_unbox = TRUE) # Convert the info to JSON object remove(params, q_req, q_content) # Remove the variables that are not needed prettify(q_response) # Print the prettified quality information ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B10&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B11&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B12&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;ASTGTM_NC.003&quot;, ## &quot;Layer&quot;: &quot;ASTER_GDEM_DEM&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;ASTGTM_NUMNC.003&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;ASTER_GDEM_NUM&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;Layer&quot;: &quot;SRB1&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;Layer&quot;: &quot;SRB2&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## } ## ] ## 7.18.1 List Quality Layers This API call will list all of the quality layer information for a product. For more information visit AppEEARS API documentation productAndVersion &lt;- &#39;MCD15A3H.006&#39; # Assign ProductAndVersion to a variable # Request the quality info from quality API for a specific product MCD15A3H_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion)) MCD15A3H_content &lt;- content(MCD15A3H_req) # Retrieve the content of request MCD15A3H_quality &lt;- toJSON(MCD15A3H_content, auto_unbox = TRUE)# Convert the info to JSON object remove(MCD15A3H_req, MCD15A3H_content) # Remove the variables that are not needed anymore prettify(MCD15A3H_quality) # Print the prettified quality information ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Fpar_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;FparStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Lai_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;LaiStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## } ## ] ## 7.18.2 Inspect Quality Values This API call will list all of the values for a given quality layer. quality_layer &lt;- &#39;FparLai_QC&#39; # assign a quality layer to a variable # Request the specified quality layer info from quality API quality_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, sep = &quot;&quot;)) quality_content &lt;- content(quality_req) # Retrieve the content of request quality_response &lt;- toJSON(quality_content, auto_unbox = TRUE) # Convert the info to JSON object remove(quality_req, quality_content) # Remove the variables that are not needed prettify(quality_response) # Print the quality response as a data frame ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Good quality (main algorithm with or without saturation)&quot;, ## &quot;Acceptable&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot;, ## &quot;Acceptable&quot;: false ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Terra&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Aqua&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Dead detectors caused &gt;50% adjacent detector retrieval&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Significant clouds NOT present (clear)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Significant clouds WERE present&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Mixed cloud present in pixel&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Cloud state not defined, assumed clear&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Main (RT) method used with saturation. Good, very usable&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Main (RT) method failed due to bad geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Main (RT) method failed due to problems other than geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 4, ## &quot;Description&quot;: &quot;Pixel not produced at all, value couldn&#39;t be retrieved (possible reasons: bad L1B data, unusable MOD09GA data)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## } ## ] ## 7.19 Decode Quality Values This API call will decode the bits for a given quality value. quality_value &lt;- 1 # Assign a quality value to a variable # Request and retrieve information for provided quality value from quality API URL response &lt;- content(GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, &quot;/&quot;, quality_value))) q_response &lt;- toJSON(response, auto_unbox = TRUE) # Convert the info to JSON object remove(response) # Remove the variables that are not needed anymore prettify(q_response) # Print the prettified response ## { ## &quot;Binary Representation&quot;: &quot;0b00000001&quot;, ## &quot;MODLAND&quot;: { ## &quot;bits&quot;: &quot;0b1&quot;, ## &quot;description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot; ## }, ## &quot;Sensor&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Terra&quot; ## }, ## &quot;DeadDetector&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot; ## }, ## &quot;CloudState&quot;: { ## &quot;bits&quot;: &quot;0b00&quot;, ## &quot;description&quot;: &quot;Significant clouds NOT present (clear)&quot; ## }, ## &quot;SCF_QC&quot;: { ## &quot;bits&quot;: &quot;0b000&quot;, ## &quot;description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot; ## } ## } ## 7.20 Load Request Output and Visualize Here, load the CSV file containing the results from your request using readr package, and create some basic visualizations using the ggplot2 package. 7.21 Load a CSV Use the readr package to load the CSV file containing the results from the AppEEARS request. # Make a list of csv files in the output directory files &lt;- list.files(outDir, pattern = &quot;\\\\MOD11A2-006-results.csv$&quot;) # Read the MOD11A2 results df &lt;- read_csv(paste0(outDir,&quot;/&quot;, files)) Select the MOD11A2.006 LST Day column for the data from SOAP using the dplyr package. lst_SOAP &lt;- df %&gt;% # Filter df filter(Category == &quot;SOAP&quot;) %&gt;% # Select desired columns select(Latitude, Longitude, Date ,MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km) Extract information for LST_DAY_1KM from MOD11_response of product service call from earlier in the tutorial. #fromJSON(MOD11_response)$LST_Day_1km # Extract all the info for LST_Day_1km layer fillValue &lt;- fromJSON(MOD11_response)$LST_Day_1km$FillValue # Assign fill value to a variable unit &lt;- fromJSON(MOD11_response)$LST_Day_1km$Units # Assign unit to a variable sprintf(&quot;Fill value for LST_DAY_1KM is: %i&quot; ,fillValue) # Print LST_DAY_1KM fill value ## [1] &quot;Fill value for LST_DAY_1KM is: 0&quot; sprintf(&quot;Unit for LST_DAY_1KM is: %s&quot; ,unit) # Print LST_DAY_1KM unit ## [1] &quot;Unit for LST_DAY_1KM is: Kelvin&quot; 7.22 Plot Results (Line/Scatter Plots) Next, plot a time series of daytime LST for the selected point. Below, filter the LST data to exclude fill values. lst_SOAP &lt;- lst_SOAP %&gt;% # exclude NoData filter(MOD11A2_006_LST_Day_1km != fillValue)%&gt;% filter(MOD11A2_006_LST_Night_1km != fillValue) Next, plot LST Day as a time series with some additional formatting using ggplot2. ggplot(lst_SOAP)+ geom_line(aes(x= Date, y = MOD11A2_006_LST_Day_1km), size=1, color=&quot;blue&quot;)+ geom_point(aes(x= Date, y = MOD11A2_006_LST_Day_1km), shape=18 , size = 3, color=&quot;blue&quot;)+ labs(title = &quot;Time Series&quot;, x = &quot;Date&quot;, y = sprintf( &quot;LST_Day_1km (%s)&quot;, unit))+ scale_x_date(date_breaks = &quot;16 day&quot;)+ scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(2.5),hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 315 , size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 0, size = 10) ) Using the tidyr package, the LST Day and Night values for Grand Canyon NP are being gathered in a single column to be used to make a plot including both LST_Day_1km and LST_Night_1km. lst_SOAP_DN &lt;- tidyr::gather(lst_SOAP, key = Tstat , value = LST, MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km) lst_SOAP_DN[1:5,] # print the five first observations ## # A tibble: 5 × 5 ## Latitude Longitude Date Tstat LST ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 37.0 -119. 2019-12-27 MOD11A2_006_LST_Day_1km 278. ## 2 37.0 -119. 2020-01-01 MOD11A2_006_LST_Day_1km 283. ## 3 37.0 -119. 2020-01-09 MOD11A2_006_LST_Day_1km 280. ## 4 37.0 -119. 2020-01-17 MOD11A2_006_LST_Day_1km 283. ## 5 37.0 -119. 2020-01-25 MOD11A2_006_LST_Day_1km 285. Next, plot LST Day and Night as a time series with some additional formatting. ggplot(lst_SOAP_DN)+ geom_line(aes(x= Date, y = LST, color = Tstat), size=1)+ geom_point(aes(x= Date, y = LST, color = Tstat), shape=18 , size = 3)+ scale_fill_manual(values=c(&quot;red&quot;, &quot;blue&quot;))+ scale_color_manual(values=c(&#39;red&#39;,&#39;blue&#39;))+ labs(title = &quot;Time Series&quot;, x = &quot;Date&quot;, y = sprintf( &quot;LST_Day_1km (%s)&quot;,unit))+ scale_x_date(date_breaks = &quot;16 day&quot;)+ scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(2.5), hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 315 , size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 0, size = 10), legend.position = &quot;bottom&quot;, legend.title = element_blank() ) Finally, bring in the daytime LST data from SJER , and compare with daytime LST at SOAP , shown below in a scatterplot using ggplot2 package. Here, the dplyr is used to extract the LST_DAY_1km for SJER. lst_SJER &lt;- df %&gt;% filter(MOD11A2_006_LST_Day_1km != fillValue) %&gt;% # Filter fill value filter(Category == &quot;SJER&quot;)%&gt;% # Filter SJER national park select(Date, MOD11A2_006_LST_Day_1km) # Select desired columns Make a scatterplot. ggplot()+ geom_point(aes(x=lst_SOAP$MOD11A2_006_LST_Day_1km, y=lst_SJER$MOD11A2_006_LST_Day_1km[-11]), shape=18 , size = 3, color=&quot;blue&quot;)+ labs(title = &quot;MODIS LST: SOAP vs. SJER, 2020&quot;, x = sprintf(&quot;SOAP: LST_Day_1km (%s)&quot;,unit), y = sprintf( &quot;SJER: LST_Day_1km (%s)&quot;,unit))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(1.5), hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, size = 10) ) This example can provide a template to use for your own research workflows. Leveraging the AppEEARS API for searching, extracting, and formatting analysis ready data, and loading it directly into R means that you can keep your entire research workflow in a single software program, from start to finish. 7.23 Submit an Area Request The following section was adapted from LPDAACs E-Learning Tutorials on the AppEEARS API and modified to request data for NEON sites. Contributing Authors: Contact Information Material written by Mahsa Jami1 and Cole Krehbiel1 Contact: LPDAAC@usgs.gov Voice: +1-866-573-3222 Organization: Land Processes Distributed Active Archive Center (LP DAAC) Website: https://lpdaac.usgs.gov/ Date last modified: 06-12-2020 1 KBR, Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Work performed under USGS contract G15PD00467 for LP DAAC2. 2 LP DAAC Work performed under NASA contract NNG14HH33I. This tutorial demonstrates how to use R to connect to the AppEEARS API The Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The AppEEARS API offers users programmatic access to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial. 7.24 Submit an area request using a NEON site boundary as the region of interest for extracting elevation, vegetation and land surface temperature data In this section: * Connecting to the AppEEARS API, * Querying the list of available products, * Submitting an area sample request, downloading the request, * Working with the AppEEARS Quality API, and * Loading the results into R for visualization. AppEEARS area sample requests allow users to subset their desired data by spatial area via vector polygons (shapefiles or GeoJSONs). Users can also reproject and reformat the output data. AppEEARS returns the valid data from the parameters defined within the sample request. 7.24.0.1 Data Used in this Example: Data layers: NASA MEaSUREs Shuttle Radar Topography Mission (SRTM) Version 3 Digital Elevation Model SRTMGL1_NC.003, 30m, static: SRTM_DEM Combined MODIS Leaf Area Index (LAI) MCD15A3H.006, 500m, 4 day: Lai_500m Terra MODIS Land Surface Temperature MOD11A2.006, 1000m, 8 day: LST_Day_1km, LST_Night_1km 7.24.1 Topics Covered in this Tutorial Getting Started * Load Packages * Login Query Available Products * Search and Explore Available Products * Search and Explore Available Layers Submit an Area Request * Load a Shapefile * Search and Explore Available Projections * Compile a JSON Object * Submit a Task Request * Retrieve Task Status Download a Request * Explore Files in Request Output * Download Files in a Request (Automation) Explore AppEEARS Quality Service * List Quality Layers * Show Quality Values * Decode Quality Values BONUS: Load Request Output and Visualize * Load a GeoTIFF * Plot a GeoTIFF 7.24.2 Prerequisites: A NASA Earthdata Login account is required to login to the AppEEARS API and submit a request . You can create an account at the link provided. Install R and RStudio. These tutorials have been tested on Windows and Mac systems using R Version 4.0.0, RStudio version 1.1.463, and the specifications listed below. Required packages: getPass httr jsonlite geojsonio geojsonR rgdal sp warnings To read and visualize the GeoTIFF: raster rasterVis RColorBrewer ggplot2 7.24.3 Procedures: 7.24.3.1 Getting Started: Clone/download AppEEARS API Getting Started in R Repository from the LP DAAC Data User Resources Repository or use the code in this textbook. Open the AppEEARS_API_R.Rproj file to directly open the project. Next, select the AppEEARS_API_AREA_R.Rmd from the files list and open it. 7.24.4 AppEEARS Information: To access AppEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/. For comprehensive documentation of the full functionality of the AppEEARS API, please see the AppEEARS API Documentation. 7.25 Getting Started with an Area Request 7.25.1 Load Packages for an Area Request First, load the R packages necessary to run the tutorial. # Load necessary packages into R library(httr) # To send a request to the server/receive a response from the server library(jsonlite) # Implements a bidirectional mapping between JSON data and the most important R data types library(geojsonio) # Convert data from various R classes to &#39;GeoJSON&#39; library(geojsonR) # Functions for processing GeoJSON objects library(rgdal) # Functions for spatial data input/output library(sp) # classes and methods for spatial data types library(raster) # Classes and methods for raster data library(rasterVis) # Advanced plotting functions for raster objects library(ggplot2) # Functions for graphing and mapping library(RColorBrewer) # Creates nice color schemes 7.26 Set Up the Output Directory Create an output directory for the results. outDir &lt;- &#39;./data/&#39; # Create an output directory if it doesn&#39;t exist Next, assign the AppEEARS API URL to a static variable. API_URL = &#39;https://lpdaacsvc.cr.usgs.gov/appeears/api/&#39; # Set the AppEEARS API to a variable 7.27 Load your Earth Data Token To submit a request, you must first login to the AppEEARS API. Use your token script to import your credentials. source(&#39;EARTHDATA_Token.R&#39;) #path will change based on where you stored it Decode the username and password to be used to post login request. secret &lt;- jsonlite::base64_enc(paste(user, password, sep = &quot;:&quot;)) # Encode the string of username and password Next, assign the AppEEARS API URL to a static variable. API_URL = &#39;https://appeears.earthdatacloud.nasa.gov/api/&#39; # Set the AppEEARS API to a variable Use the httr package to post your username and password. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the API Documentation. # Insert API URL, call login service, set the component of HTTP header, and post the request to the server response &lt;- httr::POST(paste0(API_URL,&quot;login&quot;), add_headers(&quot;Authorization&quot; = paste(&quot;Basic&quot;, gsub(&quot;\\n&quot;, &quot;&quot;, secret)), &quot;Content-Type&quot; = &quot;application/x-www-form-urlencoded;charset=UTF-8&quot;), body = &quot;grant_type=client_credentials&quot;) response_content &lt;- content(response) # Retrieve the content of the request token_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the response to the JSON object remove(user, password, secret, response) # Remove the variables that are not needed anymore prettify(token_response) # Print the prettified response ## { ## &quot;token_type&quot;: &quot;Bearer&quot;, ## &quot;token&quot;: &quot;3XvrYjK2H8Q4vCMa3OaW712gYBtzUIVXMpE-qN4DDk0dCDSRP82pfAQExZ91nQ-cvnG_UQQaI3_K5bcfTh5iTw&quot;, ## &quot;expiration&quot;: &quot;2022-09-30T22:34:33Z&quot; ## } ## Above, you should see a Bearer token. Notice that this token will expire approximately 48 hours after being acquired. 7.28 Query Available Products The product API provides details about all of the products and layers available in AppEEARS. For more information, please see the API Documentation. Below, call the product API to list all of the products available in AppEEARS. prods_req &lt;- GET(paste0(API_URL, &quot;product&quot;)) # Request the info of all products from product service prods_content &lt;- content(prods_req) # Retrieve the content of request all_Prods &lt;- toJSON(prods_content, auto_unbox = TRUE) # Convert the info to JSON object remove(prods_req, prods_content) # Remove the variables that are not needed anymore # prettify(all_Prods) # Print the prettified product response 7.29 Search and Explore Available Products Create a list indexed by product name to make it easier to query a specific product. # divides information from each product. divided_products &lt;- split(fromJSON(all_Prods), seq(nrow(fromJSON(all_Prods)))) # Create a list indexed by the product name and version products &lt;- setNames(divided_products,fromJSON(all_Prods)$ProductAndVersion) # Print no. products available in AppEEARS sprintf(&quot;AppEEARS currently supports %i products.&quot; ,length(products)) ## [1] &quot;AppEEARS currently supports 162 products.&quot; Next, look at the products names and descriptions. Below, the ProductAndVersion and Description are printed for all products. # Loop through the products in the list and Print the product name and description for (p in products){ print(paste0(p$ProductAndVersion,&quot; is &quot;,p$Description,&quot; from &quot;,p$Source)) } ## [1] &quot;GPW_DataQualityInd.411 is Quality of Input Data for Population Count and Density Grids from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopCount.411 is UN-adjusted Population Count from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopDensity.411 is UN-adjusted Population Density from SEDAC&quot; ## [1] &quot;MCD12Q1.006 is Land Cover Type from LP DAAC&quot; ## [1] &quot;MCD12Q2.006 is Land Cover Dynamics from LP DAAC&quot; ## [1] &quot;MCD12Q1.061 is Land Cover Type from LP DAAC&quot; ## [1] &quot;MCD12Q2.061 is Land Cover Dynamics from LP DAAC&quot; ## [1] &quot;MCD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A3H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A3H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD43A1.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A1.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A2.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A3.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A3.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A4.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A4.061 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD64A1.006 is Burned Area (fire) from LP DAAC&quot; ## [1] &quot;MCD64A1.061 is Burned Area (fire) from LP DAAC&quot; ## [1] &quot;MOD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09A1.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GA.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09GQ.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09Q1.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MOD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MOD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A1.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A2.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A3.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13Q1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MOD14A2.061 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MOD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MOD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MOD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2.061 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A3GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2H.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2HGF.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2HGF.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MOD17A3HGF.061 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MOD21A1D.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD21A1N.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD21A2.061 is Temperature and Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD44B.006 is Vegetation Continuous Fields (VCF) from LP DAAC&quot; ## [1] &quot;MOD44W.006 is Land/Water Mask from LP DAAC&quot; ## [1] &quot;MODOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MODTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;MYD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09A1.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GA.061 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09GQ.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09Q1.061 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MYD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MYD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A1.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A2.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A3.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13Q1.061 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MYD14A2.061 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MYD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MYD15A2H.061 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MYD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2.061 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A3GF.061 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MYD17A2H.061 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MYD17A2HGF.006 is Gross Primary Productivity (GPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A2HGF.061 is Gross Primary Productivity (GPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A3HGF.061 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD21A1D.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1D.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1N.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1N.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A2.061 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYDOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MYDTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;NASADEM_NC.001 is Elevation from LP DAAC&quot; ## [1] &quot;NASADEM_NUMNC.001 is Source from LP DAAC&quot; ## [1] &quot;SPL3SMP_E.005 is Enhanced L3 Radiometer Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3SMP.008 is Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL4CMDL.006 is Carbon Net Ecosystem Exchange from NSIDC DAAC&quot; ## [1] &quot;SPL4SMGP.006 is Surface and Root Zone Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3FTP.003 is Freeze/Thaw State from NSIDC DAAC&quot; ## [1] &quot;SRTMGL1_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL1_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;ASTGTM_NC.003 is Elevation from LP DAAC&quot; ## [1] &quot;ASTGTM_NUMNC.003 is Source from LP DAAC&quot; ## [1] &quot;ASTWBD_ATTNC.001 is Water Bodies Database Attributes from LP DAAC&quot; ## [1] &quot;ASTWBD_NC.001 is Water Bodies Database Elevation from LP DAAC&quot; ## [1] &quot;VNP09H1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP09A1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP09GA.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP13A1.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A2.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A3.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP14A1.001 is Thermal Anomalies/Fire from LP DAAC&quot; ## [1] &quot;VNP15A2H.001 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;VNP21A1D.001 is Land Surface Temperature &amp; Emissivity Day (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A1N.001 is Land Surface Temperature &amp; Emissivity Night (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A2.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP22Q2.001 is Global Land Surface Phenology (GLSP) from LP DAAC&quot; ## [1] &quot;VNP43IA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43IA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43IA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43IA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;VNP43MA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43MA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43MA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43MA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;DAYMET.004 is Daily Surface Weather Data for North America from ORNL&quot; ## [1] &quot;ECO2LSTE.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;ECO2CLD.001 is Cloud Mask from LP DAAC&quot; ## [1] &quot;ECO3ETPTJPL.001 is Evapotranspiration PT-JPL from LP DAAC&quot; ## [1] &quot;ECO3ANCQA.001 is L3/L4 Ancillary Data Quality Assurance (QA) Flags from LP DAAC&quot; ## [1] &quot;ECO4ESIPTJPL.001 is Evaporative Stress Index PT-JPL from LP DAAC&quot; ## [1] &quot;ECO4WUE.001 is Water Use Efficiency from LP DAAC&quot; ## [1] &quot;ECO1BGEO.001 is Geolocation from LP DAAC&quot; ## [1] &quot;ECO1BMAPRAD.001 is Resampled Radiance from LP DAAC&quot; ## [1] &quot;ECO3ETALEXI.001 is Evapotranspiration dis-ALEXI from LP DAAC&quot; ## [1] &quot;ECO4ESIALEXI.001 is Evaporative Stress Index dis-ALEXI from LP DAAC&quot; ## [1] &quot;ECO_L1B_GEO.002 is Geolocation from LP DAAC&quot; ## [1] &quot;ECO_L2_CLOUD.002 is Cloud Mask Instantaneous from LP DAAC&quot; ## [1] &quot;ECO_L2_LSTE.002 is Swath Land Surface Temperature and Emissivity Instantaneous from LP DAAC&quot; ## [1] &quot;HLSS30.020 is Land Surface Reflectance from LP DAAC&quot; ## [1] &quot;HLSL30.020 is Land Surface Reflectance from LP DAAC&quot; The product service provides many useful details, including if a product is currently available in AppEEARS, a description, and information on the spatial and temporal resolution. Below, the product details are retrieved using ProductAndVersion. # Convert the MCD15A3H.006 info to JSON object and print the prettified info prettify(toJSON(products$&quot;MCD15A3H.006&quot;)) ## [ ## { ## &quot;Product&quot;: &quot;MCD15A3H&quot;, ## &quot;Platform&quot;: &quot;Combined MODIS&quot;, ## &quot;Description&quot;: &quot;Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR)&quot;, ## &quot;RasterType&quot;: &quot;Tile&quot;, ## &quot;Resolution&quot;: &quot;500m&quot;, ## &quot;TemporalGranularity&quot;: &quot;4 day&quot;, ## &quot;Version&quot;: &quot;006&quot;, ## &quot;Available&quot;: true, ## &quot;DocLink&quot;: &quot;https://doi.org/10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;Source&quot;: &quot;LP DAAC&quot;, ## &quot;TemporalExtentStart&quot;: &quot;2002-07-04&quot;, ## &quot;TemporalExtentEnd&quot;: &quot;Present&quot;, ## &quot;Deleted&quot;: false, ## &quot;DOI&quot;: &quot;10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## Below, search for products containing Leaf Area Index in their description. LAI_Products &lt;- list() # Create an empty list for (p in products){ # Loop through the product list if (grepl(&#39;Leaf Area Index&#39;, p$Description )){ # Look through the product description for a keyword LAI_Products &lt;- append(LAI_Products, p$ProductAndVersion) # Append the LAI products to the list } } LAI_Products ## [[1]] ## [1] &quot;MCD15A2H.006&quot; ## ## [[2]] ## [1] &quot;MCD15A2H.061&quot; ## ## [[3]] ## [1] &quot;MCD15A3H.006&quot; ## ## [[4]] ## [1] &quot;MCD15A3H.061&quot; ## ## [[5]] ## [1] &quot;MOD15A2H.006&quot; ## ## [[6]] ## [1] &quot;MOD15A2H.061&quot; ## ## [[7]] ## [1] &quot;MYD15A2H.006&quot; ## ## [[8]] ## [1] &quot;MYD15A2H.061&quot; ## ## [[9]] ## [1] &quot;VNP15A2H.001&quot; Using the info above, Create a list of desired products. desired_products &lt;- c(&#39;MCD15A3H.006&#39;,&#39;MOD11A2.006&#39;,&#39;SRTMGL1_NC.003&#39;) # Create a list of desired products desired_products ## [1] &quot;MCD15A3H.006&quot; &quot;MOD11A2.006&quot; &quot;SRTMGL1_NC.003&quot; 7.30 Search and Explore Available Layers This API call will list all of the layers available for a given product. Each product is referenced by its ProductAndVersion property which is also referred to as the product_id. First, request the layers for the MCD15A3H.006 product. # Request layers for the 1st product in the list: MCD15A3H.006 MCD15A3H_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[1])) # Request the info of a product from product URL MCD15A3H_content &lt;- content(MCD15A3H_req) # Retrieve content of the request MCD15A3H_response &lt;- toJSON(MCD15A3H_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MCD15A3H_req, MCD15A3H_content) # Remove the variables that are not needed anymore #prettify(MCD15A3H_response) # Print the prettified response names(fromJSON(MCD15A3H_response)) # print the layer&#39;s names ## [1] &quot;FparExtra_QC&quot; &quot;FparLai_QC&quot; &quot;FparStdDev_500m&quot; &quot;Fpar_500m&quot; ## [5] &quot;LaiStdDev_500m&quot; &quot;Lai_500m&quot; Next, request the layers for the MOD11A2.006 product. MOD11_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[2])) # Request the info of a product from product URL MOD11_content &lt;- content(MOD11_req) # Retrieve content of the request MOD11_response &lt;- toJSON(MOD11_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MOD11_req, MOD11_content) # Remove the variables that are not needed anymore names(fromJSON(MOD11_response)) # print the layer names ## [1] &quot;Clear_sky_days&quot; &quot;Clear_sky_nights&quot; &quot;Day_view_angl&quot; &quot;Day_view_time&quot; ## [5] &quot;Emis_31&quot; &quot;Emis_32&quot; &quot;LST_Day_1km&quot; &quot;LST_Night_1km&quot; ## [9] &quot;Night_view_angl&quot; &quot;Night_view_time&quot; &quot;QC_Day&quot; &quot;QC_Night&quot; Next, request the layers for theSRTMGL1_NC.003 product. SRTMGL1_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[3]))# Request the info of a product from product URL SRTMGL1_content &lt;- content(SRTMGL1_req) # Retrieve content of the request SRTMGL1_response &lt;- toJSON(SRTMGL1_content, auto_unbox = TRUE) # Convert the content to JSON object remove(SRTMGL1_req, SRTMGL1_content) # Remove the variables that are not needed anymore names(fromJSON(SRTMGL1_response)) ## [1] &quot;SRTMGL1_DEM&quot; Lastly, select the desired layers and pertinent products and make a data frame using this information. This list will be inserted into the JSON file used to submit a request. # Create a vector of desired layers desired_layers &lt;- c(&quot;LST_Day_1km&quot;,&quot;LST_Night_1km&quot;,&quot;Lai_500m&quot;,&quot;SRTMGL1_DEM&quot;) # Create a vector of products including the desired layers desired_prods &lt;- c(&quot;MOD11A2.006&quot;,&quot;MOD11A2.006&quot;,&quot;MCD15A3H.006&quot;,&quot;SRTMGL1_NC.003&quot;) # Create a data frame including the desired data products and layers layers &lt;- data.frame(product = desired_prods, layer = desired_layers) 7.31 Submit an Area Request The Submit task API call provides a way to submit a new request to be processed. It can accept data via JSON or query string. In the example below, create a JSON object and submit a request. Tasks in AppEEARS correspond to each request associated with your user account. Therefore, each of the calls to this service requires an authentication token. 7.31.1 Load a Shapefile In this section, begin by loading a shapefile using the rgdal package. The shapefile is publicly available for download from the NPS website. onaq &lt;- readOGR(dsn= &quot;./ONAQ_extent&quot;, layer = &quot;D15_ONAQ_C1_P1_v2&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Users\\rohan\\katharynduffy.github.io\\ONAQ_extent&quot;, layer: &quot;D15_ONAQ_C1_P1_v2&quot; ## with 1 features ## It has 1 fields onaq_wgs84 &lt;- spTransform(onaq, CRS(&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;)) convert the Spatial data frame to GeoJSON using geojson_json from geojsonio package, which makes it easy to create the geospatial data in and out of GeoJSON format. onaq_json &lt;- geojsonio::geojson_json(onaq_wgs84, geometry = &quot;polygon&quot;) # Convert the data frame to GeoJSON ## Warning: &#39;geojsonlint&#39; not installed, skipping GeoJSON linting Next, the GeoJSON object can be read as a list using the FROM_GeoJson function from geojsonR package. onaq_js &lt;- geojsonR::FROM_GeoJson(onaq_json) # Read the GeoJSON 7.32 Search and Explore Available Projections The spatial API provides some helper services used to support submitting area task requests. The call below will retrieve the list of supported projections in AppEEARS. For more information, please see the AppEEARS API proj_req &lt;- GET(paste0(API_URL, &quot;spatial/proj&quot;)) # Request the projection info from API_URL proj_content &lt;- content(proj_req) # Retrieve content of the request proj_response &lt;- toJSON(proj_content, auto_unbox = TRUE) # Convert the content to JSON object remove(proj_req, proj_content) # Remove the variables that are not needed projs &lt;- fromJSON(proj_response) # Read the projects as a R object projection &lt;- projs[projs$Name==&quot;geographic&quot;,] # Choose the projection for your output 7.33 Compile a JSON Object In this section, begin by setting up the information needed for a nested data list that will be later converted to a JSON object for submitting an AppEEARS area request.For detailed information on required JSON parameters, see the API Documentation. taskName &lt;- &#39;NEON-ONAQ&#39; # Enter name of the task, &#39;ONAQ&#39; used here taskType &lt;- &#39;area&#39; # Type of task, it can be either &quot;area&quot; or &quot;point&quot; projection &lt;- projection$Name # Set output projection outFormat &lt;- &#39;geotiff&#39; # Set output file format type. it can be either &#39;geotiff&#39; or &#39;netcdf4&#39; startDate &lt;- &#39;01-01-2018&#39; # Start of the date range for which to extract data: MM-DD-YYYY endDate &lt;- &#39;12-31-2019&#39; # End of the date range for which to extract data: MM-DD-YYYY recurring &lt;- FALSE # Specify True for a recurring date range #yearRange = [2000,2016] # If recurring = True, set yearRange, change start/end date to MM-DD To be able to successfully submit a task, the JSON object should be structured in a certain way. The code chunk below uses the information from the previous chunk to create a nested data frame. This nested data frame will be converted to JSON object that can be used to complete the request. # Create a data frame including the date range for the request date &lt;- data.frame(startDate = startDate, endDate = endDate) # Create a list including the projection and add the output format info out &lt;- list(projection ) names(out) &lt;- c(&quot;projection&quot;) out$format$type &lt;- outFormat # Change the GeoJSON format for successful task submission onaq_js$features[[1]]$geometry$coordinates &lt;- list(onaq_js$features[[1]]$geometry$coordinates) task_info &lt;- list(date, layers, out, onaq_js) # Create a list of data frames names(task_info) &lt;- c(&quot;dates&quot;, &quot;layers&quot;, &quot;output&quot;, &quot;geo&quot;) # Assign names task &lt;- list(task_info, taskName, taskType) # Create a nested list names(task) &lt;- c(&quot;params&quot;, &quot;task_name&quot;, &quot;task_type&quot;) # Assign names toJSON function from jsonlite package converts the type of data frame to a string that can be recognized as a JSON object to be submitted as an area request. task_json &lt;- jsonlite::toJSON(task,auto_unbox = TRUE, digits = 10) 7.34 Submit a Task Request Token information is needed to submit a request. Below the login token is assigned to a variable. token &lt;- paste(&quot;Bearer&quot;, fromJSON(token_response)$token) # Save login token to a variable Below, post a call to the API task service, using the task_json created above. # Post the point request to the API task service response &lt;- POST(paste0(API_URL, &quot;task&quot;), body = task_json , encode = &quot;json&quot;, add_headers(Authorization = token, &quot;Content-Type&quot; = &quot;application/json&quot;)) task_content &lt;- content(response) # Retrieve content of the request task_response &lt;- toJSON(task_content, auto_unbox = TRUE) # Convert the content to JSON and prettify it prettify(task_response) # Print the task response A JSON downloaded from a previous request containing the information from that request can also be used if available # task &lt;- jsonlite::toJSON(jsonlite::read_json(&quot;LST-request.json&quot;), digits = 10, auto_unbox = TRUE) # response &lt;- POST(paste0(API_URL,&quot;task&quot;), body = task, encode = &quot;json&quot;, # add_headers(Authorization = token, &quot;Content-Type&quot; = &quot;application/json&quot;)) # task_response &lt;- prettify(toJSON(content(response), auto_unbox = TRUE)) # task_response 7.35 Retrieve Task Status This API call will list all of the requests associated with your user account, automatically sorted by date, with the most recent requests listed first. The AppEEARS API contains some helpful formatting resources. Below, limit the API response to 2 entries for the last 2 requests and set pretty to True to format the response as an organized JSON, making it easier to read. Additional information on AppEEARS API retrieve task, pagination, and formatting can be found in the API documentation. params &lt;- list(limit = 2, pretty = TRUE) # Set up query parameters # Request the task status of last 2 requests from task URL response_req &lt;- GET(paste0(API_URL,&quot;task&quot;), query = params, add_headers(Authorization = token)) response_content &lt;- content(response_req) # Retrieve content of the request status_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the content to JSON object remove(response_req, response_content) # Remove the variables that are not needed anymore prettify(status_response) # Print the prettified response ## [ ## { ## &quot;error&quot;: { ## ## }, ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;done&quot;, ## &quot;created&quot;: &quot;2022-09-29T00:01:49.833402&quot;, ## &quot;task_id&quot;: &quot;f6cd3e97-1424-4401-9440-434a19b53c5c&quot;, ## &quot;updated&quot;: &quot;2022-09-29T00:10:11.803810&quot;, ## &quot;user_id&quot;: &quot;rdb273@nau.edu&quot;, ## &quot;attempts&quot;: 1, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;retry_at&quot;: { ## ## }, ## &quot;completed&quot;: &quot;2022-09-29T00:10:11.792231&quot;, ## &quot;has_swath&quot;: false, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;3.12&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;size_category&quot;: &quot;0&quot;, ## &quot;has_nsidc_daac&quot;: false, ## &quot;expires_on&quot;: &quot;2022-11-28T00:10:11.803810&quot; ## }, ## { ## &quot;error&quot;: { ## ## }, ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;done&quot;, ## &quot;created&quot;: &quot;2022-09-29T00:00:00.370981&quot;, ## &quot;task_id&quot;: &quot;29c96457-4900-4631-8313-c41337ef3fbc&quot;, ## &quot;updated&quot;: &quot;2022-09-29T00:06:11.906162&quot;, ## &quot;user_id&quot;: &quot;rdb273@nau.edu&quot;, ## &quot;attempts&quot;: 1, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;retry_at&quot;: { ## ## }, ## &quot;completed&quot;: &quot;2022-09-29T00:06:11.894539&quot;, ## &quot;has_swath&quot;: false, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;3.12&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;size_category&quot;: &quot;0&quot;, ## &quot;has_nsidc_daac&quot;: false, ## &quot;expires_on&quot;: &quot;2022-11-28T00:06:11.906162&quot; ## } ## ] ## The task_id that was generated when submitting your request can also be used to retrieve a task status. Notice, this could get longer because of the number of vertices in the shapefile that will be printed. # task_id &lt;- fromJSON(task_response)$task_id # Extract the task_id of submitted point request # # Request the task status of a task with the provided task_id from task URL # status_req &lt;- GET(paste0(API_URL,&quot;task/&quot;, task_id), add_headers(Authorization = token)) # status_content &lt;- content(status_req) # Retrieve content of the request # statusResponse &lt;-toJSON(status_content, auto_unbox = TRUE) # Convert the content to JSON object # remove(status_req, status_content) # Remove the variables that are not needed # prettify(statusResponse) # Print the prettified response Retrieve the task status every 60 seconds. The task status should be done to be able to download the output. # Request the task status of last request from task URL stat_req &lt;- GET(paste0(API_URL,&quot;task&quot;), query = list(limit = 1), add_headers(Authorization = token)) stat_content &lt;- content(stat_req) # Retrieve content of the request stat_response &lt;- toJSON(stat_content, auto_unbox = TRUE) # Convert the content to JSON object stat &lt;- fromJSON(stat_response)$status # Assign the task status to a variable remove(stat_req, stat_content) # Remove the variables that are not needed 7.36 Download a Request 7.36.1 Explore Files in Request Output Before downloading the request output, examine the files contained in the request output. task_id &lt;- fromJSON(task_response)[[1]] response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id), add_headers(Authorization = token)) bundle_response &lt;- prettify(toJSON(content(response), auto_unbox = TRUE)) 7.37 Download Files in a Request (Automation) The bundle API provides information about completed tasks. For any completed task, a bundle can be queried to return the files contained as a part of the task request. Below, call the bundle API and return all of the output files. Next, read the contents of the bundle in JSON format and loop through file_id to automate downloading all of the output files into the output directory. For more information, please see AppEEARS API Documentation. bundle &lt;- fromJSON(bundle_response)$files for (id in bundle$file_id){ # retrieve the filename from the file_id filename &lt;- bundle[bundle$file_id == id,]$file_name # create a destination directory to store the file in filepath &lt;- paste(outDir,filename, sep = &quot;/&quot;) suppressWarnings(dir.create(dirname(filepath))) # write the file to disk using the destination directory and file name response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id, &quot;/&quot;, id), write_disk(filepath, overwrite = TRUE), progress(), add_headers(Authorization = token)) } Now, take a look at R_output directory. Separate folders are made for each product and the outputs are saved in these folders. # List of directories in the R_output directory list.dirs(outDir) ## [1] &quot;./data/&quot; ## [2] &quot;./data//__MACOSX&quot; ## [3] &quot;./data//__MACOSX/All_NEON_TOS_Plots_V8&quot; ## [4] &quot;./data//All_NEON_TOS_Plots_V8&quot; ## [5] &quot;./data//AOPTerrestrial&quot; ## [6] &quot;./data//DP3.30015.001&quot; ## [7] &quot;./data//DP3.30015.001/neon-aop-products&quot; ## [8] &quot;./data//DP3.30015.001/neon-aop-products/2017&quot; ## [9] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite&quot; ## [10] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16&quot; ## [11] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1&quot; ## [12] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/L3&quot; ## [13] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar&quot; ## [14] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif&quot; ## [15] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata&quot; ## [16] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar&quot; ## [17] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary&quot; ## [18] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/kmls&quot; ## [19] &quot;./data//DP3.30015.001/neon-aop-products/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps&quot; ## [20] &quot;./data//filesToStack00200&quot; ## [21] &quot;./data//pointreyes&quot; ## [22] &quot;./data//pointreyesclear&quot; ## [23] &quot;./data//pointreyesfoggy&quot; ## [24] &quot;./data//SJER&quot; ## [25] &quot;./data//SOAP&quot; Below, the list of relative path and file names is assigned to a variable and part of the list is printed. relative_path &lt;- bundle$file_name # Assign relative path to a variable Later in this tutorial, the SRTMGL1_NC GeoTIFF is loaded for visulalization. Below, the directory to this file is assigned to a variable. SRTMGL1_NC_dir &lt;- (&#39;./data/NASADEM_NC.001_NASADEM_HGT_doy2000042_aid0001.tif&#39;) # Assign absolute path to a variable SRTMGL1_NC_dir # Print the absolute path ## [1] &quot;./data/NASADEM_NC.001_NASADEM_HGT_doy2000042_aid0001.tif&quot; 7.38 Explore AppEEARS Quality Service The quality API provides quality details about all of the data products available in AppEEARS. Below are examples of how to query the quality API for listing quality products, layers, and values. The final example demonstrates how AppEEARS quality services can be leveraged to decode pertinent quality values for your data. For more information visit AppEEARS API documentation First, reset pagination to include offset which allows you to set the number of results to skip before starting to return entries. Next, make a call to list all of the data product layers and the associated quality product and layer information. params &lt;- list(limit = 6, offset = 20, pretty = TRUE) # Assign query to a variable # Request the specified quality layer info from quality API quality_req &lt;- GET(paste0(API_URL, &quot;quality&quot;), query = params) quality_content &lt;- content(quality_req) # Retrieve the content of request quality_response &lt;- toJSON(quality_content, auto_unbox = TRUE) # Convert the info to JSON object remove(quality_req, quality_content) # Remove the variables that are not needed prettify(quality_response) ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B10&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B11&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;Layer&quot;: &quot;B12&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;HLSS30.020&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;Fmask&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;ASTGTM_NC.003&quot;, ## &quot;Layer&quot;: &quot;ASTER_GDEM_DEM&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;ASTGTM_NUMNC.003&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;ASTER_GDEM_NUM&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;Layer&quot;: &quot;SRB1&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;Layer&quot;: &quot;SRB2&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LC08.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ], ## &quot;Continuous&quot;: false, ## &quot;VisibleToWorker&quot;: true ## } ## ] ## 7.39 List Quality Layers This API call will list all of the quality layer information for a product. For more information visit AppEEARS API documentation productAndVersion &lt;- &#39;MCD15A3H.006&#39; # Assign productAndVersion to a variable MCD15A3H_q_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion))# Request quality info for a product MCD15A3H_q_content &lt;- content(MCD15A3H_q_req) # Retrieve the content of request MCD15A3H_quality &lt;- toJSON(MCD15A3H_q_content, auto_unbox = TRUE) # Convert the info to JSON object remove(MCD15A3H_q_req, MCD15A3H_q_content) prettify(MCD15A3H_quality) ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Fpar_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;FparStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Lai_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;LaiStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ], ## &quot;VisibleToWorker&quot;: true ## } ## ] ## 7.40 Show Quality Values This API call will list all of the values for a given quality layer. quality_layer &lt;- &#39;FparLai_QC&#39; # assign a quality layer to a variable # Request the specified quality layer info from quality API quality_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, sep = &quot;&quot;)) quality_content &lt;- content(quality_req) # Retrieve the content of request quality_response &lt;- toJSON(quality_content, auto_unbox = TRUE) # Convert the info to JSON object remove(quality_req, quality_content) # Remove the variables that are not needed prettify(quality_response) # Print the quality response as a data frame ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Good quality (main algorithm with or without saturation)&quot;, ## &quot;Acceptable&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot;, ## &quot;Acceptable&quot;: false ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Terra&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Aqua&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Dead detectors caused &gt;50% adjacent detector retrieval&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Significant clouds NOT present (clear)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Significant clouds WERE present&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Mixed cloud present in pixel&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Cloud state not defined, assumed clear&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Main (RT) method used with saturation. Good, very usable&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Main (RT) method failed due to bad geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Main (RT) method failed due to problems other than geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 4, ## &quot;Description&quot;: &quot;Pixel not produced at all, value couldn&#39;t be retrieved (possible reasons: bad L1B data, unusable MOD09GA data)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## } ## ] ## 7.41 Decode Quality Values This API call will decode the bits for a given quality value. quality_value &lt;- 1 # Assign a quality value to a variable # Request and retrieve information for provided quality value from quality API URL response &lt;- content(GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, &quot;/&quot;, quality_value))) q_response &lt;- toJSON(response, auto_unbox = TRUE) # Convert the info to JSON object remove(response) # Remove the variables that are not needed anymore prettify(q_response) # Print the prettified response ## { ## &quot;Binary Representation&quot;: &quot;0b00000001&quot;, ## &quot;MODLAND&quot;: { ## &quot;bits&quot;: &quot;0b1&quot;, ## &quot;description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot; ## }, ## &quot;Sensor&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Terra&quot; ## }, ## &quot;DeadDetector&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot; ## }, ## &quot;CloudState&quot;: { ## &quot;bits&quot;: &quot;0b00&quot;, ## &quot;description&quot;: &quot;Significant clouds NOT present (clear)&quot; ## }, ## &quot;SCF_QC&quot;: { ## &quot;bits&quot;: &quot;0b000&quot;, ## &quot;description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot; ## } ## } ## 7.42 BONUS: Load Request Output and Visualize Here, load one of the output GeoTIFFs and show some basic visualizations using the rasterVis and ggplot2 packages. 7.43 Load a GeoTIFF First, created a raster object by calling the raster() function from the raster package. # Create a raster object using the directory previously extracted dem &lt;- raster(SRTMGL1_NC_dir) 7.44 Plot a GeoTIFF Make a plot of DEM data and add some additional parameters to the plot. gplot(dem) + geom_raster(aes(fill = value)) + scale_fill_distiller(name = &quot;Elevation (m)&quot;, palette = &quot;BrBG&quot;, na.value=NA) + coord_fixed(expand = F, ratio = 1) + labs(title = &quot;SRTM DEM: NEON SOAP&quot;)+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(2),hjust = 0.5), axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_rect(fill = NA , color = &quot;black&quot;, size = 0.8), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), panel.grid.major = element_line(size = 0.001, linetype = &#39;solid&#39;,colour = &quot;gray&quot;), panel.grid.minor = element_line(size = 0.001, linetype = &#39;solid&#39;,colour = &quot;gray&quot;)) This example can provide a template to use for your own research workflows. Leveraging the AppEEARS API for searching, extracting, and formatting analysis ready data, and loading it directly into R means that you can keep your entire research workflow in a single software program, from start to finish. 7.45 NASA EOS Coding Lab #2 Choose two NEON sites in different ecoregions. Then complete the following for each of your two NEON sites: Using your Earth Data account submit a point-based request to AppEEARS to pull 250m NDVI from AQUA and TERA for 2017, 2018, &amp; 2019. Hint: How might you decide on the lat/lon to submit? Metadata for the site? NEON TOS data? Use QA/QC values to filter out poor quality. Hint: Look at the QA/QC files that accompany your request Plot 3 years of NDVI from MODIS AQUA and TERA as a timeseries. Constrain a 3-week window for peak greeness from MODIS and highlight it on your timeseries plot. Pull the canopy-level gcc90 from PhenoCam for the same site and the same time period as above. Hint: Check the numbering of your PhenoCam on the PhenoCam gallery Plot the PhenoCam and MODIS timeseries on the same plot. Constrain a 3-week window for peak greeness from PhenoCam and highlight it on your timeseries. Hint: Remember our PhenoCam discussions regarding early extra green leaves? Youll need to use some logic for this, not just a max Find the timing of the AOP flights that have occured at your sites over the same time period. Add those dates as a vertical line. BONUS: If youd like to submit an area-based request instead you can do so using the shapefiles for each NEON site in this folder 7.46 NASA EOS Written Questions How does the scientific method of Earth Science differ from that of traditional/classical labratory sciences? 7.47 NASA EOS Culmination Write Up Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using NASA EOS data. Include the types of data that you would need to implement this project and how you would retrieve them. Save this summary as you will be refining and adding to your ideas over the course of the semester. Suggestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragraph summarizing how this data or analysis is useful to you and/or the infrastructure. "],["usgs-national-water-information-system-nwis.html", "Chapter 8 USGS National Water Information System (NWIS) 8.1 USGS Mission: 8.2 USGS Water Resources Mission: 8.3 Types of USGS NWIS Data 8.4 USGS R Packages: Collaborative and reproducible data analysis using R 8.5 Introduction to USGS R Packages 8.6 Data available 8.7 Common NWIS function arguments 8.8 Discovering NWIS data 8.9 Common WQP function arguments 8.10 Discovering WQP data 8.11 readNWIS functions 8.12 Additional Features 8.13 readWQP functions 8.14 Attributes and metadata 8.15 USGS Coding Lab Exercises 8.16 geoKnife - Introduction 8.17 Lesson Summary 8.18 Lesson Objectives 8.19 Lesson Resources 8.20 Remote processing 8.21 geoknife components: fabric, stencil, knife 8.22 Available webdata 8.23 Available webgeoms 8.24 Available webprocesses 8.25 Setting up a geojob 8.26 Checking the geojob status 8.27 Getting geojob data 8.28 wait and email 8.29 Putting it all together, mapping precipitation from Tropical Storm Colin 8.30 USGS NWIS Culmination Write Up", " Chapter 8 USGS National Water Information System (NWIS) United States Geological Surveys National Water Information System (NWIS) 8.1 USGS Mission: Changes to the natural world combined with growing human demands put our health and safety, national security, and economy at risk. We are focused on some of the most significant issues society faces, and our science is making a substantial contribution to the well-being of the Nation and the world. You can explore USGS Missions and programs here. 8.2 USGS Water Resources Mission: Water information is fundamental to national and local economic well-being, protection of life and property, and effective management of the Nations water resources. The USGS works with partners to monitor, assess, conduct targeted research, and deliver information on a wide range of water resources and conditions including streamflow, groundwater, water quality, and water use and availability. The United States Geological Survey (USGS) has collected water-resources data at approximately 1.5 million sites in all 50 States, the District of Columbia, Puerto Rico, the Virgin Islands, Guam, American Samoa and the Commonwealth of the Northern Mariana Islands. A map of collection sites can be found here 8.3 Types of USGS NWIS Data The types of data collected are varied, but generally fit into the broad categories of surface water and groundwater. Surface-water data, such as gage height (stage) and streamflow (discharge), are collected at major rivers, lakes, and reservoirs. Groundwater data, such as water level, are collected at wells and springs. Water-quality data are available for both surface water and groundwater. Examples of water-quality data collected are temperature, specific conductance, pH, nutrients, pesticides, and volatile organic compounds. The NWIS web site serves current and historical data. Data are retrieved by category of data, such as surface water, groundwater, or water quality, and by geographic area. 8.4 USGS R Packages: Collaborative and reproducible data analysis using R Contributors: Jordan Read, Lindsay Carr Adapted from USGS Getting Started with USGS R Packages course materials Recently, the USGS has built a suite of software packages and tutorials for R users to to interact with their data and streamline workflows. Here we have adapted course materials from thier USGS R packages course materials written and developed by Lindsay R. Carr. The common workflow for completing the data processing pipeline is subject to human error at every step: accessing data, analyzing data, and producing final figures. Multi-site analyses are especially error-prone because the same workflow needs to be repeated many times. This course teaches a modular approach to the common data analysis workflow by building on basic R data analysis skills and leveraging existing USGS R packages that can create advanced, reproducible workflows, such as for accessing gridded climate data, analyzing high frequency water observations, and for taking full advantage of the USGS ScienceBase repository. The USGS packages covered in this course span a variety of applications: accessing web data, accessing personally stored data, and releasing data for publication. The modular workflows taught in this section will prepare researchers to create automated, robust data processing workflows through more efficient code development. Following the course, students will be capable of integrating these packages into their own scientific workflows. 8.4.1 Suggested prerequisite knowledge This course assumes a moderate to advanced knowledge of the statistical programming language R. If youre interested in using USGS packages for data analysis but have no R experience, please visit the Introduction to R curriculum available at this site. Experience using R to import, view, and summarize data Recommended: experience creating simple plots in R Recommended: familiarity with RStudio 8.4.2 Course outline Summary of Modules Module Description Duration dataRetrieval Accessing time series data. 2 hours geoknife Accessing gridded data. 1 hour Application Use the packages introduced in previous modules to create and use a robust modular workflow. 1.5 hour 8.4.3 Software requirements See the R installation instructions page for how to install/upgrade R and RStudio, add GRAN to your settings, and install some basic packages. Then, execute these lines so that you have the most up-to-date version of the packages used in this course. install.packages(c(&#39;dataRetrieval&#39;, &#39;geoknife&#39;)) 8.4.4 Lesson Summary This lesson will focus on finding and retrieving hydrologic time series data using the USGS R package, dataRetrieval. The package was created to make querying and downloading hydrologic data from the web easier, less error-prone, and reproducible. The package allows users to easily access data stored in the USGS National Water Information System (NWIS) and the multi-agency database, Water Quality Portal (WQP). NWIS only contains data collected by or for the USGS. Conversely, WQP is a database that aggregates water quality data from multiple agencies, including USGS, Environmental Protection Agency (EPA), US Department of Agriculture (USDA), and many state, tribal, and local agencies. dataRetrieval functions take user-defined arguments and construct web service calls. The web service returns the data as XML (a standard data structure), and dataRetrieval takes care of parsing that into a useable R data.frame, complete with metadata. When web services change, dataRetrieval users arent affected because the package maintainers will update the functions to handle these modifications. This is what makes dataRetrieval so user-friendly. Neither NWIS nor WQP are static databases. Users should be aware that data is constantly being added, so a query one week might return differing amounts of data from the next. For more information about NWIS, please visit waterdata.usgs.gov/nwis. For more information about WQP, visit their site www.waterqualitydata.us or read about WQP for aquatic research applications in the publication, Water quality data for national-scale aquatic research: The Water Quality Portal. 8.4.5 Lesson Objectives Learn about data available in the National Water Information System (NWIS) and Water Quality Portal (WQP). Discover how to construct your retrieval calls to get exactly what you are looking for, and access information stored as metadata in the R object. By the end of this lesson, the learner will be able to: Investigate what data are available in National Water Information System (NWIS) and Water Quality Portal (WQP) through package functions. Construct function calls to pull a variety of NWIS and WQP data. Access metadata information from retrieval call output. 8.4.6 Lesson Resources USGS publication: The dataRetrieval R package Source code: dataRetrieval on GitHub Report a bug or suggest a feature: dataRetrieval issues on GitHub USGS Presentation: dataRetrieval Tutorial 8.4.7 Lesson Slide Deck Browse the slide deck here 8.5 Introduction to USGS R Packages Before continuing with this lesson, you should make sure that the dataRetrieval package is installed and loaded. If you havent recently updated, you could reinstall the package by running install.packages('dataRetrieval') or go to the Update button in the Packages tab in RStudio. # load the dataRetrival package library(dataRetrieval) There is an overwhelming amount of data and information stored in the National Water Information System (NWIS). This lesson will attempt to give an overview of what you can access. If you need more detail on a subject or have a question that is not answered here, please visit the NWIS help system. 8.6 Data available Data types: NWIS and WQP store a lot of water information. NWIS contains streamflow, peak flow, rating curves, groundwater, and water quality data. As can be assumed from the name, WQP only contains water quality data. Time series types: the databases store water data at various reporting frequencies, and have different language to describe these. There are 3 main types: unit value, daily value, and discrete. WQP only contains discrete data. instantaneous value (sometimes called unit value) data is reported at the frequency in which it was collected, and includes real-time data. It is generally available from 2007-present. daily value data aggregated to a daily statistic (e.g. mean daily, minimum daily, or maximum daily). This is available for streamflow, groundwater levels, and water quality sensors. discrete data collected at a specific point in time, and is not a continuous time series. This includes most water quality data, groundwater levels, rating curves, surface water measurements, and peak flow. Metadata types: both NWIS and WQP contain metadata describing the site at which the data was collected (e.g. latitude, longitude, elevation, etc), and include information about the type of data being used (e.g. units, dissolved or not, collection method, etc). 8.7 Common NWIS function arguments siteNumber All NWIS data are stored based on the geographic location of the equipment used to collected the data. These are known as streamgages and they take continuous timeseries measurements for a number of water quality and quantity parameters. Streamgages are identified based on an 8-digit (surface water) or 15-digit (groundwater) code. In dataRetrieval, we refer to this as the siteNumber. Any time you use a siteNumber in dataRetrieval, make sure it is a string and not numeric. Oftentimes, NWIS sites have leading zeroes which are dropped when treated as a numeric value. parameterCd NWIS uses 5-digit codes to refer to specific data of interest called parameter codes, parameterCd in dataRetrieval. For example, you use the code 00060 to specify discharge data. If youd like to explore the full list, see the Parameter Groups Table. The package also has a built in parameter code table that you can use by executing parameterCdFile in your console. service Identifier referring to the time series frequencies explained above, or the type of data that should be returned. For more information, visit the Water Services website. instaneous = iv daily values = dv groundwater levels = gwlevels water quality = qw statistics = stat site = site startDate and endDate Strings in the format YYYY-MM-DDTHH:SS:MM, either as a date or character class. The start and end date-times are inclusive. stateCd Two character abbreviation for a US state or territory. Execute state.abb in the console to get a vector of US state abbreviations. Territories include: AS (American Samoa) GU (Guam) MP (Northern Mariana Islands) PR (Puerto Rico) VI (U.S. Virgin Islands) For more query parameters, visit NWIS service documentation. 8.8 Discovering NWIS data In some cases, users might have specific sites and data that they are pulling with dataRetrieval but what if you wanted to know what data exists in the database before trying to download it? You can use the function whatNWISdata, described below. Another option is to download the data using readNWISdata, and see the first and last available dates of that data with the arguments seriesCatalogOutput=TRUE and service=\"site\". Downloading data will be covered in the next section, readNWIS. 8.8.1 whatNWISdata whatNWISdata will return a data.frame specifying the types of data available for a specified major filter that fits your querying criteria. You can add queries by the data service, USGS parameter code, or statistics code. You need at least one major filter in order for the query to work. Major filters include siteNumber, stateCd, huc, bBox, or countyCd. In this example, lets find South Carolina stream temperature data. We specify the state, South Carolina, using the stateCd argument and South Carolinas two letter abbreviation, SC. data_sc &lt;- whatNWISdata(stateCd=&quot;SC&quot;) nrow(data_sc) ## [1] 203528 Lets look at the dataframe returned from whatNWISdata: head(data_sc) ## agency_cd site_no station_nm site_tp_cd dec_lat_va ## 1 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 2 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 3 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 4 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 5 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 6 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## dec_long_va coord_acy_cd dec_coord_datum_cd alt_va alt_acy_va alt_datum_cd ## 1 -78.71974 S NAD83 5.3 0.01 NAVD88 ## 2 -78.71974 S NAD83 5.3 0.01 NAVD88 ## 3 -78.71974 S NAD83 5.3 0.01 NAVD88 ## 4 -78.71974 S NAD83 5.3 0.01 NAVD88 ## 5 -78.71974 S NAD83 5.3 0.01 NAVD88 ## 6 -78.71974 S NAD83 5.3 0.01 NAVD88 ## huc_cd data_type_cd parm_cd stat_cd ts_id loc_web_ds medium_grp_cd ## 1 03040206 ad &lt;NA&gt; &lt;NA&gt; 0 &lt;NA&gt; wat ## 2 03040206 dv 00010 00001 124327 &lt;NA&gt; wat ## 3 03040206 dv 00010 00002 124328 &lt;NA&gt; wat ## 4 03040206 dv 00010 00003 124329 &lt;NA&gt; wat ## 5 03040206 dv 00045 00006 124351 &lt;NA&gt; wat ## 6 03040206 dv 00060 00001 124348 &lt;NA&gt; wat ## parm_grp_cd srs_id access_cd begin_date end_date count_nu ## 1 &lt;NA&gt; 0 0 2006-01-01 2021-01-01 16 ## 2 &lt;NA&gt; 1645597 0 2005-10-01 2022-09-28 6099 ## 3 &lt;NA&gt; 1645597 0 2005-10-01 2022-09-28 6099 ## 4 &lt;NA&gt; 1645597 0 2005-10-01 2022-09-28 6099 ## 5 &lt;NA&gt; 1644459 0 2006-01-06 2022-09-27 5876 ## 6 &lt;NA&gt; 1645423 0 2005-11-17 2019-08-17 4394 The data returned from this query can give you information about the data available for each site including, date of first and last record (begin_date, end_date), number of records (count_nu), site altitude (alt_va), corresponding hydrologic unit code (huc_cd), and parameter units (parameter_units). These columns allow even more specification of data requirements before actually downloading the data. This function returns one row per unique combination of site number, dates, parameter, etc. In order to just get the sites, use unique: sites_sc &lt;- unique(data_sc$site_no) length(sites_sc) ## [1] 10164 To be more specific, lets say we only want stream sites. This requires the siteType argument and the abbreviation ST for stream. See other siteTypes here. We also only want to use sites that have temperature data (USGS parameter code is 00010). Use the argument parameterCd and enter the code as a character string, otherwise leading zeroes will be dropped. Recall that you can see a table of all parameter codes by executing parameterCdFile in your console. data_sc_stream_temp &lt;- whatNWISdata(stateCd=&quot;SC&quot;, siteType=&quot;ST&quot;, parameterCd=&quot;00010&quot;) nrow(data_sc_stream_temp) ## [1] 683 We are now down to just 683 rows of data, much less than our original 203,528 rows. Downloading NWIS data will be covered in the next section, readNWIS. The whatNWISdata function can also be very useful for making quick maps with site locations, see the columns dec_lat_va and dec_long_va (decimal latitude and longitude value). For instance, # SC stream temperature sites library(maps) map(&#39;state&#39;, regions=&#39;south carolina&#39;) title(main=&quot;South Carolina Stream Temp Sites&quot;) points(x=data_sc_stream_temp$dec_long_va, y=data_sc_stream_temp$dec_lat_va) (#fig:sc_streamtemp_data_map)Geographic locations of NWIS South Carolina stream sites with temperature data Continuing with the South Carolina temperature data example, lets look for the mean daily stream temperature. # Average daily SC stream temperature data data_sc_stream_temp_avg &lt;- whatNWISdata( stateCd=&quot;SC&quot;, siteType=&quot;ST&quot;, parameterCd=&quot;00010&quot;, service=&quot;dv&quot;, statCd=&quot;00003&quot;) nrow(data_sc_stream_temp_avg) ## [1] 99 Lets apply an additional filter to these data using the filter function from dplyr. Imagine that the trend analysis you are conducting requires a minimum of 300 records and the most recent data needs to be no earlier than 1975. # Useable average daily SC stream temperature data library(dplyr) data_sc_stream_temp_avg_applicable &lt;- data_sc_stream_temp_avg %&gt;% filter(count_nu &gt;= 300, end_date &gt;= &quot;1975-01-01&quot;) nrow(data_sc_stream_temp_avg_applicable) ## [1] 97 This means you would have 97 sites to work with for your study. 8.9 Common WQP function arguments countrycode, statecode, and countycode These geopolitical filters can be specified by a two letter abbreviation, state name, or Federal Information Processing Standard (FIPS) code. If you are using the FIPS code for a state or county, it must be preceded by the FIPS code of the larger geopolitical filter. For example, the FIPS code for the United States is US, and the FIPS code for South Carolina is 45. When querying with the statecode, you can enter statecode=\"US:45\". The same rule extends to county FIPS; for example, you can use countycode=\"45:001\" to query Abbeville County, South Carolina. You can find all state and county codes and abbreviations by executing stateCd or countyCd in your console. siteType Specify the hydrologic location the sample was taken, e.g. streams, lakes, groundwater sources. These should be listed as a string. Available types can be found here. organization The ID of the reporting organization. All USGS science centers are written USGS- and then the two-letter state abbrevation. For example, the Wisconsin Water Science Center would be written USGS-WI. For all available organization IDs, please see this list of org ids. The id is listed in the value field, but they are accompanied by the organization name in the desc (description) field. siteid This is the unique identification number associated with a data collection station. Site IDs for the same location may differ depending on the reporting organization. The site ID string is written as the agency code then the site number separated by a hyphen. For example, the USGS site 01594440 would be written as USGS-01594440. characteristicName and characteristicType Unlike NWIS, WQP does not have codes for each parameter. Instead, you need to search based on the name of the water quality constituent (referred to as characteristicName in dataRetrieval) or a group of parameters (characteristicType in dataRetrieval). For example, Nitrate is a characteristicName and Nutrient is the characteristicType that it fits into. For a complete list of water quality types and names, see characteristicType list and characteristicName list. startDate and endDate Arguments specifying the beginning and ending of the period of record you are interested in. For the dataRetrieval functions, these can be a date or character class in the form YYYY-MM-DD. For example, startDate = as.Date(\"2010-01-01\") or startDate = \"2010-01-01\" could both be your input arguments. 8.10 Discovering WQP data WQP has millions of records, and if you arent careful, your query could take hours because of the amount of data that met your criteria. To avoid this, you can query just for the number of records and number of sites that meet your criteria using the argument querySummary=TRUE in the function, readWQPdata. See the lesson on downloading WQP data to learn more about getting data. You can also use whatWQPsites to get the site information that matches your criteria. Lets follow a similar pattern to NWIS data discovery sections and explore available stream temperature data in South Carolina. 8.10.1 readWQPdata + querySummary readWQPdata is the function used to actually download WQP data. In this application, we are just querying for a count of sites and results that match our criteria. Since WQP expect state and county codes as their FIPS code, you will need to use the string US:45 for South Carolina. wqpcounts_sc &lt;- readWQPdata(statecode=&quot;US:45&quot;, querySummary = TRUE) names(wqpcounts_sc) ## [1] &quot;content-type&quot; &quot;content-length&quot; ## [3] &quot;connection&quot; &quot;server&quot; ## [5] &quot;date&quot; &quot;content-disposition&quot; ## [7] &quot;total-site-count&quot; &quot;nwis-site-count&quot; ## [9] &quot;storet-site-count&quot; &quot;total-activity-count&quot; ## [11] &quot;nwis-activity-count&quot; &quot;storet-activity-count&quot; ## [13] &quot;total-result-count&quot; &quot;nwis-result-count&quot; ## [15] &quot;storet-result-count&quot; &quot;x-frame-options&quot; ## [17] &quot;x-content-type-options&quot; &quot;x-xss-protection&quot; ## [19] &quot;strict-transport-security&quot; &quot;x-cache&quot; ## [21] &quot;via&quot; &quot;x-amz-cf-pop&quot; ## [23] &quot;x-amz-cf-id&quot; This returns a list with 23 different items, including total number of sites, breakdown of the number of sites by source (BioData, NWIS, STORET), total number of records, and breakdown of records count by source. Lets just look at total number of sites and total number of records. wqpcounts_sc[[&#39;total-site-count&#39;]] ## [1] 7608 wqpcounts_sc[[&#39;total-result-count&#39;]] ## [1] 4010616 This doesnt provide any information about the sites, just the total number. I know that with 4,010,616 results, I will want to add more criteria before trying to download. Lets continue to add query parameters before moving to whatWQPsites. # specify that you only want data from streams wqpcounts_sc_stream &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, querySummary = TRUE) wqpcounts_sc_stream[[&#39;total-site-count&#39;]] ## [1] 2143 wqpcounts_sc_stream[[&#39;total-result-count&#39;]] ## [1] 1868560 1,868,560 results are still a lot to download. Lets add more levels of criteria: # specify that you want water temperature data and it should be from 1975 or later wqpcounts_sc_stream_temp &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;, querySummary = TRUE) wqpcounts_sc_stream_temp[[&#39;total-site-count&#39;]] ## [1] 1575 wqpcounts_sc_stream_temp[[&#39;total-result-count&#39;]] ## [1] 140472 140,472 is little more manageble. We can also easily compare avilable stream temperature and lake temperature data. wqpcounts_sc_lake_temp &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Lake, Reservoir, Impoundment&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;, querySummary = TRUE) # comparing site counts wqpcounts_sc_stream_temp[[&#39;total-site-count&#39;]] ## [1] 1575 wqpcounts_sc_lake_temp[[&#39;total-site-count&#39;]] ## [1] 660 # comparing result counts wqpcounts_sc_stream_temp[[&#39;total-result-count&#39;]] ## [1] 140472 wqpcounts_sc_lake_temp[[&#39;total-result-count&#39;]] ## [1] 52494 From these query results, it looks like South Carolina has much more stream data than it does lake data. Now, lets try our South Carolina stream temperature query with whatWQPsites and see if we can narrow the results at all. 8.10.2 whatWQPsites whatWQPsites gives back site information that matches your search criteria. You can use any of the regular WQP web service arguments here. We are going to use whatWQPsites with the final criteria of the last query summary call - state, site type, parameter, and the earliest start date. This should return the same amount of sites as the last readWQPdata query did, 1,575. # Getting the number of sites and results for stream # temperature measurements in South Carolina after 1975. wqpsites_sc_stream_temp &lt;- whatWQPsites(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;) # number of sites nrow(wqpsites_sc_stream_temp) ## [1] 1575 # names of available columns names(wqpsites_sc_stream_temp) ## [1] &quot;OrganizationIdentifier&quot; ## [2] &quot;OrganizationFormalName&quot; ## [3] &quot;MonitoringLocationIdentifier&quot; ## [4] &quot;MonitoringLocationName&quot; ## [5] &quot;MonitoringLocationTypeName&quot; ## [6] &quot;MonitoringLocationDescriptionText&quot; ## [7] &quot;HUCEightDigitCode&quot; ## [8] &quot;DrainageAreaMeasure.MeasureValue&quot; ## [9] &quot;DrainageAreaMeasure.MeasureUnitCode&quot; ## [10] &quot;ContributingDrainageAreaMeasure.MeasureValue&quot; ## [11] &quot;ContributingDrainageAreaMeasure.MeasureUnitCode&quot; ## [12] &quot;LatitudeMeasure&quot; ## [13] &quot;LongitudeMeasure&quot; ## [14] &quot;SourceMapScaleNumeric&quot; ## [15] &quot;HorizontalAccuracyMeasure.MeasureValue&quot; ## [16] &quot;HorizontalAccuracyMeasure.MeasureUnitCode&quot; ## [17] &quot;HorizontalCollectionMethodName&quot; ## [18] &quot;HorizontalCoordinateReferenceSystemDatumName&quot; ## [19] &quot;VerticalMeasure.MeasureValue&quot; ## [20] &quot;VerticalMeasure.MeasureUnitCode&quot; ## [21] &quot;VerticalAccuracyMeasure.MeasureValue&quot; ## [22] &quot;VerticalAccuracyMeasure.MeasureUnitCode&quot; ## [23] &quot;VerticalCollectionMethodName&quot; ## [24] &quot;VerticalCoordinateReferenceSystemDatumName&quot; ## [25] &quot;CountryCode&quot; ## [26] &quot;StateCode&quot; ## [27] &quot;CountyCode&quot; ## [28] &quot;AquiferName&quot; ## [29] &quot;LocalAqfrName&quot; ## [30] &quot;FormationTypeText&quot; ## [31] &quot;AquiferTypeName&quot; ## [32] &quot;ConstructionDateText&quot; ## [33] &quot;WellDepthMeasure.MeasureValue&quot; ## [34] &quot;WellDepthMeasure.MeasureUnitCode&quot; ## [35] &quot;WellHoleDepthMeasure.MeasureValue&quot; ## [36] &quot;WellHoleDepthMeasure.MeasureUnitCode&quot; ## [37] &quot;ProviderName&quot; Similar to what we did with the NWIS functions, we can filter the sites further using the available metadata in wqpsites_sc_stream_temp. We are going to imagine that for our study the sites must have an associated drainage area and cannot be below sea level. Using dplyr::filter: # Filtering the number of sites and results for stream temperature # measurements in South Carolina after 1975 to also have an # associated drainage area and collected above sea level. wqpsites_sc_stream_temp_applicable &lt;- wqpsites_sc_stream_temp %&gt;% filter(!is.na(DrainageAreaMeasure.MeasureValue), VerticalMeasure.MeasureValue &gt; 0) nrow(wqpsites_sc_stream_temp_applicable) ## [1] 72 This brings the count down to a much more manageable 72 sites. Now we are ready to download this data. 8.11 readNWIS functions We have learned how to discover data available in NWIS, but now we will look at how to retrieve data. There are many functions to do this, see the table below for a description of each. Each variation of readNWIS is accessing a different web service. For a definition and more information on each of these services, please see https://waterservices.usgs.gov/rest/. Also, refer to the previous lesson for a description of the major arguments to readNWIS functions. Table 1. readNWIS function definitions Function Description Arguments readNWISdata Most general NWIS data import function. User must explicitly define the service parameter. More flexible than the other functions. , asDateTime, convertType, tz readNWISdv Returns time-series data summarized to a day. Default is mean daily. siteNumbers, parameterCd, startDate, endDate, statCd readNWISgwl Groundwater levels. siteNumbers, startDate, endDate, parameterCd, convertType, tz readNWISmeas Surface water measurements. siteNumbers, startDate, endDate, tz, expanded, convertType readNWISpCode Metadata information for one or many parameter codes. parameterCd readNWISpeak Annual maximum instantaneous streamflows and gage heights. siteNumbers, startDate, endDate, asDateTime, convertType readNWISqw Discrete water quality data. siteNumbers, parameterCd, startDate, endDate, expanded, reshape, tz readNWISrating Rating table information for active stream gages siteNumber, type, convertType readNWISsite Site metadata information siteNumbers readNWISstat Daily, monthly, or annual statistics for time-series data. Default is mean daily. siteNumbers, parameterCd, startDate, endDate, convertType, statReportType, statType readNWISuse Data from the USGS National Water Use Program. stateCd, countyCd, years, categories, convertType, transform readNWISuv Returns time-series data reported from the USGS Instantaneous Values Web Service. siteNumbers, parameterCd, startDate, endDate, tz Each service-specific function is a wrapper for the more flexible readNWISdata. They set a default for the service argument and have limited user defined arguments. All readNWIS functions require a major filter as an argument, but readNWISdata can accept any major filter while others are limited to site numbers or state/county codes (see Table 1 for more info). Other major filters that can be used in readNWISdata include hydrologic unit codes (huc) and bounding boxes (bBox). More information about major filters can be found in the NWIS web services documentation. The following are examples of how to use each of the readNWIS family of functions. Dont forget to load the dataRetrieval library if you are in a new session. readNWISdata, county major filter readNWISdata, huc major filter readNWISdata, bbox major filter readNWISdv readNWISgwl readNWISmeas readNWISpCode readNWISpeak readNWISqw, multiple sites readNWISqw, multiple parameters readNWISrating, using base table readNWISrating, corrected table readNWISrating, shift table readNWISsite readNWISstat readNWISuse readNWISuv 8.11.1 readNWISdata This function is the generic, catch-all for pulling down NWIS data. It can accept a number of arguments, but the argument name must be included. To use this function, you need to specify at list one major filter (state, county, site number, huc, or bounding box) and the NWIS service (daily value, instantaneous, groundwater, etc). The rest are optional query parameters. Follow along with the three examples below or see ?readNWISdata for more information. Historic mean daily streamflow for sites in Maui County, Hawaii. # Major filter: Maui County ## need to also include the state when using counties as the major filter # Service: daily value, dv # Parameter code: streamflow in cfs, 00060 MauiCo_avgdailyQ &lt;- readNWISdata(stateCd=&quot;Hawaii&quot;, countyCd=&quot;Maui&quot;, service=&quot;dv&quot;, parameterCd=&quot;00060&quot;) head(MauiCo_avgdailyQ) ## agency_cd site_no dateTime X_00060_00003 X_00060_00003_cd tz_cd ## 1 USGS 16400000 2022-09-28 3.44 P UTC ## 2 USGS 16401000 1929-08-31 18.00 A UTC ## 3 USGS 16402000 1957-07-31 51.00 A UTC ## 4 USGS 16403000 1957-06-30 5.50 A UTC ## 5 USGS 16403600 1970-09-30 2.40 A UTC ## 6 USGS 16403900 1996-09-30 1.30 A UTC # How many sites are returned? length(unique(MauiCo_avgdailyQ$site_no)) ## [1] 134 Historic minimum water temperatures for the HUC8 corresponding to the island of Maui, Hawaii. To see all HUCs available, visit https://water.usgs.gov/GIS/huc_name.html. The default statistic for daily values in readNWISdata is to return the max (00001), min (00002), and mean (00003). We will specify the minimum only for this example. You will need to use the statistic code, not the name. For all the available statistic codes, see the statType web service documentation and NWIS table mapping statistic names to codes. Caution! In readNWISdata and readNWISdv the argument is called statCd, but in readNWISstat the argument is statType. # Major filter: HUC 8 for Maui, 20020000 # Service: daily value, dv # Statistic: minimum, 00002 # Parameter code: water temperature in deg C, 00010 MauiHUC8_mindailyT &lt;- readNWISdata(huc=&quot;20020000&quot;, service=&quot;dv&quot;, statCd=&quot;00002&quot;, parameterCd=&quot;00010&quot;) head(MauiHUC8_mindailyT) ## agency_cd site_no dateTime X_00010_00002 X_00010_00002_cd tz_cd ## 1 USGS 16508000 2003-11-24 17.4 A UTC ## 2 USGS 16516000 2003-11-24 16.3 A UTC ## 3 USGS 16520000 2004-04-14 17.5 A UTC ## 4 USGS 16527000 2004-01-13 15.4 A UTC ## 5 USGS 16555000 2004-01-13 16.4 A UTC ## 6 USGS 16618000 2022-09-28 20.5 P UTC # How many sites are returned? length(unique(MauiHUC8_mindailyT$site_no)) ## [1] 47 Total nitrogen in mg/L for last 30 days around Great Salt Lake in Utah. This example uses Sys.Date to get the most recent date, so your dates will differ. To get any data around Great Salt Lake, we will use a bounding box as the major filter. The bounding box must be a vector of decimal numbers indicating the western longitude, southern latitude, eastern longitude, and northern latitude. The vector must be in that order. # Major filter: bounding box around Great Salt Lake # Service: water quality, qw # Parameter code: total nitrogen in mg/L, 00600 # Beginning: this past 30 days, use Sys.Date() prev30days &lt;- Sys.Date() - 30 SaltLake_totalN &lt;- readNWISdata(bBox=c(-113.0428, 40.6474, -112.0265, 41.7018), service=&quot;qw&quot;, parameterCd=&quot;00600&quot;, startDate=prev30days) # This service returns a lot of columns: names(SaltLake_totalN) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; ## [3] &quot;sample_dt&quot; &quot;sample_tm&quot; ## [5] &quot;sample_end_dt&quot; &quot;sample_end_tm&quot; ## [7] &quot;tm_datum_rlbty_cd&quot; &quot;coll_ent_cd&quot; ## [9] &quot;medium_cd&quot; &quot;tu_id&quot; ## [11] &quot;body_part_id&quot; &quot;p00004&quot; ## [13] &quot;p00010&quot; &quot;p00020&quot; ## [15] &quot;p00025&quot; &quot;p00041&quot; ## [17] &quot;p00061&quot; &quot;p00063&quot; ## [19] &quot;p00065&quot; &quot;p00095&quot; ## [21] &quot;p00098&quot; &quot;p00191&quot; ## [23] &quot;p00300&quot; &quot;p00301&quot; ## [25] &quot;p00400&quot; &quot;p00480&quot; ## [27] &quot;p01300&quot; &quot;p01305&quot; ## [29] &quot;p01320&quot; &quot;p01325&quot; ## [31] &quot;p01330&quot; &quot;p01340&quot; ## [33] &quot;p01345&quot; &quot;p01350&quot; ## [35] &quot;p30207&quot; &quot;p30209&quot; ## [37] &quot;p30211&quot; &quot;p50280&quot; ## [39] &quot;p70305&quot; &quot;p71820&quot; ## [41] &quot;p71999&quot; &quot;p72012&quot; ## [43] &quot;p72013&quot; &quot;p72020&quot; ## [45] &quot;p72105&quot; &quot;p72263&quot; ## [47] &quot;p81904&quot; &quot;p82398&quot; ## [49] &quot;p84164&quot; &quot;p84171&quot; ## [51] &quot;p84182&quot; &quot;p99111&quot; ## [53] &quot;p99156&quot; &quot;p99159&quot; ## [55] &quot;p99171&quot; &quot;p99173&quot; ## [57] &quot;p99206&quot; &quot;sample_start_time_datum_cd_reported&quot; ## [59] &quot;sample_end_time_datum_cd_reported&quot; &quot;startDateTime&quot; ## [61] &quot;tz_cd&quot; # How many sites are returned? length(unique(SaltLake_totalN$site_no)) ## [1] 10 8.11.2 readNWISdv This function is the daily value service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISdv for more information. Minimum and maximum pH daily data for a site on the Missouri River near Townsend, MT. # Remember, you can always use whatNWISdata to see what is available at the site before querying mt_available &lt;- whatNWISdata(siteNumber=&quot;462107111312301&quot;, service=&quot;dv&quot;, parameterCd=&quot;00400&quot;) head(mt_available) ## agency_cd site_no station_nm ## 4 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## 5 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## 6 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## site_tp_cd dec_lat_va dec_long_va coord_acy_cd dec_coord_datum_cd alt_va ## 4 ST 46.35188 -111.5239 S NAD83 3790 ## 5 ST 46.35188 -111.5239 S NAD83 3790 ## 6 ST 46.35188 -111.5239 S NAD83 3790 ## alt_acy_va alt_datum_cd huc_cd data_type_cd parm_cd stat_cd ts_id ## 4 20 NGVD29 10030101 dv 00400 00001 82218 ## 5 20 NGVD29 10030101 dv 00400 00002 82219 ## 6 20 NGVD29 10030101 dv 00400 00008 82220 ## loc_web_ds medium_grp_cd parm_grp_cd srs_id access_cd begin_date end_date ## 4 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## 5 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## 6 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## count_nu ## 4 72 ## 5 72 ## 6 72 # Major filter: site number, 462107111312301 # Statistic: minimum and maximum, 00001 and 00002 # Parameter: pH, 00400 mt_site_pH &lt;- readNWISdv(siteNumber=&quot;462107111312301&quot;, parameterCd=&quot;00400&quot;, statCd=c(&quot;00001&quot;, &quot;00002&quot;)) head(mt_site_pH) ## agency_cd site_no Date X_00400_00001 X_00400_00001_cd ## 1 USGS 462107111312301 2010-08-18 8.9 A ## 2 USGS 462107111312301 2010-08-19 8.9 A ## 3 USGS 462107111312301 2010-08-20 8.9 A ## 4 USGS 462107111312301 2010-08-21 8.9 A ## 5 USGS 462107111312301 2010-08-22 8.8 A ## 6 USGS 462107111312301 2010-08-23 8.9 A ## X_00400_00002 X_00400_00002_cd ## 1 8.3 A ## 2 8.3 A ## 3 8.4 A ## 4 8.4 A ## 5 8.4 A ## 6 8.4 A 8.11.3 readNWISgwl This function is the groundwater level service function. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISgwl for more information. Historic groundwater levels for a site near Portland, Oregon. # Major filter: site number, 452840122302202 or_site_gwl &lt;- readNWISgwl(siteNumbers=&quot;452840122302202&quot;) head(or_site_gwl) ## agency_cd site_no site_tp_cd lev_dt lev_tm lev_tz_cd_reported ## 1 USGS 452840122302202 GW 1988-03-14 &lt;NA&gt; UTC ## 2 USGS 452840122302202 GW 1988-03-14 &lt;NA&gt; UTC ## 3 USGS 452840122302202 GW 1988-03-14 &lt;NA&gt; UTC ## 4 USGS 452840122302202 GW 1988-04-05 17:50 UTC ## 5 USGS 452840122302202 GW 1988-04-05 17:50 UTC ## 6 USGS 452840122302202 GW 1988-04-05 17:50 UTC ## lev_va sl_lev_va sl_datum_cd lev_status_cd lev_agency_cd lev_dt_acy_cd ## 1 NA 232.07 NGVD29 1 &lt;NA&gt; D ## 2 NA 235.56 NAVD88 1 &lt;NA&gt; D ## 3 9.78 NA &lt;NA&gt; 1 &lt;NA&gt; D ## 4 NA 233.08 NGVD29 1 &lt;NA&gt; m ## 5 NA 236.57 NAVD88 1 &lt;NA&gt; m ## 6 8.77 NA &lt;NA&gt; 1 &lt;NA&gt; m ## lev_acy_cd lev_src_cd lev_meth_cd lev_age_cd parameter_cd lev_dateTime ## 1 &lt;NA&gt; &lt;NA&gt; Z A 62610 &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; Z A 62611 &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; Z A 72019 &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; S A 62610 1988-04-05 17:50:00 ## 5 &lt;NA&gt; &lt;NA&gt; S A 62611 1988-04-05 17:50:00 ## 6 &lt;NA&gt; &lt;NA&gt; S A 72019 1988-04-05 17:50:00 ## lev_tz_cd ## 1 UTC ## 2 UTC ## 3 UTC ## 4 UTC ## 5 UTC ## 6 UTC 8.11.4 readNWISmeas This function is the field measurement service function which pulls manual measurements for streamflow and gage height. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISmeas for more information. Historic surface water measurements for a site near Dade City, Florida. # Major filter: site number, 02311500 fl_site_meas &lt;- readNWISmeas(siteNumbers=&quot;02311500&quot;) # Names of columns returned: names(fl_site_meas) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; ## [3] &quot;measurement_nu&quot; &quot;measurement_dt&quot; ## [5] &quot;measurement_tm&quot; &quot;q_meas_used_fg&quot; ## [7] &quot;party_nm&quot; &quot;site_visit_coll_agency_cd&quot; ## [9] &quot;gage_height_va&quot; &quot;discharge_va&quot; ## [11] &quot;measured_rating_diff&quot; &quot;gage_va_change&quot; ## [13] &quot;gage_va_time&quot; &quot;control_type_cd&quot; ## [15] &quot;discharge_cd&quot; &quot;measurement_dateTime&quot; ## [17] &quot;tz_cd&quot; 8.11.5 readNWISpCode This function returns the parameter information associated with a parameter code. It only has one argument - the parameter code. See the example below or ?readNWISpCode for more information. Get information about the parameters gage height, specific conductance, and total phosphorus. This function only has one argument, the parameter code. You can supply one or multiple and you will get a dataframe with information about each parameter. # gage height, 00065 readNWISpCode(&quot;00065&quot;) ## parameter_cd parameter_group_nm parameter_nm casrn srsname ## 48 00065 Physical Gage height, feet Height, gage ## parameter_units ## 48 ft # specific conductance and total phosphorus, 00095 and 00665 readNWISpCode(c(&quot;00095&quot;, &quot;00665&quot;)) ## parameter_cd parameter_group_nm ## 63 00095 Physical ## 210 00665 Nutrient ## parameter_nm ## 63 Specific conductance, water, unfiltered, microsiemens per centimeter at 25 degrees Celsius ## 210 Phosphorus, water, unfiltered, milligrams per liter as phosphorus ## casrn srsname parameter_units ## 63 Specific conductance uS/cm @25C ## 210 7723-14-0 Phosphorus mg/l as P 8.11.6 readNWISpeak This function is the peak flow service function. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISpeak for more information. The default settings will return data where the date of the peak flow is known. To see peak flows with incomplete dates, change convertType to FALSE. This allows the date column to come through as character, keeping any incomplete or incorrect dates. Peak flow values for a site near Cassia, Florida. # Major filter: site number, 02235200 fl_site_peak &lt;- readNWISpeak(siteNumbers=&quot;02235200&quot;) fl_site_peak$peak_dt ## [1] &quot;1962-10-06&quot; &quot;1964-09-13&quot; &quot;1965-08-11&quot; &quot;1966-08-15&quot; &quot;1967-08-30&quot; ## [6] &quot;1968-09-01&quot; &quot;1968-10-22&quot; &quot;1969-10-05&quot; &quot;1971-02-10&quot; &quot;1972-04-02&quot; ## [11] &quot;1973-09-16&quot; &quot;1974-09-07&quot; &quot;1975-09-01&quot; &quot;1976-06-06&quot; NA ## [16] &quot;1978-08-08&quot; &quot;1979-09-29&quot; &quot;1980-04-04&quot; &quot;1981-09-18&quot; &quot;1982-04-12&quot; ## [21] &quot;1983-04-24&quot; &quot;1984-04-11&quot; &quot;1985-09-21&quot; &quot;1986-01-14&quot; &quot;1987-04-01&quot; ## [26] &quot;1988-09-11&quot; &quot;1989-01-24&quot; &quot;1990-02-27&quot; &quot;1991-06-02&quot; &quot;1991-10-08&quot; ## [31] &quot;1993-03-27&quot; &quot;1994-09-12&quot; &quot;1994-11-18&quot; &quot;1995-10-12&quot; &quot;1996-10-12&quot; ## [36] &quot;1998-02-21&quot; &quot;1998-10-05&quot; &quot;1999-10-08&quot; &quot;2001-09-17&quot; &quot;2002-08-16&quot; ## [41] &quot;2003-03-10&quot; &quot;2004-09-13&quot; &quot;2004-10-01&quot; &quot;2005-10-25&quot; &quot;2007-07-21&quot; ## [46] &quot;2008-08-26&quot; &quot;2009-05-26&quot; &quot;2010-03-16&quot; &quot;2011-04-07&quot; &quot;2012-08-30&quot; ## [51] &quot;2012-10-08&quot; &quot;2014-07-31&quot; &quot;2015-09-20&quot; &quot;2016-02-06&quot; &quot;2017-09-12&quot; ## [56] &quot;2018-07-06&quot; &quot;2019-08-17&quot; &quot;2020-07-17&quot; &quot;2020-10-04&quot; # Compare complete with incomplete/incorrect dates fl_site_peak_incomp &lt;- readNWISpeak(siteNumbers=&quot;02235200&quot;, convertType = FALSE) fl_site_peak_incomp$peak_dt[is.na(fl_site_peak$peak_dt)] ## [1] &quot;1977-00-00&quot; 8.11.7 readNWISqw This function is the water quality service function. It has a limited number of arguments and requires a site number and a parameter code. Follow along with the two examples below or see ?readNWISqw for more information. Dissolved oxygen for two sites near the Columbia River in Oregon for water year 2016 # Major filter: site numbers, 455415119314601 and 454554119121801 # Parameter: dissolved oxygen in mg/L, 00300 # Begin date: October 1, 2015 # End date: September 30, 2016 or_site_do &lt;- readNWISqw(siteNumbers=c(&quot;455415119314601&quot;, &quot;454554119121801&quot;), parameterCd=&quot;00300&quot;, startDate=&quot;2015-10-01&quot;, endDate=&quot;2016-09-30&quot;) ncol(or_site_do) ## [1] 36 head(or_site_do[,c(&quot;site_no&quot;,&quot;sample_dt&quot;,&quot;result_va&quot;)]) ## site_no sample_dt result_va ## 1 455415119314601 2015-10-28 0.7 ## 2 455415119314601 2016-01-20 0.1 ## 3 455415119314601 2016-03-18 0.1 ## 4 455415119314601 2016-04-21 0.4 ## 5 455415119314601 2016-06-22 0.5 ## 6 455415119314601 2016-07-28 0.0 Post Clean Water Act lead and mercury levels in McGaw, Ohio. # Major filter: site number, 03237280 # Parameter: mercury and lead in micrograms/liter, 71890 and 01049 # Begin date: January 1, 1972 oh_site_cwa &lt;- readNWISqw(siteNumbers=&quot;03237280&quot;, parameterCd=c(&quot;71890&quot;, &quot;01049&quot;), startDate=&quot;1972-01-01&quot;) nrow(oh_site_cwa) ## [1] 76 ncol(oh_site_cwa) ## [1] 36 head(oh_site_cwa[,c(&quot;parm_cd&quot;,&quot;sample_dt&quot;,&quot;result_va&quot;)]) ## parm_cd sample_dt result_va ## 1 01049 1972-06-20 0.0 ## 2 01049 1973-06-21 NA ## 3 71890 1973-06-21 0.5 ## 4 01049 1973-10-31 NA ## 5 71890 1973-10-31 0.5 ## 6 01049 1980-03-04 10.0 8.11.8 readNWISrating This function is the rating curve service function. It has a limited number of arguments and requires a site number. Follow along with the three examples below or see ?readNWISrating for more information. There are three different types of rating tables that can be accessed using the argument type. They are base, corr (corrected), and exsa (shifts). For type==\"base\" (the default), the result is a data frame with 3 columns: INDEP, DEP, and STOR. For type==\"corr\", the resulting data frame will have 3 columns: INDEP, CORR, and CORRINDEP. For type==\"exsa\", the data frame will have 4 columns: INDEP, DEP, STOR, and SHIFT. See below for definitions of each column. INDEP is the gage height in feet DEP is the streamflow in cubic feet per second STOR * indicates a fixed point of the rating curve, NA for non-fixed points SHIFT indicates shifting in rating for the corresponding INDEP value CORR are the corrected values of INDEP CORRINDEP are the corrected values of CORR There are also a number of attributes associated with the data.frame returned - url, queryTime, comment, siteInfo, and RATING. RATING will only be included when type is base. See this section for how to access attributes of dataRetrieval data frames. Rating tables for Mississippi River at St. Louis, MO # Major filter: site number, 07010000 # Type: default, base miss_rating_base &lt;- readNWISrating(siteNumber=&quot;07010000&quot;) head(miss_rating_base) ## INDEP DEP STOR ## 1 -10.95 30000.0 * ## 2 9.40 188188.6 * ## 3 17.89 293140.6 * ## 4 25.00 400400.0 * ## 5 30.00 501300.0 * ## 6 34.34 600000.0 * # Major filter: site number, 07010000 # Type: corr miss_rating_corr &lt;- readNWISrating(siteNumber=&quot;07010000&quot;, type=&quot;corr&quot;) head(miss_rating_corr) ## INDEP CORR CORRINDEP ## 1 -10.95 0 -10.95 ## 2 -10.94 0 -10.94 ## 3 -10.93 0 -10.93 ## 4 -10.92 0 -10.92 ## 5 -10.91 0 -10.91 ## 6 -10.90 0 -10.90 # Major filter: site number, 07010000 # Type: exsa miss_rating_exsa &lt;- readNWISrating(siteNumber=&quot;07010000&quot;, type=&quot;exsa&quot;) head(miss_rating_exsa) ## INDEP SHIFT DEP STOR ## 1 -10.95 0 30000.00 * ## 2 -10.94 0 30041.15 &lt;NA&gt; ## 3 -10.93 0 30082.33 &lt;NA&gt; ## 4 -10.92 0 30123.54 &lt;NA&gt; ## 5 -10.91 0 30164.78 &lt;NA&gt; ## 6 -10.90 0 30206.06 &lt;NA&gt; 8.11.9 readNWISsite This function is pulls data from a USGS site file. It only has one argument - the site number. Follow along with the example below or see ?readNWISsite for more information. Get metadata information for a site in Bronx, NY # site number, 01302020 readNWISsite(siteNumbers=&quot;01302020&quot;) ## agency_cd site_no station_nm site_tp_cd ## 1 USGS 01302020 BRONX RIVER AT NY BOTANICAL GARDEN AT BRONX NY ST ## lat_va long_va dec_lat_va dec_long_va coord_meth_cd coord_acy_cd ## 1 405144.3 735227.8 40.86231 -73.87439 N 1 ## coord_datum_cd dec_coord_datum_cd district_cd state_cd county_cd country_cd ## 1 NAD83 NAD83 36 36 005 US ## land_net_ds map_nm map_scale_fc alt_va alt_meth_cd alt_acy_va ## 1 NA FLUSHING, NY 24000 49.86 X 0.08 ## alt_datum_cd huc_cd basin_cd topo_cd instruments_cd ## 1 NAVD88 02030102 &lt;NA&gt; &lt;NA&gt; YNNNYNNNNYNNNNNNNNNNNNNNNNNNNN ## construction_dt inventory_dt drain_area_va contrib_drain_area_va tz_cd ## 1 NA NA 38.4 NA EST ## local_time_fg reliability_cd gw_file_cd nat_aqfr_cd aqfr_cd aqfr_type_cd ## 1 N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## well_depth_va hole_depth_va depth_src_cd project_no ## 1 NA NA &lt;NA&gt; &lt;NA&gt; 8.11.10 readNWISstat This function is the statistics service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISstat for more information. The NWIS Statistics web service is currently in Beta mode, so use at your own discretion. Additionally, mean is the only statType that can be used for annual or monthly report types at this time. Historic annual average discharge near Mississippi River outlet # Major filter: site number, 07374525 # Parameter: discharge in cfs, 00060 # Time division: annual # Statistic: average, &quot;mean&quot; mississippi_avgQ &lt;- readNWISstat(siteNumbers=&quot;07374525&quot;, parameterCd=&quot;00060&quot;, statReportType=&quot;annual&quot;, statType=&quot;mean&quot;) head(mississippi_avgQ) ## agency_cd site_no parameter_cd ts_id loc_web_ds year_nu mean_va ## 1 USGS 07374525 00060 61182 NA 2009 618800 ## 2 USGS 07374525 00060 61182 NA 2010 563700 ## 3 USGS 07374525 00060 61182 NA 2012 362100 ## 4 USGS 07374525 00060 61182 NA 2014 489000 ## 5 USGS 07374525 00060 61182 NA 2015 625800 ## 6 USGS 07374525 00060 61182 NA 2020 706300 8.11.11 readNWISuse This function is the water use service function. The water use data web service requires a state and/or county as the major filter. The default will return all years and all categories available. The following table shows the water-use categories and their corresponding abbreviation for county and state data. Note that categories have changed over time, and vary by data sets requested. National and site-specific data sets exist, but only county/state data are available through this service. Please visit the USGS National Water Use Information Program website for more information. Table 2. Water-use category names and abbreviations. Name Abbreviation Aquaculture AQ Commercial CO Domestic DO Hydroelectric Power HY Irrigation, Crop IC Irrigation, Golf Courses IG Industrial IN Total Irrigation IT Livestock (Animal Specialties) LA Livestock LI Livestock (Stock) LS Mining MI Other Industrial OI Thermoelectric Power (Closed-loop cooling) PC Fossil-fuel Thermoelectric Power PF Geothermal Thermoelectric Power PG Nuclear Thermoelectric Power PN Thermoelectric Power (Once-through cooling) PO Public Supply PS Total Power PT Total Thermoelectric Power PT Reservoir Evaporation RE Total Population TP Wastewater Treatment WW Follow along with the example below or see ?readNWISuse for more information. Las Vegas historic water use # Major filter: Clark County, NV # Water-use category: public supply, PS vegas_wu &lt;- readNWISuse(stateCd=&quot;NV&quot;, countyCd=&quot;Clark&quot;, categories=&quot;PS&quot;) ncol(vegas_wu) ## [1] 26 names(vegas_wu) ## [1] &quot;state_cd&quot; ## [2] &quot;state_name&quot; ## [3] &quot;county_cd&quot; ## [4] &quot;county_nm&quot; ## [5] &quot;year&quot; ## [6] &quot;Public.Supply.population.served.by.groundwater..in.thousands&quot; ## [7] &quot;Public.Supply.population.served.by.surface.water..in.thousands&quot; ## [8] &quot;Public.Supply.total.population.served..in.thousands&quot; ## [9] &quot;Public.Supply.self.supplied.groundwater.withdrawals..fresh..in.Mgal.d&quot; ## [10] &quot;Public.Supply.self.supplied.groundwater.withdrawals..saline..in.Mgal.d&quot; ## [11] &quot;Public.Supply.total.self.supplied.withdrawals..groundwater..in.Mgal.d&quot; ## [12] &quot;Public.Supply.self.supplied.surface.water.withdrawals..fresh..in.Mgal.d&quot; ## [13] &quot;Public.Supply.self.supplied.surface.water.withdrawals..saline..in.Mgal.d&quot; ## [14] &quot;Public.Supply.total.self.supplied.withdrawals..surface.water..in.Mgal.d&quot; ## [15] &quot;Public.Supply.total.self.supplied.withdrawals..fresh..in.Mgal.d&quot; ## [16] &quot;Public.Supply.total.self.supplied.withdrawals..saline..in.Mgal.d&quot; ## [17] &quot;Public.Supply.total.self.supplied.withdrawals..total..in.Mgal.d&quot; ## [18] &quot;Public.Supply.deliveries.to.domestic..in.Mgal.d&quot; ## [19] &quot;Public.Supply.deliveries.to.commercial..in.Mgal.d&quot; ## [20] &quot;Public.Supply.deliveries.to.industrial..in.Mgal.d&quot; ## [21] &quot;Public.Supply.deliveries.to.thermoelectric..in.Mgal.d&quot; ## [22] &quot;Public.Supply.total.deliveries..in.Mgal.d&quot; ## [23] &quot;Public.Supply.public.use.and.losses..in.Mgal.d&quot; ## [24] &quot;Public.Supply.per.capita.use..in.gallons.person.day&quot; ## [25] &quot;Public.Supply.reclaimed.wastewater..in.Mgal.d&quot; ## [26] &quot;Public.Supply.number.of.facilities&quot; head(vegas_wu[,1:7]) ## state_cd state_name county_cd county_nm year ## 1 32 Nevada 003 Clark County 1985 ## 2 32 Nevada 003 Clark County 1990 ## 3 32 Nevada 003 Clark County 1995 ## 4 32 Nevada 003 Clark County 2000 ## 5 32 Nevada 003 Clark County 2005 ## 6 32 Nevada 003 Clark County 2010 ## Public.Supply.population.served.by.groundwater..in.thousands ## 1 149.770 ## 2 108.140 ## 3 128.010 ## 4 176.850 ## 5 - ## 6 - ## Public.Supply.population.served.by.surface.water..in.thousands ## 1 402.210 ## 2 618.000 ## 3 844.060 ## 4 1169.600 ## 5 - ## 6 - 8.11.12 readNWISuv This function is the unit value (or instantaneous) service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISuv for more information. Turbidity and discharge for April 2016 near Lake Tahoe in California. # Major filter: site number, 10336676 # Parameter: discharge in cfs and turbidity in FNU, 00060 and 63680 # Begin date: April 1, 2016 # End date: April 30, 2016 ca_site_do &lt;- readNWISuv(siteNumbers=&quot;10336676&quot;, parameterCd=c(&quot;00060&quot;, &quot;63680&quot;), startDate=&quot;2016-04-01&quot;, endDate=&quot;2016-04-30&quot;) nrow(ca_site_do) ## [1] 2880 head(ca_site_do) ## agency_cd site_no dateTime X_00060_00000 X_00060_00000_cd ## 1 USGS 10336676 2016-04-01 07:00:00 28.9 A ## 2 USGS 10336676 2016-04-01 07:15:00 28.2 A ## 3 USGS 10336676 2016-04-01 07:30:00 28.2 A ## 4 USGS 10336676 2016-04-01 07:45:00 28.9 A ## 5 USGS 10336676 2016-04-01 08:00:00 28.9 A ## 6 USGS 10336676 2016-04-01 08:15:00 28.2 A ## X_63680_00000 X_63680_00000_cd tz_cd ## 1 1.2 A UTC ## 2 1.3 A UTC ## 3 1.2 A UTC ## 4 1.1 A UTC ## 5 1.2 A UTC ## 6 1.3 A UTC 8.12 Additional Features 8.12.1 Accessing attributes dataRetrieval returns a lot of useful information as attributes to the data returned. This includes site metadata information, the NWIS url used, date and time the query was performed, and more. First, you want to use attributes() to see what information is available. It returns a list of all the metadata information. Then you can use attr() to actually get that information. Lets use the base rating table example from before to illustrate this. It has a special attribute called RATING. # Major filter: site number, 07010000 # Type: default, base miss_rating_base &lt;- readNWISrating(siteNumber=&quot;07010000&quot;) # how many attributes are there and what are they? length(attributes(miss_rating_base)) ## [1] 9 names(attributes(miss_rating_base)) ## [1] &quot;row.names&quot; &quot;names&quot; &quot;class&quot; &quot;queryTime&quot; &quot;url&quot; ## [6] &quot;headerInfo&quot; &quot;comment&quot; &quot;RATING&quot; &quot;siteInfo&quot; # look at the site info attr(miss_rating_base, &quot;siteInfo&quot;) ## agency_cd site_no station_nm site_tp_cd lat_va ## 1 USGS 07010000 Mississippi River at St. Louis, MO ST 383744.4 ## long_va dec_lat_va dec_long_va coord_meth_cd coord_acy_cd coord_datum_cd ## 1 901047.2 38.629 -90.17978 N 5 NAD83 ## dec_coord_datum_cd district_cd state_cd county_cd country_cd ## 1 NAD83 29 29 510 US ## land_net_ds map_nm map_scale_fc alt_va alt_meth_cd ## 1 S T45N R07E 5 GRANITE CITY 24000 379.58 L ## alt_acy_va alt_datum_cd huc_cd basin_cd topo_cd ## 1 0.05 NAVD88 07140101 &lt;NA&gt; &lt;NA&gt; ## instruments_cd construction_dt inventory_dt drain_area_va ## 1 NNNNYNNNNNNNNNNNNNNNNNNNNNNNNN NA 19891229 697000 ## contrib_drain_area_va tz_cd local_time_fg reliability_cd gw_file_cd ## 1 NA CST Y C NNNNNNNN ## nat_aqfr_cd aqfr_cd aqfr_type_cd well_depth_va hole_depth_va depth_src_cd ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; ## project_no ## 1 &lt;NA&gt; # now look at the special RATING attribute attr(miss_rating_base, &quot;RATING&quot;) ## [1] &quot;ID=19.0&quot; ## [2] &quot;TYPE=STGQ&quot; ## [3] &quot;NAME=stage-discharge&quot; ## [4] &quot;AGING=Working&quot; ## [5] &quot;REMARKS= developed for persistent trend to right of rating below 25&quot; ## [6] &quot;EXPANSION=logarithmic&quot; ## [7] &quot;OFFSET1=-2.800000E+01&quot; All attributes are an R object once you extract them. They can be lists, data.frames, vectors, etc. If we want to use information from one of the attributes, index it just like you would any other object of that type. For example, we want the drainage area for this Mississippi site: # save site info metadata as its own object miss_site_info &lt;- attr(miss_rating_base, &quot;siteInfo&quot;) class(miss_site_info) ## [1] &quot;data.frame&quot; # extract the drainage area miss_site_info$drain_area_va ## [1] 697000 8.12.2 Using lists as input readNWISdata allows users to give a list of named arguments as the input to the call. This is especially handy if you would like to build up a list of arguments and use it in multiple calls. This only works in readNWISdata, none of the other readNWIS... functions have this ability. chicago_q_args &lt;- list(siteNumbers=c(&quot;05537500&quot;, &quot;05536358&quot;, &quot;05531045&quot;), startDate=&quot;2015-10-01&quot;, endDate=&quot;2015-12-31&quot;, parameterCd=&quot;00060&quot;) # query for unit value data with those arguments chicago_q_uv &lt;- readNWISdata(chicago_q_args, service=&quot;uv&quot;) nrow(chicago_q_uv) ## [1] 14488 # same query but for daily values chicago_q_dv &lt;- readNWISdata(chicago_q_args, service=&quot;dv&quot;) nrow(chicago_q_dv) ## [1] 151 8.12.3 Helper functions There are currently 3 helper functions: renameNWIScolumns, addWaterYear, and zeroPad. renameNWIScolumns takes some of the default column names and makes them more human-readable (e.g. X_00060_00000 becomes Flow_Inst). addWaterYear adds an additional column of integers indicating the water year. zeroPad is used to add leading zeros to any string that is missing them, and is not restricted to dataRetrieval output. renameNWIScolumns renameNWIScolumns can be used in two ways: it can be a standalone function following the dataRetrieval call or it can be piped (as long as magrittr or dplyr are loaded). Both examples are shown below. Note that renameNWIScolumns is intended for use with columns named using pcodes. It does not work with all possible data returned. # get discharge and temperature data for July 2016 in Ft Worth, TX ftworth_qt_july &lt;- readNWISuv(siteNumbers=&quot;08048000&quot;, parameterCd=c(&quot;00060&quot;, &quot;00010&quot;), startDate=&quot;2016-07-01&quot;, endDate=&quot;2016-07-31&quot;) names(ftworth_qt_july) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;X_00010_00000&quot; ## [5] &quot;X_00010_00000_cd&quot; &quot;X_00060_00000&quot; &quot;X_00060_00000_cd&quot; &quot;tz_cd&quot; # now rename columns ftworth_qt_july_renamed &lt;- renameNWISColumns(ftworth_qt_july) names(ftworth_qt_july_renamed) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;Wtemp_Inst&quot; ## [5] &quot;Wtemp_Inst_cd&quot; &quot;Flow_Inst&quot; &quot;Flow_Inst_cd&quot; &quot;tz_cd&quot; Now try with a pipe. Remember to load a packge that uses %&gt;%. library(magrittr) # get discharge and temperature data for July 2016 in Ft Worth, TX # pipe straight into rename function ftworth_qt_july_pipe &lt;- readNWISuv(siteNumbers=&quot;08048000&quot;, parameterCd=c(&quot;00060&quot;, &quot;00010&quot;), startDate=&quot;2016-07-01&quot;, endDate=&quot;2016-07-31&quot;) %&gt;% renameNWISColumns() names(ftworth_qt_july_pipe) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;Wtemp_Inst&quot; ## [5] &quot;Wtemp_Inst_cd&quot; &quot;Flow_Inst&quot; &quot;Flow_Inst_cd&quot; &quot;tz_cd&quot; addWaterYear Similar to renameNWIScolumns, addWaterYear can be used as a standalone function or with a pipe. This function defines a water year as October 1 of the previous year to September 30 of the current year. Additionally, addWaterYear is limited to data.frames with date columns titled dateTime, Date, ActivityStartDate, and ActivityEndDate. # mean daily discharge on the Colorado River in Grand Canyon National Park for fall of 2014 # The dates in Sept should be water year 2014, but the dates in Oct and Nov are water year 2015 co_river_q_fall &lt;- readNWISdv(siteNumber=&quot;09403850&quot;, parameterCd=&quot;00060&quot;, startDate=&quot;2014-09-28&quot;, endDate=&quot;2014-11-30&quot;) head(co_river_q_fall) ## agency_cd site_no Date X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-28 319.00 A ## 2 USGS 09403850 2014-09-29 411.00 A ## 3 USGS 09403850 2014-09-30 375.00 A ## 4 USGS 09403850 2014-10-01 26.40 A ## 5 USGS 09403850 2014-10-02 10.80 A ## 6 USGS 09403850 2014-10-03 5.98 A # now add the water year column co_river_q_fall_wy &lt;- addWaterYear(co_river_q_fall) head(co_river_q_fall_wy) ## agency_cd site_no Date waterYear X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-28 2014 319.00 A ## 2 USGS 09403850 2014-09-29 2014 411.00 A ## 3 USGS 09403850 2014-09-30 2014 375.00 A ## 4 USGS 09403850 2014-10-01 2015 26.40 A ## 5 USGS 09403850 2014-10-02 2015 10.80 A ## 6 USGS 09403850 2014-10-03 2015 5.98 A unique(co_river_q_fall_wy$waterYear) ## [1] 2014 2015 Now try with a pipe. # mean daily discharge on the Colorado River in Grand Canyon National Park for fall of 2014 # pipe straight into rename function co_river_q_fall_pipe &lt;- readNWISdv(siteNumber=&quot;09403850&quot;, parameterCd=&quot;00060&quot;, startDate=&quot;2014-09-01&quot;, endDate=&quot;2014-11-30&quot;) %&gt;% addWaterYear() names(co_river_q_fall_pipe) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;Date&quot; &quot;waterYear&quot; ## [5] &quot;X_00060_00003&quot; &quot;X_00060_00003_cd&quot; head(co_river_q_fall_pipe) ## agency_cd site_no Date waterYear X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-01 2014 3.79 A e ## 2 USGS 09403850 2014-09-02 2014 3.50 A e ## 3 USGS 09403850 2014-09-03 2014 3.54 A e ## 4 USGS 09403850 2014-09-04 2014 3.63 A e ## 5 USGS 09403850 2014-09-05 2014 3.85 A e ## 6 USGS 09403850 2014-09-06 2014 3.76 A e zeroPad zeroPad is designed to work on any string, so it is not specific to dataRetrieval data.frame output like the previous helper functions. Oftentimes, when reading in Excel or other local data, leading zeros are dropped from site numbers. This function allows you to put them back in. x is the string you would like to pad, and padTo is the total number of characters the string should have. For instance if an 8-digit site number was read in as numeric, we could pad that by: siteNum &lt;- 02121500 class(siteNum) ## [1] &quot;numeric&quot; siteNum ## [1] 2121500 siteNum_fix &lt;- zeroPad(siteNum, 8) class(siteNum_fix) ## [1] &quot;character&quot; siteNum_fix ## [1] &quot;02121500&quot; The next lesson looks at how to use dataRetrieval functions for Water Quality Portal retrievals. 8.13 readWQP functions After discovering Water Quality Portal (WQP) data in the data discovery section, we can now read it in using the desired parameters. There are two functions to do this in dataRetrieval. Table 1 describes them below. Table 1. readWQP function definitions Function Description Arguments readWQPdata Most general WQP data import function. Users must define each parameter. , querySummary, tz, ignore_attributes readWQPqw Used for querying by site numbers and parameter codes only. siteNumbers, parameterCd, startDate, endDate, tz, querySummary The main difference between these two functions is that readWQPdata is general and accepts any of the paremeters described in the WQP Web Services Guide. In contrast, readWQPqw has five arguments and users can only use this if they know the site number(s) and parameter code(s) for which they want data. The following are examples of how to use each of the readWQP family of functions. Dont forget to load the dataRetrieval library if you are in a new session. readWQPdata, state, site type, and characteristic name readWQPdata, county and characteristic group readWQPdata, bbox, characteristic name, and start date readWQPqw 8.13.1 readWQPdata The generic function used to pull Water Quality Portal data. This function is very flexible. You can specify any of the parameters from the WQP Web Service Guide. To learn what the possible values for each, see the table of domain values. Follow along with the three examples below or see ?readWQPdata for more information. Phosphorus data in Wisconsin lakes for water year 2010 # This takes about 3 minutes to complete. WI_lake_phosphorus_2010 &lt;- readWQPdata(statecode=&quot;WI&quot;, siteType=&quot;Lake, Reservoir, Impoundment&quot;, characteristicName=&quot;Phosphorus&quot;, startDate=&quot;2009-10-01&quot;, endDate=&quot;2010-09-30&quot;) # What columns are available? names(WI_lake_phosphorus_2010) ## [1] &quot;OrganizationIdentifier&quot; ## [2] &quot;OrganizationFormalName&quot; ## [3] &quot;ActivityIdentifier&quot; ## [4] &quot;ActivityTypeCode&quot; ## [5] &quot;ActivityMediaName&quot; ## [6] &quot;ActivityMediaSubdivisionName&quot; ## [7] &quot;ActivityStartDate&quot; ## [8] &quot;ActivityStartTime.Time&quot; ## [9] &quot;ActivityStartTime.TimeZoneCode&quot; ## [10] &quot;ActivityEndDate&quot; ## [11] &quot;ActivityEndTime.Time&quot; ## [12] &quot;ActivityEndTime.TimeZoneCode&quot; ## [13] &quot;ActivityDepthHeightMeasure.MeasureValue&quot; ## [14] &quot;ActivityDepthHeightMeasure.MeasureUnitCode&quot; ## [15] &quot;ActivityDepthAltitudeReferencePointText&quot; ## [16] &quot;ActivityTopDepthHeightMeasure.MeasureValue&quot; ## [17] &quot;ActivityTopDepthHeightMeasure.MeasureUnitCode&quot; ## [18] &quot;ActivityBottomDepthHeightMeasure.MeasureValue&quot; ## [19] &quot;ActivityBottomDepthHeightMeasure.MeasureUnitCode&quot; ## [20] &quot;ProjectIdentifier&quot; ## [21] &quot;ActivityConductingOrganizationText&quot; ## [22] &quot;MonitoringLocationIdentifier&quot; ## [23] &quot;ActivityCommentText&quot; ## [24] &quot;SampleAquifer&quot; ## [25] &quot;HydrologicCondition&quot; ## [26] &quot;HydrologicEvent&quot; ## [27] &quot;SampleCollectionMethod.MethodIdentifier&quot; ## [28] &quot;SampleCollectionMethod.MethodIdentifierContext&quot; ## [29] &quot;SampleCollectionMethod.MethodName&quot; ## [30] &quot;SampleCollectionEquipmentName&quot; ## [31] &quot;ResultDetectionConditionText&quot; ## [32] &quot;CharacteristicName&quot; ## [33] &quot;ResultSampleFractionText&quot; ## [34] &quot;ResultMeasureValue&quot; ## [35] &quot;ResultMeasure.MeasureUnitCode&quot; ## [36] &quot;MeasureQualifierCode&quot; ## [37] &quot;ResultStatusIdentifier&quot; ## [38] &quot;StatisticalBaseCode&quot; ## [39] &quot;ResultValueTypeName&quot; ## [40] &quot;ResultWeightBasisText&quot; ## [41] &quot;ResultTimeBasisText&quot; ## [42] &quot;ResultTemperatureBasisText&quot; ## [43] &quot;ResultParticleSizeBasisText&quot; ## [44] &quot;PrecisionValue&quot; ## [45] &quot;ResultCommentText&quot; ## [46] &quot;USGSPCode&quot; ## [47] &quot;ResultDepthHeightMeasure.MeasureValue&quot; ## [48] &quot;ResultDepthHeightMeasure.MeasureUnitCode&quot; ## [49] &quot;ResultDepthAltitudeReferencePointText&quot; ## [50] &quot;SubjectTaxonomicName&quot; ## [51] &quot;SampleTissueAnatomyName&quot; ## [52] &quot;ResultAnalyticalMethod.MethodIdentifier&quot; ## [53] &quot;ResultAnalyticalMethod.MethodIdentifierContext&quot; ## [54] &quot;ResultAnalyticalMethod.MethodName&quot; ## [55] &quot;MethodDescriptionText&quot; ## [56] &quot;LaboratoryName&quot; ## [57] &quot;AnalysisStartDate&quot; ## [58] &quot;ResultLaboratoryCommentText&quot; ## [59] &quot;DetectionQuantitationLimitTypeName&quot; ## [60] &quot;DetectionQuantitationLimitMeasure.MeasureValue&quot; ## [61] &quot;DetectionQuantitationLimitMeasure.MeasureUnitCode&quot; ## [62] &quot;PreparationStartDate&quot; ## [63] &quot;ProviderName&quot; ## [64] &quot;ActivityStartDateTime&quot; ## [65] &quot;ActivityEndDateTime&quot; #How much data is returned? nrow(WI_lake_phosphorus_2010) ## [1] 495 All nutrient data in Napa County, California # Use empty character strings to specify that you want the historic record. # This takes about 3 minutes to run. Napa_lake_nutrients_Aug2010 &lt;- readWQPdata(statecode=&quot;CA&quot;, countycode=&quot;055&quot;, characteristicType=&quot;Nutrient&quot;) #How much data is returned? nrow(Napa_lake_nutrients_Aug2010) ## [1] 5579 Everglades water temperature data since 2016 # Bounding box defined by a vector of Western-most longitude, Southern-most latitude, # Eastern-most longitude, and Northern-most longitude. # This takes about 3 minutes to run. Everglades_temp_2016_present &lt;- readWQPdata(bBox=c(-81.70, 25.08, -80.30, 26.51), characteristicName=&quot;Temperature, water&quot;, startDate=&quot;2016-01-01&quot;) #How much data is returned? nrow(Everglades_temp_2016_present) ## [1] 23752 8.13.2 readWQPqw This function has a limited number of arguments - it can only be used for pulling WQP data by site number and parameter code. By default, dates are set to pull the entire record available. When specifying USGS sites as siteNumbers to readWQP functions, precede the number with USGS-. See the example below or ?readWQPqw for more information. Dissolved oxygen data since 2010 for 2 South Carolina USGS sites # Using a few USGS sites, get dissolved oxygen data # This takes ~ 30 seconds to complete. SC_do_data_since2010 &lt;- readWQPqw(siteNumbers = c(&quot;USGS-02146110&quot;, &quot;USGS-325427080014600&quot;), parameterCd = &quot;00300&quot;, startDate = &quot;2010-01-01&quot;) # How much data was returned? nrow(SC_do_data_since2010) ## [1] 20 # What are the DO values and the dates the sample was collected? head(SC_do_data_since2010[, c(&quot;ResultMeasureValue&quot;, &quot;ActivityStartDate&quot;)]) ## ResultMeasureValue ActivityStartDate ## 1 5.8 2010-09-26 ## 2 5.9 2010-04-08 ## 3 5.0 2011-09-06 ## 4 7.2 2011-03-09 ## 5 4.8 2011-09-06 ## 6 6.4 2011-10-18 8.14 Attributes and metadata Similar to the data frames returned from readNWIS functions, there are attributes (aka metadata) attached to the data. Use attributes to see all of them and attr to extract a particular attribute. # What are the attributes available? wqp_attributes &lt;- attributes(Everglades_temp_2016_present) names(wqp_attributes) ## [1] &quot;names&quot; &quot;class&quot; &quot;row.names&quot; &quot;siteInfo&quot; &quot;variableInfo&quot; ## [6] &quot;queryTime&quot; &quot;url&quot; # Look at the variableInfo attribute head(attr(Everglades_temp_2016_present, &quot;variableInfo&quot;)) ## characteristicName param_units valueType ## 1 Temperature, water deg C &lt;NA&gt; ## 2 Temperature, water deg C &lt;NA&gt; ## 3 Temperature, water deg C &lt;NA&gt; ## 4 Temperature, water deg C &lt;NA&gt; ## 5 Temperature, water deg C &lt;NA&gt; ## 6 Temperature, water deg C &lt;NA&gt; Lets make a quick map to look at the stations that collected the Everglades data: siteInfo &lt;- attr(Everglades_temp_2016_present, &quot;siteInfo&quot;) library(maps) map(&#39;state&#39;, regions=&#39;florida&#39;) title(main=&quot;Everglade Sites&quot;) points(x=siteInfo$dec_lon_va, y=siteInfo$dec_lat_va) # Add a rectangle to see where your original query bounding box in relation to sites rect(-81.70, 25.08, -80.30, 26.51, col = NA, border = &#39;red&#39;) Figure 8.1: A map of NWIS site locations in the Everglades You can now find and download Water Quality Portal data from R! 8.15 USGS Coding Lab Exercises 8.15.1 Just how dry was the 2020 monsoon? Recently NOAA published a press release demonstrating that 2020 was both the hottest and driest summer on record for Arizona. In this lab we will look at USGS NWIS stream gauge and preciptitation data to investigate just how anomalous 2020 data are. 1: Use the readNWISstat function to retrieve the following statewide data for 2015-2020: 1. Precipitation total (inches/week) 2. Streamflow (ft^3/s) 2: Create two timeseries plots (one for preciptitation, one for streamflow), where color is a function of year (e.g. the x axis is month, the y axis is precipitation, legend shows color by year). 3. Calculate the monthly mean precipitation and streamflow from 2015-2019, and use that mean to calculate a 2020 anomaly timeseries. Create two new plots (like #2 above) with the 2015-2019 mean as a thick black line, and 2020 anomaly as a thin red line. 8.16 geoKnife - Introduction 8.17 Lesson Summary This lesson will explore how to find and download large gridded datasets via the R package geoknife. The package was created to allow easy access to data stored in the Geo Data Portal (GDP), or any gridded dataset available through the OPeNDAP protocol DAP2 that meets some basic metadata requirements. geoknife refers to the gridded dataset as the fabric, the spatial feature of interest as the stencil, and the subset algorithm parameters as the knife (see below). figure illustrating definitions of fabric, stencil, and knife 8.18 Lesson Objectives Explore and query large gridded datasets for efficient and reproducible large-scale analyses. By the end of this lesson, the learner will be able to: Explain the differences between remote and local processing. Differentiate the three main concepts of this package: fabric, stencil, and knife. Use this package to retrieve pre-processed data from the Geo Data Portal (GDP) via geoknife. 8.19 Lesson Resources Publication: geoknife: reproducible web-processing of large gridded datasets Tutorial (vignette): geoknife vignette An overview of remote processing basics with the Geo Data Portal A blog post on visualizing Tropical Storm Colin rainfall with geoknife Source code: geoknife on GitHub Report a bug or suggest a feature: geoknife issues on GitHub 8.20 Remote processing The USGS Geo Data Portal is designed to perform web-service processing on large gridded datasets. geoknife allows R users to take advantage of these services by processing large data, such as data available in the GDP catalog or a user-defined dataset, on the web. This type of workflow has three main advantages: it allows the user to avoid downloading large datasets, it avoids reinventing the wheel for the creation and optimization of complex geoprocessing algorithms, and computing resources are dedicated elsewhere, so geoknife operations do not have much of an impact on a local computer. 8.21 geoknife components: fabric, stencil, knife The main components of a geoknife workflow are the fabric, stencil, and knife. These three components go into the final element, the geoknife job (geojob), which defines what the user needs from the processing, allows tracking of the processing status, and methods to dowload the results. The fabric is the gridded web dataset to be processed, the stencil is the feature of interest, and the knife is the processing algorithm parameters. Each of the geoknife components is created using a corresponding function: fabrics are created using webdata(), stencils are created using webgeom() or simplegeom(), and knives are created using webprocess(). This lesson will focus on discovering what options exist for each of those components. The next lesson will teach you how to construct each component and put it together to get the processed data. Before continuing this lesson, load the geoknife library (install if necessary). # load the geoknife package library(geoknife) ## ## Attaching package: &#39;geoknife&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## id ## The following object is masked from &#39;package:stats&#39;: ## ## start ## The following object is masked from &#39;package:graphics&#39;: ## ## title ## The following object is masked from &#39;package:base&#39;: ## ## url 8.22 Available webdata 8.22.1 GDP datasets To learn what data is available through GDP, you can explore the online catalog or use the function query. Note that the access pattern for determining available fabrics differs slightly from stencils and knives. Running query(\"webdata\") will return every dataset available through GDP: all_webdata &lt;- query(&quot;webdata&quot;) head(all_webdata) ## An object of class &quot;datagroup&quot;: ## [1] 4km Monthly Parameter-elevation Regressions on Independent Slopes Model Monthly Climate Data for the Continental United States. January 2012 Shapshot ## url: https://cida.usgs.gov/thredds/dodsC/prism ## [2] 4km Monthly Parameter-elevation Regressions on Independent Slopes Model Monthly Climate Data for the Continental United States. January 2022 Snapshot ## url: https://cida.usgs.gov/thredds/dodsC/prism_v2 ## [3] ACCESS 1980-1999 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_1980_1999 ## [4] ACCESS 2040-2059 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_2040_2059 ## [5] ACCESS 2080-2099 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_2080_2099 ## [6] Bias Corrected Constructed Analogs V2 Daily Future CMIP5 Climate Projections ## url: https://cida.usgs.gov/thredds/dodsC/cmip5_bcca/future length(all_webdata) ## [1] 532 Notice that the object returned is a special geoknife class of datagroup. There are specific geoknife functions that only operate on an object of this class, see ?title and ?abstract. These two functions are used to extract metadata information about each of the available GDP datasets. With 532 datasets available, it is likely that reading through each to find ones that are of interest to you would be time consuming. You can use grep along with the functions title and abstract to figure out which datasets you would like to use for processing. Lets say that we were interested in evapotranspiration data. To search for which GDP datasets might contain evapotranspiration data, you can use the titles and abstracts. # notice that you cannot perform a grep on all_webdata - it is because it is a special class # `grep(&quot;evapotranspiration&quot;, all_webdata)` will fail # you need to perform pattern matching on vectors all_titles &lt;- title(all_webdata) which_titles &lt;- grep(&quot;evapotranspiration&quot;, all_titles) evap_titles &lt;- all_titles[which_titles] head(evap_titles) ## [1] &quot;Monthly Conterminous U.S. actual evapotranspiration data&quot; ## [2] &quot;Yearly Conterminous U.S. actual evapotranspiration data&quot; all_abstracts &lt;- abstract(all_webdata) which_abstracts &lt;- grep(&quot;evapotranspiration&quot;, all_abstracts) evap_abstracts &lt;- all_abstracts[which_abstracts] evap_abstracts[1] ## [1] &quot;Reference evapotranspiration (ET0), like potential evapotranspiration, is a measure of atmospheric evaporative demand. It was used in the context of this study to evaluate drought conditions that can lead to wildfire activity in Alaska using the Evaporative Demand Drought Index (EDDI) and the Standardized Precipitation Evapotranspiration Index (SPEI). The ET0 data are on a 20km grid with daily temporal resolution and were computed using the meteorological inputs from the dynamically downscaled ERA-Interim reanalysis and two global climate model projections (CCSM4 and GFDL-CM3). The model projections are from CMIP5 and use the RCP8.5 scenario. The dynamically downscaled data are available at https://registry.opendata.aws/wrf-alaska-snap/. The ET0 was computed following the American Society of Civil Engineers Standardized Reference Evapotranspiration Equation based on the downscaled daily temperature, humidity and winds. The full details of the computation of ET0, an evaluation of the underlying data, and the assessment of the fire indices are described in Ziel et al. (2020; https://doi.org/10.3390/f11050516).&quot; 13 possible datasets to look through is a lot more manageable than 532. Lets say the dataset titled Yearly Conterminous U.S. actual evapotranspiration data interested us enough to explore more. We have now identified a fabric of interest. We might want to know more about the dataset, such as what variables and time periods are available. To actually create the fabric, you will need to use webdata and supply the appropriate datagroup object as the input. This should result in an object with a class of webdata. The following functions will operate only on an object of class webdata. evap_fabric &lt;- webdata(all_webdata[&quot;Yearly Conterminous U.S. actual evapotranspiration data&quot;]) class(evap_fabric) ## [1] &quot;webdata&quot; ## attr(,&quot;package&quot;) ## [1] &quot;geoknife&quot; Now that we have a defined fabric, we can explore what variables and time period are within that data. First, we use query to determine what variables exist. Youll notice that the function variable returns NA. This is fine when you are just exploring available data; however, exploring available times requires that the variable be defined because sometimes individual variables within a dataset can have different time ranges. Thus, we need to set which variable from the dataset will be used. Then, we can explore times that are available in the data. Using query() is helpful when exploring datasets, but is unecessary if you already know what times and variables youd like to access. # no variables defined yet variables(evap_fabric) ## [1] NA # find what variables are available query(evap_fabric, &quot;variables&quot;) ## [1] &quot;et&quot; # trying to find available times before setting the variable results in an error # `query(evap_fabric, &quot;times&quot;)` will fail # only one variable, &quot;et&quot; variables(evap_fabric) &lt;- &quot;et&quot; variables(evap_fabric) ## [1] &quot;et&quot; # now that the variable is set, we can explore available times query(evap_fabric, &quot;times&quot;) ## [1] &quot;2000-01-01 UTC&quot; &quot;2021-01-01 UTC&quot; 8.22.2 Datasets not in GDP Any gridded dataset available online that follows OPeNDAP protocol and some additional conventions can be used with geoknife. These datasets can be found through web searches or other catalogs and require finding out the OPeNDAP endpoint (URL) for the dataset. This url is used as the input to the argument url in webdata. Please email the GDP development team (gdp@usgs.gov) with questions abvout these specifics. There are hundreds (or potentially thousands) of additional OPeNDAP datasets that will work with geoknife, but need to be found through web searches or catalogs (e.g., www.esrl.noaa.gov/psd/thredds/dodsC/Datasets and apdrc.soest.hawaii.edu/data/data.php ). We searched NOAAs OPenDAP data catalog and found data from the Center for Operational Oceanographic Products and Services THREDDS server. It includes forecasts for water levels, water currents, water temperatures, and salinity levels for Delaware Bay. Since it is forecasted data, the times associated with the data will change. To create a webdata object from this dataset, just use the OPeNDAP url. Then query variables and time as we did before. DelBay_fabric &lt;- webdata(url=&quot;https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/DBOFS/fmrc/Aggregated_7_day_DBOFS_Fields_Forecast_best.ncd&quot;) query(DelBay_fabric, &quot;variables&quot;) ## [1] &quot;zeta&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;temp&quot; &quot;Pair&quot; &quot;Uwind&quot; &quot;Vwind&quot; &quot;h&quot; ## [10] &quot;f&quot; &quot;pm&quot; &quot;pn&quot; &quot;angle&quot; # need to set the variable(s) in order to query the times variables(DelBay_fabric) &lt;- &quot;temp&quot; query(DelBay_fabric, &quot;times&quot;) ## [1] &quot;2022-09-21 UTC&quot; &quot;2022-10-01 UTC&quot; The first example weve included here uses aggregated data, meaning there is a single URL for all the data of this type on the server. Some data that you encounter might be non-aggregated, meaning there are multiple URLs to access the same data. In these cases, you will need to create more than one geojob and join data at the end. Here is a second example of using a non-GDP dataset which is not aggregated in times or variables. This data was found under the data section on Unidatas THREDDS server. This example includes water vapor at 8km resolution at national extent from here. wv_fabric &lt;- webdata(url=&quot;https://thredds.ucar.edu/thredds/dodsC/satellite/WV/SUPER-NATIONAL_8km/current/SUPER-NATIONAL_8km_WV_20200904_1330.gini&quot;) query(wv_fabric, &quot;variables&quot;) ## character(0) # need to set the variable(s) in order to query the times variables(wv_fabric) &lt;- &quot;IR_WV&quot; query(wv_fabric, &quot;times&quot;) # Note the times of this dataset include only a single timestep, since the data collection has a file for each time ## POSIXct of length 0 The url used here for webdata() contains the date and the time, so we would need to construct a loop through multiple timesteps in order to assemble a time series from this data source. 8.23 Available webgeoms Now that we have explored options for our webdata, lets look at what options exist for geospatial features. The next component to geoknife jobs is the spatial extent of the data, a.k.a. the stencil. The stencil is defined by using either of the functions simplegeom or webgeom. simplegeom is used to explicitly define an area by the user, but webgeom is used to specify an existing web feature service (WFS) as the geospatial extent. Defining your stencil using simplegeom will be covered in the next lesson. This lesson will just show you how to learn what available webgeoms exist. Users can use any WFS url to create their stencil, but there are a number of features that exist through GDP already. To determine what features exist, you can create a default webgeom object and then query its geom name, attributes, and values. This will return all available GDP default features. # setup a default stencil by using webgeom and not supplying any arguments default_stencil &lt;- webgeom() # now determine what geoms are available with the default default_geoms &lt;- query(default_stencil, &quot;geoms&quot;) length(default_geoms) ## [1] 25 head(default_geoms) ## [1] &quot;sample:Alaska&quot; &quot;upload:CIDA_TEST_2&quot; ## [3] &quot;sample:CONUS_Climate_Divisions&quot; &quot;derivative:CONUS_States&quot; ## [5] &quot;sample:CONUS_states&quot; &quot;sample:CSC_Boundaries&quot; You will notice a pattern with the names of the geoms: a category followed by :, and then a specific name. These category-name combinations are the strings you would use to define your geom. Additionally, webgeom can accept a URL that points directly to a WFS. The categories you should be familiar with are sample and upload. sample geoms are any that are available through geoknife by default. upload geoms are custom shapefiles that someone uploaded through GDP. If you would like to upload a specific shapefile to GDP, go to the advanced spatial section in GDP. Be aware that uploaded shapefiles are wiped from the server at regular intervals (could be as often as weekly). To use your own shapefile: upload it as a zip shapefile, execute your job and then save the output; re-upload your shapefile the next time you need it on GDP if it has expired. Similar to fabrics where you could not query times without setting the variables, you cannot query attributes of stencils before defining the geoms. Likewise, you cannot query for values of a stencil until you have set the attributes. Attributes give the metadata associated with the stencil and its geom. Values tell you the individual spatial features available in that attribute of the geom. # add a geom to see what values are available geom(default_stencil) ## [1] NA geom(default_stencil) &lt;- &quot;sample:CONUS_states&quot; # now that geom is set, you can query for available attributes query(default_stencil, &quot;attributes&quot;) ## [1] &quot;STATE&quot; &quot;FIPS&quot; attribute(default_stencil) &lt;- &quot;STATE&quot; # now that attribute is set, you can query for available values STATE_values &lt;- query(default_stencil, &quot;values&quot;) head(STATE_values) ## [1] &quot;Alabama&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; ## [6] &quot;Connecticut&quot; # switch the stencil to see the differences ecoreg_stencil &lt;- default_stencil geom(ecoreg_stencil) &lt;- &quot;sample:Ecoregions_Level_III&quot; query(ecoreg_stencil, &quot;attributes&quot;) ## [1] &quot;LEVEL3_NAM&quot; attribute(ecoreg_stencil) &lt;- &quot;LEVEL3_NAM&quot; ecoreg_values &lt;- query(ecoreg_stencil, &quot;values&quot;) head(ecoreg_values) ## [1] &quot;Ahklun And Kilbuck Mountains&quot; &quot;Alaska Peninsula Mountains&quot; ## [3] &quot;Alaska Range&quot; &quot;Aleutian Islands&quot; ## [5] &quot;Arctic Coastal Plain&quot; &quot;Arctic Foothills&quot; # now set the values to the Driftless Area and Blue Ridge ecoregions values(ecoreg_stencil) &lt;- ecoreg_values[c(12, 33)] values(ecoreg_stencil) ## [1] &quot;Blue Ridge&quot; &quot;Driftless Area&quot; There are some built-in templates that allow stencils to be defined more specifically. Currently, the package only supports US States, Level III Ecoregions, or HUC8s. Below are shortcuts to setting the geom, attribute, and values. # creating geoms from the available templates webgeom(&#39;state::Wisconsin&#39;) ## An object of class &quot;webgeom&quot;: ## url: https://labs.waterdata.usgs.gov/gdp_web/geoserver/wfs ## geom: sample:CONUS_states ## attribute: STATE ## values: Wisconsin ## wfs version: 1.1.0 webgeom(&#39;state::Wisconsin,Maine&#39;) # multiple states separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://labs.waterdata.usgs.gov/gdp_web/geoserver/wfs ## geom: sample:CONUS_states ## attribute: STATE ## values: Wisconsin, Maine ## wfs version: 1.1.0 webgeom(&#39;HUC8::09020306,14060009&#39;) # multiple HUCs separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://labs.waterdata.usgs.gov/gdp_web/geoserver/wfs ## geom: sample:simplified_huc8 ## attribute: HUC_8 ## values: 09020306, 14060009 ## wfs version: 1.1.0 webgeom(&#39;ecoregion::Colorado Plateaus,Driftless Area&#39;) # multiple ecoregions separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://labs.waterdata.usgs.gov/gdp_web/geoserver/wfs ## geom: sample:Ecoregions_Level_III ## attribute: LEVEL3_NAM ## values: Colorado Plateaus, Driftless Area ## wfs version: 1.1.0 8.24 Available webprocesses The final component to a geojob is the process algorithm used to aggregate the data across the defined stencil. Web process algorithms can be defined by the user, but lets explore the defaults available through GDP. # setup a default knife by using webprocess and not supplying any arguments default_knife &lt;- webprocess() # now determine what web processing algorithms are available with the default default_algorithms &lt;- query(default_knife, &#39;algorithms&#39;) length(default_algorithms) ## [1] 6 head(default_algorithms) ## $`Timeseries Service Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureTimeSeriesAlgorithm&quot; ## ## $`Categorical Coverage Fraction` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCategoricalGridCoverageAlgorithm&quot; ## ## $`OPeNDAP Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm&quot; ## ## $`Area Grid Statistics (unweighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureGridStatisticsAlgorithm&quot; ## ## $`Area Grid Statistics (weighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm&quot; ## ## $`WCS Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageIntersectionAlgorithm&quot; From this list, you can define which algorithm you would like the webprocess component to use. Definitions of each of the default algorithms can be found in the Geo Data Portal Algorithm Summaries section of the GDP process documentation. For example, we want to use the OPeNDAP subsetting algorithm, OPeNDAP Subset, which downloads a spatiotemporal chunk of data without any spatial processing. # algorithm actually has a default of the weighted average algorithm(default_knife) ## $`Area Grid Statistics (weighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm&quot; # change the algorithm to OPeNDAP&#39;s subset algorithm(default_knife) &lt;- default_algorithms[&#39;OPeNDAP Subset&#39;] algorithm(default_knife) ## $`OPeNDAP Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm&quot; Now that we can explore all of our options, we will learn how to construct each component and execute a geojob in the next lesson. 8.25 Setting up a geojob A geojob is the object that contains all of the necessary processing information to execute a data request to GDP. The geojob is made up of the stencil, fabric, and knife. To create a geojob, use the function geoknife and give the three components as arguments. stencil and fabric must be indicated, but knife has a default. Any additional arguments are specifications for the webprocessing step. See ?'webprocess-class' for options. This portion of the lesson will not discuss all of the options. # load the geoknife package library(geoknife) Lets setup a geojob to find the unweighted annual evapotranspiration rates for the state of Indiana using the annual evapotranspiration data we saw in the previous section. Since we have seen the URL and know the available variables and times, we can set all of that directly in the webdata function. Note: the times field must be a vector of start then end date, and be class POSIXct or character. # create fabric evap_fabric &lt;- webdata(times = c(&quot;2005-01-01&quot;, &quot;2015-01-01&quot;), variables = &quot;et&quot;, url = &#39;https://cida.usgs.gov/thredds/dodsC/ssebopeta/yearly&#39;) # create stencil evap_stencil &lt;- webgeom(&#39;state::Indiana&#39;) # create knife (which defaults to weighted) evap_knife &lt;- webprocess() # find unweighted algorithm all_algorithms &lt;- query(evap_knife, &#39;algorithms&#39;) unw_algorithm &lt;- all_algorithms[grep(&#39;unweighted&#39;, names(all_algorithms))] # set knife algorithm to unweighted algorithm(evap_knife) &lt;- unw_algorithm # create the geojob evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) ## Process Accepted 8.26 Checking the geojob status The geojob has been created and started on the server. Now, you can check to see if the processing is complete by using check. check(evap_geojob) ## $status ## [1] &quot;Process successful&quot; ## ## $URL ## [1] &quot;https://labs.waterdata.usgs.gov:443/gdp-process-wps/RetrieveResultServlet?id=670f3c50-b7e4-4770-afa2-2c5f1b8042edOUTPUT&quot; ## ## $statusType ## [1] &quot;ProcessSucceeded&quot; ## ## $percentComplete ## [1] &quot;100&quot; Other helpful functions to get status information about the job are running (returns TRUE/FALSE to say if the job is still processing), error (returns TRUE/FALSE to say if there was an error during the processing), and successful (returns TRUE/FALSE indicating whether the job process was able to complete without any issues). Only one of these can return TRUE at a time. running(evap_geojob) ## [1] FALSE error(evap_geojob) ## [1] FALSE successful(evap_geojob) ## [1] TRUE The results of all the status checks say that our job was successful. These status checks are useful if you put a geojob in a script and want to fail gracefully when there is an error in the job. 8.27 Getting geojob data Since this job has finished processing and was successful, you can now get the data. Youll notice that evap_geojob does not actually contain any data. It only contains information about the job that you submitted. To get the data, you need to use result or download. The feature summary algorithms will return simple tabular data, so you can use result to automatically take the output and parse it into an R data.frame. We used a feature summary algorithm in the evapotranspiration example, which returned tabular data. So, lets use result to get the geojob output. evap_data &lt;- result(evap_geojob) nrow(evap_data) ## [1] 11 head(evap_data) ## DateTime Indiana variable statistic ## 1 2005-01-01 603.7376 et MEAN ## 2 2006-01-01 688.1866 et MEAN ## 3 2007-01-01 576.3254 et MEAN ## 4 2008-01-01 691.2009 et MEAN ## 5 2009-01-01 689.2706 et MEAN ## 6 2010-01-01 630.4045 et MEAN There are additional algorithms that return subsets of the raw data as netcdf or geotiff formats. These formats will require you to handle the output manually using download. Use this function to download the output to a file and then read it using your preferred data parsing method. download can also be used for tabular data if you have a parsing method that differs from what is used in result. See ?download for more information. 8.28 wait and email This was not a computationally or spatially intensive request, so the job finished almost immediately. However, if we had setup a more complex job, it could still be running. Even though the processing of these large gridded datasets uses resources on a remote server, your workflow needs to account for processing time before using the results. There are a few scenarios to consider: You are manually executing a job and manually checking it. You are running a script that kicks off a geoknife process followed by lines of code that use the returned data. You are running a long geoknife process and want to be able to close R/RStudio and come back to a completed job later. For the first scenario, the workflow from above was fine. If you are manually checking that the job has completed before trying to extract results, then nothing should fail. For the second scenario, your code will fail because it will continue to execute the code line by line after starting the job. So, your code will fail at the code that gets the data (result/download) since the job is still running. You can prevent scripts from continuing until the job is complete by using the function wait. This function makes a call to GDP at specified intervals to see if the job is complete, and allows the code to continue once the job is complete. This function has two arguments: the geojob object and sleep.time. sleep.time defines the interval at which to check the status of the job in seconds (the default for sleep.time is 5 seconds). Please try to adjust sleep.time to limit the number of calls to GDP, e.g. if you know the job will take about an hour, set sleep.time=120 (2 min) because every 5 seconds would be excessive. # typical wait workflow evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) wait(evap_geojob, sleep.time = 20) evap_data &lt;- result(evap_geojob) If you know ahead of time that your process will be long, you can tell the job to wait when defining your knife (the default is to not wait). sleep.time can be specified as an argument to webprocess. The following is functionally the same as the use of wait() from above. # create knife with the args wait and sleep.time evap_knife &lt;- webprocess(wait=TRUE, sleep.time=20) # follow the same code from before to get the unweighted algorithm all_algorithms &lt;- query(evap_knife, &#39;algorithms&#39;) unw_algorithm &lt;- all_algorithms[grep(&#39;unweighted&#39;, names(all_algorithms))] algorithm(evap_knife) &lt;- unw_algorithm # create geojob using knife w/ wait evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) # get result evap_data &lt;- result(evap_geojob) As for the third scenario, if you have a job that will take a long time and plan to close R in the interim, you can specify the argument email when creating the knife. Then when you use your new knife in the geoknife call, it will send an email with appropriate information upon job completion (you will see gdp_data@usgs.gov as the sender). # example of how to specify an email address to get a job completion alert knife_willemail &lt;- webprocess(email=&#39;fake.email@gmail.com&#39;) knife_willemail ## An object of class &quot;webprocess&quot;: ## url: https://labs.waterdata.usgs.gov/gdp-process-wps/WebProcessingService ## algorithm: Area Grid Statistics (weighted) ## web processing service version: 1.0.0 ## process inputs: ## SUMMARIZE_TIMESTEP: false ## SUMMARIZE_FEATURE_ATTRIBUTE: false ## DELIMITER: COMMA ## REQUIRE_FULL_COVERAGE: true ## STATISTICS: ## GROUP_BY: ## wait: FALSE ## email: fake.email@gmail.com The email alert will contain the completed job URL. Since this process requires you to leave R and get information from an email, it is often only recommended if you dont plan to do further analysis in R, or you use saveRDS to save the geojob object so you can load it in a later session. Otherwise, we recommend using the wait() function in a script. Use the URL as a string in this workflow to get your results: geojob &lt;- geojob(&quot;my url as a string&quot;) check(geojob) mydata &lt;- result(geojob) 8.29 Putting it all together, mapping precipitation from Tropical Storm Colin The following example is slightly modified from the blog post here and shows a workflow for accessing rainfall data for the time period covering Tropical Storm Colin and visualizing the total rainfall per county during the time range. Define a function to retrieve precip data using geoknife getPrecip &lt;- function(states, startDate, endDate){ # use fips data from maps package counties_fips &lt;- maps::county.fips %&gt;% mutate(statecounty=as.character(polyname)) %&gt;% # character to split into state &amp; county tidyr::separate(polyname, c(&#39;statename&#39;, &#39;county&#39;), &#39;,&#39;) %&gt;% mutate(fips = sprintf(&#39;%05d&#39;, fips)) %&gt;% # fips need 5 digits to join w/ geoknife result dplyr::filter(statename %in% states) stencil &lt;- webgeom(geom = &#39;sample:Counties&#39;, attribute = &#39;FIPS&#39;, values = counties_fips$fips) fabric &lt;- webdata(url = &#39;https://cida.usgs.gov/thredds/dodsC/UofIMETDATA&#39;, variables = &quot;precipitation_amount&quot;, times = c(startDate, endDate)) job &lt;- geoknife(stencil, fabric, wait = TRUE, REQUIRE_FULL_COVERAGE=FALSE) check(job) precipData_result &lt;- result(job, with.units=TRUE) precipData &lt;- precipData_result %&gt;% select(-variable, -statistic, -units) %&gt;% gather(key = fips, value = precipVal, -DateTime) %&gt;% left_join(counties_fips, by=&quot;fips&quot;) #join w/ counties data return(precipData) } # Function to map cumulative precipitation data using R package maps: precipMap &lt;- function(precipData, startDate, endDate){ cols &lt;- colorRampPalette(brewer.pal(9,&#39;Blues&#39;))(9) precip_breaks &lt;- c(seq(0,80,by = 10), 200) precipData_cols &lt;- precipData %&gt;% group_by(statename, statecounty) %&gt;% summarize(cumprecip = sum(precipVal)) %&gt;% mutate(cols = cut(cumprecip, breaks = precip_breaks, labels = cols, right=FALSE)) %&gt;% mutate(cols = as.character(cols)) par(mar = c(0,0,3,0)) map(&#39;county&#39;, regions = precipData_cols$statecounty, fill = TRUE, col = precipData_cols$cols, exact=TRUE) legend(x = &quot;bottomright&quot;, fill = cols, cex = 0.7, bty = &#39;n&#39;, title = &quot;Cumulative\\nPrecipitation (mm)&quot;, legend = c(paste(&#39;&lt;&#39;, precip_breaks[-c(1,length(precip_breaks))]), paste(&#39;&gt;&#39;, tail(precip_breaks,2)[1]))) # greater graphics::title(&quot;Cumulative Precipitation from Tropical Storm Colin&quot;, line = 2, cex.main=1.2) #title was being masked by geoknife mtext(side = 3, line = 1, cex = 0.9, text= paste(&quot;By county from&quot;, startDate, &quot;to&quot;, endDate)) } # Now, we can use those functions to fetch data for specific counties and time periods. # TS Colin made landfall on June 6th and moved into open ocean on June 7th. Use these dates as the start and end times in our # function (need to account for timezone, +5 UTC). We can visualize the path of the storm by mapping cumulative precipitation for each county. library(dplyr) library(tidyr) library(geoknife) #order matters because &#39;query&#39; is masked by a function in dplyr library(RColorBrewer) library(maps) library(conflicted) conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;) conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) statesTSColin &lt;- c(&#39;florida&#39;, &#39;alabama&#39;, &#39;georgia&#39;, &#39;south carolina&#39;, &#39;north carolina&#39;) startTSColin &lt;- &quot;2016-06-06 05:00:00&quot; endTSColin &lt;- &quot;2016-06-08 05:00:00&quot; precipData &lt;- getPrecip(states = statesTSColin, startDate = startTSColin, endDate = endTSColin) precipMap(precipData, startDate = startTSColin, endDate = endTSColin) 8.30 USGS NWIS Culmination Write Up Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using USGS NWIS and USGS GDP data. Include the types of data that you would need to implement this project and how you would retrieve them. Save this summary as you will be refining and adding to your ideas over the course of the semester. Suggestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragraph summarizing how this data or analysis is useful to you and/or the infrastructure. "],["frequently-asked-questions.html", "Frequently Asked Questions: 8.31 Where can I find due dates for assignments? 8.32 How do I submit assignments? 8.33 Do I still have to submit written exercises as .Rmd and .pdf? 8.34 Whats better for code, conciseness or readability? 8.35 How find I find resources to navigate the NEON Data Portal? 8.36 How can I best prepare for class and succeed?", " Frequently Asked Questions: 8.31 Where can I find due dates for assignments? All assignment deadlines can be found in BBLearn. Within this textbook we have suggestions for the timing of all writen questions, coding labs and final culmination writes ups. All material by infrastructure is due before we begin the next infrastructure. 8.32 How do I submit assignments? All assignments (except the very first git assignment) are submitted via BBLearn as both .Rmds and .pdfs 8.33 Do I still have to submit written exercises as .Rmd and .pdf? Yes. 8.34 Whats better for code, conciseness or readability? Readability &gt; Conciseness I personally almost always use dplyr, my thoughts, pulled largely from Dr. Derek Sondereggers Statistical Computing Course are below: The pipe command in dplyr %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that take input arguments of the same type as their output. They are human readable! For example if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Dr. Hadley Wickham (aka R genius) gave the following example of readability: bopping( scooping_up( hopping_through(foo_foo), field_mice), head) is more readably written: foo_foo %&gt;% hopping_through(forest) %&gt;% scooping_up( field_mice) %&gt;% bopping( head ) In dplyr, all the functions take a data set as its first argument and outputs an appropriately modified data set. This allows me to chain together commands in a readable fashion. Then in 3 months I dont have to wonder what on earth I was doing last time I opened this project, if I filtered the data, etc etc. Your future self will sincerely thank your past self. 8.35 How find I find resources to navigate the NEON Data Portal? A fantastic powerpoint giving you step-by-step directions can be found here. 8.36 How can I best prepare for class and succeed? Read the textbook, click on linked resources including videos, review materials as we go or read ahead. Complete assignments as we go or ahead of time. Do not wait until the last minute. Pick a coding buddy and help each other tackle errors that arise. Reach out to your instructors if you need clarification on assignments. "],["fall-2020-ignite-session.html", "Chapter 9 Fall 2020 IGNITE Session", " Chapter 9 Fall 2020 IGNITE Session "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
